{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008ebe94",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9d4a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "import os\n",
    "import ollama\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712a89a",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c0fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 資料載入選項 ===\n",
      "使用本地檔案: True\n",
      "使用串流模式: False\n",
      "下載完整資料集: False\n",
      "\n",
      "📁 從本地檔案讀取資料...\n",
      "找到的檔案:\n",
      "  CSV 檔案: 1 個\n",
      "  JSON 檔案: 1 個\n",
      "  Parquet 檔案: 1 個\n",
      "\n",
      "📊 讀取最新的 Parquet 檔案: saved_datasets/clue_corpus_small_20250901_085816.parquet\n",
      "\n",
      "✅ 資料載入成功！\n",
      "📊 資料形狀: (1000, 1)\n",
      "📋 欄位名稱: ['text']\n",
      "\n",
      "📈 文本長度統計:\n",
      "count     1000.000000\n",
      "mean       300.899000\n",
      "std        785.763282\n",
      "min          5.000000\n",
      "25%         38.000000\n",
      "50%        105.500000\n",
      "75%        274.000000\n",
      "max      17020.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "📝 前 3 筆資料範例:\n",
      "範例 1 (132 字符): 130真是佩服这家店开这么久。尽管门面已经小了一圈，但还是开着不容易啊。我们不容易，老板也不容易。自助餐，你可以吃得比平时多，但决不能浪费。想吃回20元，那是不可能的，所以还是不要去了。菜真的很一般，...\n",
      "--------------------------------------------------------------------------------\n",
      "範例 2 (8 字符): 送货速度奇慢无比\n",
      "--------------------------------------------------------------------------------\n",
      "範例 3 (12 字符): 这是自己用过最好的用的了\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🎯 資料已準備就緒，可用於後續的 LLM 評分處理！\n"
     ]
    }
   ],
   "source": [
    "# 資料讀取選項\n",
    "# 您可以選擇以下任一種方式來載入資料：\n",
    "\n",
    "# 選項 1: 從已儲存的本地檔案讀取 (推薦，速度快)\n",
    "use_local_files = False\n",
    "\n",
    "# 選項 2: 從 Hugging Face 直接串流載入 (需要網路連線)\n",
    "use_streaming = True\n",
    "\n",
    "# 選項 3: 下載完整資料集 (檔案很大，不推薦)\n",
    "use_full_download = False\n",
    "\n",
    "print(\"=== 資料載入選項 ===\")\n",
    "print(f\"使用本地檔案: {use_local_files}\")\n",
    "print(f\"使用串流模式: {use_streaming}\")\n",
    "print(f\"下載完整資料集: {use_full_download}\")\n",
    "\n",
    "# 資料載入\n",
    "if use_local_files:\n",
    "    print(\"\\n📁 從本地檔案讀取資料...\")\n",
    "    \n",
    "    # 檢查已儲存的檔案\n",
    "    save_dir = \"saved_datasets\"\n",
    "    \n",
    "    if os.path.exists(save_dir):\n",
    "        import glob\n",
    "        \n",
    "        # 尋找可用的檔案\n",
    "        csv_files = glob.glob(f\"{save_dir}/*.csv\")\n",
    "        json_files = glob.glob(f\"{save_dir}/*.json\")\n",
    "        parquet_files = glob.glob(f\"{save_dir}/*.parquet\")\n",
    "        \n",
    "        print(f\"找到的檔案:\")\n",
    "        print(f\"  CSV 檔案: {len(csv_files)} 個\")\n",
    "        print(f\"  JSON 檔案: {len(json_files)} 個\")\n",
    "        print(f\"  Parquet 檔案: {len(parquet_files)} 個\")\n",
    "        \n",
    "        # 優先使用 Parquet 檔案 (最高效)\n",
    "        if parquet_files:\n",
    "            latest_file = max(parquet_files, key=os.path.getctime)\n",
    "            print(f\"\\n📊 讀取最新的 Parquet 檔案: {latest_file}\")\n",
    "            df = pd.read_parquet(latest_file)\n",
    "            \n",
    "        # 其次使用 CSV 檔案\n",
    "        elif csv_files:\n",
    "            latest_file = max(csv_files, key=os.path.getctime)\n",
    "            print(f\"\\n📊 讀取最新的 CSV 檔案: {latest_file}\")\n",
    "            df = pd.read_csv(latest_file)\n",
    "            \n",
    "        # 最後使用 JSON 檔案\n",
    "        elif json_files:\n",
    "            latest_file = max(json_files, key=os.path.getctime)\n",
    "            print(f\"\\n📊 讀取最新的 JSON 檔案: {latest_file}\")\n",
    "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ 沒有找到已儲存的資料檔案\")\n",
    "            print(\"請先執行資料下載和儲存的程式碼\")\n",
    "            df = None\n",
    "    else:\n",
    "        print(\"❌ 找不到 saved_datasets 目錄\")\n",
    "        print(\"請先執行資料下載和儲存的程式碼\")\n",
    "        df = None\n",
    "\n",
    "elif use_streaming:\n",
    "    print(\"\\n🌐 從 Hugging Face 串流載入資料...\")\n",
    "    \n",
    "    # 使用串流模式載入資料集\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\", streaming=True)\n",
    "    \n",
    "    # 設定要載入的樣本數量 - 減少到100筆用於演示\n",
    "    num_samples = 100\n",
    "    print(f\"載入前 {num_samples} 筆資料...\")\n",
    "    \n",
    "    # 收集資料\n",
    "    sample_data = []\n",
    "    for i, example in enumerate(dataset['train']):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        sample_data.append(example)\n",
    "        if (i + 1) % 25 == 0:\n",
    "            print(f\"  已載入 {i + 1} 筆資料...\")\n",
    "    \n",
    "    # 轉換為 DataFrame\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    \n",
    "elif use_full_download:\n",
    "    print(\"\\n⬇️ 下載完整資料集...\")\n",
    "    print(\"警告：這將下載 13.7GB 的資料，可能需要很長時間\")\n",
    "    \n",
    "    # 下載完整資料集\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\")\n",
    "    df = dataset['train'].to_pandas()\n",
    "\n",
    "else:\n",
    "    print(\"❌ 沒有選擇任何資料載入選項\")\n",
    "    df = None\n",
    "\n",
    "# 顯示資料資訊\n",
    "if df is not None:\n",
    "    print(f\"\\n✅ 資料載入成功！\")\n",
    "    print(f\"📊 資料形狀: {df.shape}\")\n",
    "    print(f\"📋 欄位名稱: {list(df.columns)}\")\n",
    "    \n",
    "    # 顯示基本統計\n",
    "    if 'text' in df.columns: # type: ignore\n",
    "        df['text_length'] = df['text'].str.len() # type: ignore\n",
    "        print(f\"\\n📈 文本長度統計:\")\n",
    "        print(df['text_length'].describe()) # type: ignore\n",
    "        \n",
    "        # 顯示前幾筆資料範例\n",
    "        print(f\"\\n📝 前 3 筆資料範例:\")\n",
    "        for i in range(min(3, len(df))): # type: ignore\n",
    "            text = df.iloc[i]['text']\n",
    "            # 顯示前100個字符\n",
    "            preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            print(f\"範例 {i+1} ({len(text)} 字符): {preview}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\n🎯 資料已準備就緒，可用於後續的 LLM 評分處理！\")\n",
    "else:\n",
    "    print(\"\\n❌ 資料載入失敗，請檢查設定並重新執行\")\n",
    "\n",
    "# 儲存到全域變數供後續使用\n",
    "globals()['dataset_df'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af633432",
   "metadata": {},
   "source": [
    "## 📝 文本切分處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ebb3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔪 啟動文本切分處理...\n",
      "❌ 沒有找到資料集，請先執行 Get Data 部分\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 📝 文本切分處理 - 切分為句子級別的片段\n",
    "print(\"🔪 啟動文本切分處理...\")\n",
    "\n",
    "def split_text_to_sentences(text, min_length=10, max_length=50):\n",
    "    \"\"\"\n",
    "    將文本切分為句子級別的片段\n",
    "    \n",
    "    Args:\n",
    "        text (str): 原始文本\n",
    "        min_length (int): 最小片段長度\n",
    "        max_length (int): 最大片段長度\n",
    "    \n",
    "    Returns:\n",
    "        list: 切分後的句子片段列表\n",
    "    \"\"\"\n",
    "    # 定義標點符號分隔符\n",
    "    sentence_separators = ['。', '！', '？', '；', '…']  # 強句號分隔符\n",
    "    phrase_separators = ['，', '、', '：', '；']  # 弱分隔符\n",
    "    \n",
    "    # 1. 首先按強標點符號切分成句子\n",
    "    sentences = []\n",
    "    current_sentence = \"\"\n",
    "    \n",
    "    for char in text:\n",
    "        current_sentence += char\n",
    "        if char in sentence_separators:\n",
    "            if current_sentence.strip():\n",
    "                sentences.append(current_sentence.strip())\n",
    "                current_sentence = \"\"\n",
    "    \n",
    "    # 處理最後一個句子（如果沒有以強標點結尾）\n",
    "    if current_sentence.strip():\n",
    "        sentences.append(current_sentence.strip())\n",
    "    \n",
    "    # 2. 對每個句子進一步按逗號等分隔符切分\n",
    "    fragments = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # 跳過太短的句子\n",
    "        if len(sentence) < min_length:\n",
    "            continue\n",
    "            \n",
    "        # 如果句子長度在合理範圍內，直接使用\n",
    "        if len(sentence) <= max_length:\n",
    "            fragments.append(sentence)\n",
    "        else:\n",
    "            # 對長句子按逗號等進一步切分\n",
    "            parts = []\n",
    "            current_part = \"\"\n",
    "            \n",
    "            for char in sentence:\n",
    "                current_part += char\n",
    "                if char in phrase_separators:\n",
    "                    if current_part.strip() and len(current_part.strip()) >= min_length:\n",
    "                        parts.append(current_part.strip())\n",
    "                        current_part = \"\"\n",
    "            \n",
    "            # 處理最後一部分\n",
    "            if current_part.strip() and len(current_part.strip()) >= min_length:\n",
    "                parts.append(current_part.strip())\n",
    "            \n",
    "            # 如果切分後的部分太短，嘗試合併\n",
    "            merged_parts = []\n",
    "            temp_part = \"\"\n",
    "            \n",
    "            for part in parts:\n",
    "                if len(temp_part + part) <= max_length:\n",
    "                    temp_part = temp_part + part if temp_part else part\n",
    "                else:\n",
    "                    if temp_part and len(temp_part) >= min_length:\n",
    "                        merged_parts.append(temp_part)\n",
    "                    temp_part = part\n",
    "            \n",
    "            # 添加最後一部分\n",
    "            if temp_part and len(temp_part) >= min_length:\n",
    "                merged_parts.append(temp_part)\n",
    "            \n",
    "            # 如果切分失敗，直接截斷\n",
    "            if not merged_parts and len(sentence) >= min_length:\n",
    "                # 簡單截斷成合適長度的片段\n",
    "                for i in range(0, len(sentence), max_length):\n",
    "                    fragment = sentence[i:i+max_length]\n",
    "                    if len(fragment) >= min_length:\n",
    "                        merged_parts.append(fragment)\n",
    "            \n",
    "            fragments.extend(merged_parts)\n",
    "    \n",
    "    return fragments\n",
    "\n",
    "def split_text_by_punctuation(text, min_length=10, max_length=50):\n",
    "    \"\"\"\n",
    "    兼容性函數 - 調用新的句子切分函數\n",
    "    \"\"\"\n",
    "    return split_text_to_sentences(text, min_length, max_length)\n",
    "\n",
    "def process_text_splitting(df, text_column='text', min_length=10, max_length=50):\n",
    "    \"\"\"\n",
    "    處理整個資料集的文本切分 - 句子級別切分\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): 原始資料集\n",
    "        text_column (str): 文本欄位名稱\n",
    "        min_length (int): 最小句子片段長度\n",
    "        max_length (int): 最大句子片段長度\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: 切分後的資料集\n",
    "    \"\"\"\n",
    "    print(f\"📊 開始處理文本切分...\")\n",
    "    print(f\"  原始資料筆數: {len(df)}\")\n",
    "    print(f\"  文本欄位: {text_column}\")\n",
    "    print(f\"  句子片段長度範圍: {min_length}-{max_length} 字\")\n",
    "    \n",
    "    split_data = []\n",
    "    total_fragments = 0\n",
    "    processed_texts = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"切分進度\"):\n",
    "        original_text = row[text_column]\n",
    "        \n",
    "        # 跳過太短的文本\n",
    "        if len(original_text) < min_length:\n",
    "            continue\n",
    "        \n",
    "        # 切分文本為句子片段\n",
    "        fragments = split_text_to_sentences(original_text, min_length, max_length)\n",
    "        \n",
    "        # 為每個片段創建新記錄\n",
    "        for frag_idx, fragment in enumerate(fragments):\n",
    "            new_row = row.copy()\n",
    "            new_row[text_column] = fragment\n",
    "            new_row['original_index'] = idx\n",
    "            new_row['fragment_index'] = frag_idx\n",
    "            new_row['original_text_length'] = len(original_text)\n",
    "            new_row['fragment_length'] = len(fragment)\n",
    "            new_row['source_type'] = 'sentence_fragment'\n",
    "            \n",
    "            split_data.append(new_row)\n",
    "            total_fragments += 1\n",
    "        \n",
    "        processed_texts += 1\n",
    "    \n",
    "    # 創建新的DataFrame\n",
    "    split_df = pd.DataFrame(split_data)\n",
    "    \n",
    "    print(f\"\\n✅ 文本切分完成！\")\n",
    "    print(f\"📈 切分統計:\")\n",
    "    print(f\"  處理文本數: {processed_texts}\")\n",
    "    print(f\"  生成句子片段數: {total_fragments}\")\n",
    "    print(f\"  平均每文本片段數: {total_fragments/processed_texts:.1f}\")\n",
    "    \n",
    "    if not split_df.empty:\n",
    "        print(f\"\\n📏 片段長度統計:\")\n",
    "        length_stats = split_df['fragment_length'].describe()\n",
    "        print(f\"  平均長度: {length_stats['mean']:.1f} 字\")\n",
    "        print(f\"  最短片段: {length_stats['min']:.0f} 字\")\n",
    "        print(f\"  最長片段: {length_stats['max']:.0f} 字\")\n",
    "        print(f\"  中位數長度: {length_stats['50%']:.1f} 字\")\n",
    "        \n",
    "        # 長度分布\n",
    "        length_ranges = [\n",
    "            (10, 20, \"短片段\"),\n",
    "            (20, 30, \"中短片段\"),\n",
    "            (30, 40, \"中等片段\"),\n",
    "            (40, 50, \"長片段\"),\n",
    "            (50, 100, \"超長片段\")\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n📊 片段長度分布:\")\n",
    "        for min_len, max_len, desc in length_ranges:\n",
    "            count = len(split_df[(split_df['fragment_length'] >= min_len) & \n",
    "                                (split_df['fragment_length'] < max_len)])\n",
    "            percentage = count / len(split_df) * 100\n",
    "            print(f\"  {desc} ({min_len}-{max_len}字): {count} 個 ({percentage:.1f}%)\")\n",
    "    \n",
    "    return split_df\n",
    "\n",
    "# 執行文本切分\n",
    "if 'dataset_df' in globals() and dataset_df is not None:\n",
    "    print(f\"🎯 對載入的資料集進行句子級別文本切分...\")\n",
    "    \n",
    "    # 設定切分參數 - 改為句子級別\n",
    "    MIN_FRAGMENT_LENGTH = 10   # 最小片段長度\n",
    "    MAX_FRAGMENT_LENGTH = 50   # 最大片段長度\n",
    "    \n",
    "    print(f\"\\n⚙️ 切分參數:\")\n",
    "    print(f\"  最小片段長度: {MIN_FRAGMENT_LENGTH} 字\")\n",
    "    print(f\"  最大片段長度: {MAX_FRAGMENT_LENGTH} 字\")\n",
    "    print(f\"  切分模式: 句子級別\")\n",
    "    \n",
    "    # 執行切分\n",
    "    split_dataset_df = process_text_splitting(\n",
    "        df=dataset_df, \n",
    "        text_column='text',\n",
    "        min_length=MIN_FRAGMENT_LENGTH,\n",
    "        max_length=MAX_FRAGMENT_LENGTH\n",
    "    )\n",
    "    \n",
    "    if not split_dataset_df.empty:\n",
    "        # 顯示切分範例\n",
    "        print(f\"\\n📝 切分範例:\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # 找一個有多個片段的原始文本\n",
    "        sample_original_idx = split_dataset_df['original_index'].value_counts().index[0]\n",
    "        sample_fragments = split_dataset_df[split_dataset_df['original_index'] == sample_original_idx]\n",
    "        \n",
    "        print(f\"原始文本 #{sample_original_idx} 被切分為 {len(sample_fragments)} 個句子片段:\")\n",
    "        print()\n",
    "        \n",
    "        # 顯示原始文本\n",
    "        original_text = dataset_df.iloc[sample_original_idx]['text']\n",
    "        print(f\"原始文本: {original_text[:200]}{'...' if len(original_text) > 200 else ''}\")\n",
    "        print()\n",
    "        print(\"切分結果:\")\n",
    "        \n",
    "        for i, (_, row) in enumerate(sample_fragments.iterrows()):\n",
    "            fragment = row['text']\n",
    "            length = row['fragment_length']\n",
    "            print(f\"片段 {i+1} ({length}字): {fragment}\")\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        # 儲存切分後的資料集\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        split_dir = \"split_datasets\"\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        # 儲存為多種格式\n",
    "        base_filename = f\"{split_dir}/sentence_fragments_{timestamp}\"\n",
    "        \n",
    "        # CSV 格式\n",
    "        csv_filename = f\"{base_filename}.csv\"\n",
    "        split_dataset_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        # JSON 格式\n",
    "        json_filename = f\"{base_filename}.json\"\n",
    "        split_dataset_df.to_json(json_filename, orient='records', force_ascii=False, indent=2)\n",
    "        \n",
    "        # Parquet 格式\n",
    "        parquet_filename = f\"{base_filename}.parquet\"\n",
    "        split_dataset_df.to_parquet(parquet_filename, index=False)\n",
    "        \n",
    "        print(f\"\\n💾 句子片段資料集已儲存:\")\n",
    "        print(f\"  📄 CSV: {csv_filename}\")\n",
    "        print(f\"  📋 JSON: {json_filename}\")\n",
    "        print(f\"  📦 Parquet: {parquet_filename}\")\n",
    "        \n",
    "        # 檔案大小統計\n",
    "        print(f\"\\n📁 檔案大小:\")\n",
    "        for filename in [csv_filename, json_filename, parquet_filename]:\n",
    "            if os.path.exists(filename):\n",
    "                size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "                print(f\"  {os.path.basename(filename)}: {size_mb:.2f} MB\")\n",
    "        \n",
    "        # 儲存到全域變數\n",
    "        globals()['split_dataset_df'] = split_dataset_df\n",
    "        \n",
    "        print(f\"\\n🎉 句子級別文本切分處理完成！\")\n",
    "        print(f\"📋 變數名稱: split_dataset_df\")\n",
    "        print(f\"🎯 句子片段資料集可用於後續的 LLM 處理！\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ 切分後沒有產生有效片段，請檢查原始資料\")\n",
    "        split_dataset_df = None\n",
    "\n",
    "else:\n",
    "    print(\"❌ 沒有找到資料集，請先執行 Get Data 部分\")\n",
    "    split_dataset_df = None\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525fac3d",
   "metadata": {},
   "source": [
    "## LLM AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc2c275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 啟動最終版大陸用語識別系統...\n",
      "============================================================\n",
      "❌ 沒有找到可用的資料集\n",
      "💡 請先執行前面的資料載入、文本切分或擴增步驟\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 🎯 最終版大陸用語識別與篩選系統 - 使用 Ollama 推論並儲存結果\n",
    "print(\"🚀 啟動最終版大陸用語識別系統...\")\n",
    "\n",
    "# 定義大陸特有詞彙庫\n",
    "mainland_terms = {\n",
    "    \"計算機\": [\"電腦\"], \"軟件\": [\"軟體\"], \"硬件\": [\"硬體\"], \"網絡\": [\"網路\"], \n",
    "    \"數據\": [\"資料\"], \"程序\": [\"程式\"], \"信息\": [\"資訊\"], \"出租車\": [\"計程車\"],\n",
    "    \"公交車\": [\"公車\"], \"地鐵\": [\"捷運\"], \"質量\": [\"品質\"], \"服務員\": [\"服務生\"],\n",
    "    \"土豆\": [\"馬鈴薯\"], \"西紅柿\": [\"番茄\"], \"搞定\": [\"完成\"], \"挺\": [\"很\"],\n",
    "    \"咋\": [\"怎麼\"], \"啥\": [\"什麼\"], \"微信\": [\"\"], \"支付寶\": [\"\"], \"淘寶\": [\"\"]\n",
    "}\n",
    "\n",
    "# 大陸語法模式\n",
    "mainland_patterns = [r\"挺.*的\", r\"蠻.*的\", r\".*得很\", r\"咋.*\", r\"啥.*\"]\n",
    "\n",
    "def analyze_features(text):\n",
    "    \"\"\"快速特徵分析\"\"\"\n",
    "    mainland_count = sum(1 for term in mainland_terms if term in text)\n",
    "    pattern_count = sum(1 for pattern in mainland_patterns if re.search(pattern, text))\n",
    "    return {\n",
    "        \"mainland_terms\": [term for term in mainland_terms if term in text],\n",
    "        \"pattern_matches\": pattern_count,\n",
    "        \"authenticity_score\": mainland_count + pattern_count\n",
    "    }\n",
    "\n",
    "def mainland_score_ollama(text, model=\"gpt-oss:20b\"):\n",
    "    \"\"\"使用 Ollama 評分大陸用語特徵\"\"\"\n",
    "    prompt = f\"\"\"評估文本的大陸用語特徵，每項0或1分：\n",
    "\n",
    "文本：{text}\n",
    "\n",
    "評分標準：\n",
    "1. 大陸特有詞彙：計算機、軟件、出租車、地鐵等\n",
    "2. 大陸語法習慣：挺...的、蠻...的、咋樣等  \n",
    "3. 大陸口語表達：搞定、整、弄等\n",
    "4. 避免繁體用語：不含電腦、軟體、資料等\n",
    "5. 整體大陸化程度：綜合評估\n",
    "\n",
    "請按格式回答：\n",
    "大陸特有詞彙:0\n",
    "大陸語法習慣:0\n",
    "大陸口語表達:0\n",
    "避免繁體用語:1\n",
    "整體大陸化程度:0\n",
    "總分:1\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options={'temperature': 0.1, 'max_tokens': 100}\n",
    "        )\n",
    "        \n",
    "        # 解析回應\n",
    "        scores = {}\n",
    "        total = 0\n",
    "        categories = [\"大陸特有詞彙\", \"大陸語法習慣\", \"大陸口語表達\", \"避免繁體用語\", \"整體大陸化程度\"]\n",
    "        \n",
    "        for line in response['response'].split('\\n'):\n",
    "            for cat in categories:\n",
    "                if cat in line:\n",
    "                    match = re.search(r'[：:]\\s*([01])', line)\n",
    "                    if match:\n",
    "                        score = int(match.group(1))\n",
    "                        scores[cat] = score\n",
    "                        total += score\n",
    "        \n",
    "        if len(scores) == 5:\n",
    "            scores[\"總分\"] = total\n",
    "            return scores, response['response']\n",
    "        return None, response['response']\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def process_dataset(df, text_col='text', sample_size=100, threshold=3):\n",
    "    \"\"\"處理資料集進行大陸用語篩選\"\"\"\n",
    "    \n",
    "    print(f\"📊 處理資料集：{len(df)} 筆記錄\")\n",
    "    print(f\"📝 文本欄位：{text_col}\")\n",
    "    print(f\"🎯 樣本大小：{sample_size}\")\n",
    "    print(f\"⚖️ 篩選閾值：{threshold}/5\")\n",
    "    \n",
    "    # 取樣本\n",
    "    sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    texts = sample_df[text_col].tolist()\n",
    "    \n",
    "    # 執行推論\n",
    "    results = []\n",
    "    authentic_texts = []\n",
    "    generic_texts = []\n",
    "    \n",
    "    print(f\"\\n🔄 開始 Ollama 推論...\")\n",
    "    \n",
    "    for i, text in enumerate(tqdm(texts, desc=\"推論進度\")):\n",
    "        # 特徵分析\n",
    "        features = analyze_features(text)\n",
    "        \n",
    "        # Ollama 評分\n",
    "        scores, response = mainland_score_ollama(text)\n",
    "        \n",
    "        result = {\n",
    "            'index': i,\n",
    "            'text': text,\n",
    "            'text_length': len(text),\n",
    "            'features': features,\n",
    "            'scores': scores,\n",
    "            'response': response,\n",
    "            'success': scores is not None\n",
    "        }\n",
    "        \n",
    "        # 分類\n",
    "        if scores and scores.get(\"總分\", 0) >= threshold:\n",
    "            authentic_texts.append(result)\n",
    "            category = \"真正大陸用語\"\n",
    "        else:\n",
    "            generic_texts.append(result)\n",
    "            category = \"通用簡體中文\"\n",
    "        \n",
    "        result['category'] = category\n",
    "        results.append(result)\n",
    "        \n",
    "        # 顯示進度\n",
    "        if i % 20 == 0 or i < 3:\n",
    "            score_str = f\"{scores['總分']}/5\" if scores else \"失敗\"\n",
    "            print(f\"  第{i+1}筆: {score_str} - {category}\")\n",
    "        \n",
    "        time.sleep(0.2)  # 控制請求頻率\n",
    "    \n",
    "    return results, authentic_texts, generic_texts\n",
    "\n",
    "def save_results(results, authentic_texts, generic_texts):\n",
    "    \"\"\"儲存篩選結果 - 支援切分資料格式\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. 完整結果\n",
    "    full_data = []\n",
    "    for r in results:\n",
    "        row = {\n",
    "            'text': r['text'],\n",
    "            'text_length': r['text_length'],\n",
    "            'category': r['category'],\n",
    "            'success': r['success'],\n",
    "            'authenticity_score': r['features']['authenticity_score'],\n",
    "            'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "        }\n",
    "        \n",
    "        # 添加切分相關欄位（如果存在）\n",
    "        original_row = available_data.iloc[r['index']]\n",
    "        if 'source_type' in original_row:\n",
    "            row['source_type'] = original_row['source_type']\n",
    "        if 'source' in original_row:\n",
    "            row['source'] = original_row['source']\n",
    "        if 'fragment_length' in original_row:\n",
    "            row['fragment_length'] = original_row['fragment_length']\n",
    "        if 'augmentation_method' in original_row:\n",
    "            row['augmentation_method'] = original_row['augmentation_method']\n",
    "        \n",
    "        if r['scores']:\n",
    "            row.update({f'score_{k}': v for k, v in r['scores'].items()})\n",
    "        full_data.append(row)\n",
    "    \n",
    "    full_df = pd.DataFrame(full_data)\n",
    "    full_file = f\"mainland_filtering_complete_{timestamp}.csv\"\n",
    "    full_df.to_csv(full_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. 高質量大陸用語數據（切分格式）\n",
    "    if authentic_texts:\n",
    "        authentic_data = []\n",
    "        for r in authentic_texts:\n",
    "            original_row = available_data.iloc[r['index']]\n",
    "            auth_row = {\n",
    "                'text': r['text'],\n",
    "                'total_score': r['scores']['總分'],\n",
    "                'mainland_terms': ','.join(r['features']['mainland_terms']),\n",
    "                'text_length': r['text_length']\n",
    "            }\n",
    "            \n",
    "            # 保留切分相關欄位\n",
    "            if 'source_type' in original_row:\n",
    "                auth_row['source_type'] = original_row['source_type']\n",
    "            if 'source' in original_row:\n",
    "                auth_row['source'] = original_row['source']\n",
    "            if 'fragment_length' in original_row:\n",
    "                auth_row['fragment_length'] = original_row['fragment_length']\n",
    "            if 'augmentation_method' in original_row:\n",
    "                auth_row['augmentation_method'] = original_row['augmentation_method']\n",
    "            if 'original_idx' in original_row:\n",
    "                auth_row['original_idx'] = original_row['original_idx']\n",
    "            if 'fragment_index' in original_row:\n",
    "                auth_row['fragment_index'] = original_row['fragment_index']\n",
    "            \n",
    "            authentic_data.append(auth_row)\n",
    "        \n",
    "        auth_df = pd.DataFrame(authentic_data)\n",
    "        auth_csv = f\"authentic_mainland_texts_{timestamp}.csv\"\n",
    "        auth_json = f\"authentic_mainland_texts_{timestamp}.json\"\n",
    "        \n",
    "        auth_df.to_csv(auth_csv, index=False, encoding='utf-8-sig')\n",
    "        auth_df.to_json(auth_json, orient='records', force_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"💾 儲存完成:\")\n",
    "        print(f\"  📄 完整結果: {full_file}\")\n",
    "        print(f\"  ✅ 高質量句子片段數據: {auth_csv}\")\n",
    "        print(f\"  📋 JSON格式: {auth_json}\")\n",
    "        \n",
    "        # 顯示切分資料統計\n",
    "        if 'source' in auth_df.columns:\n",
    "            print(f\"\\n📊 高質量數據來源分布:\")\n",
    "            print(auth_df['source'].value_counts())\n",
    "        \n",
    "        return full_df, auth_df\n",
    "    \n",
    "    return full_df, None\n",
    "\n",
    "# 主要執行流程\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 檢查可用資料集 (優先使用最終切分句子片段資料集)\n",
    "available_data = None\n",
    "text_column = 'text'\n",
    "\n",
    "if 'final_split_augmented_df' in locals() and final_split_augmented_df is not None:\n",
    "    available_data = final_split_augmented_df\n",
    "    source_name = \"最終句子片段擴增資料集\"\n",
    "elif 'split_dataset_df' in locals() and split_dataset_df is not None:\n",
    "    available_data = split_dataset_df\n",
    "    source_name = \"句子片段資料集\"\n",
    "elif 'optimized_augmented_df' in locals() and optimized_augmented_df is not None:\n",
    "    available_data = optimized_augmented_df\n",
    "    source_name = \"優化擴增資料集\"\n",
    "elif 'dataset_df' in locals() and dataset_df is not None:\n",
    "    available_data = dataset_df  \n",
    "    source_name = \"原始資料集\"\n",
    "\n",
    "if available_data is not None:\n",
    "    print(f\"✅ 使用 {source_name}，共 {len(available_data)} 筆記錄\")\n",
    "    \n",
    "    # 執行篩選（可調整參數）\n",
    "    SAMPLE_SIZE = 50    # 處理樣本數量\n",
    "    THRESHOLD = 3       # 篩選閾值\n",
    "    \n",
    "    print(f\"\\n🎯 開始執行大陸用語篩選...\")\n",
    "    results, authentic_results, generic_results = process_dataset(\n",
    "        df=available_data,\n",
    "        text_col=text_column,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        threshold=THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # 統計結果\n",
    "    print(f\"\\n📊 篩選結果統計:\")\n",
    "    print(f\"  ✅ 真正大陸用語: {len(authentic_results)} 筆\")\n",
    "    print(f\"  🗑️ 通用簡體中文: {len(generic_results)} 筆\")\n",
    "    print(f\"  📈 篩選率: {len(authentic_results)/len(results)*100:.1f}%\")\n",
    "    \n",
    "    # 顯示範例\n",
    "    if authentic_results:\n",
    "        print(f\"\\n📝 高質量大陸用語範例:\")\n",
    "        for i, r in enumerate(authentic_results[:3]):\n",
    "            preview = r['text'][:60] + \"...\" if len(r['text']) > 60 else r['text']\n",
    "            print(f\"  {i+1}. (得分:{r['scores']['總分']}) {preview}\")\n",
    "    \n",
    "    # 儲存結果\n",
    "    print(f\"\\n💾 儲存結果...\")\n",
    "    full_df, auth_df = save_results(results, authentic_results, generic_results)\n",
    "    \n",
    "    # 設定全域變數\n",
    "    globals()['mainland_filtering_results'] = results\n",
    "    globals()['authentic_mainland_data'] = auth_df\n",
    "    \n",
    "    print(f\"\\n🎉 大陸用語識別與篩選完成！\")\n",
    "    print(f\"📋 可用變數: mainland_filtering_results, authentic_mainland_data\")\n",
    "    print(f\"🎯 最終輸出為句子級別的片段資料 (10-50字)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 沒有找到可用的資料集\")\n",
    "    print(\"💡 請先執行前面的資料載入、文本切分或擴增步驟\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048a5fb",
   "metadata": {},
   "source": [
    "## 📋 句子級別切分示例\n",
    "\n",
    "以下是新的句子級別切分功能的演示。原文本會被切分成10-50字的句子片段，適合進行更精細的大陸用語分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674bad99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 句子級別切分功能演示\n",
      "📝 原始文本 (197字):\n",
      "如今，不管是在大型商超还是巷子里的小铺，随处可见各种扫码支付的身影。而随着扫码支付线下布局的逐渐广泛和已培育成熟的用户消费习惯，使得近期扫码支付风潮颇有横扫支付市场的势头。不管你是商家和个人都可以开通无卡支付！对此，优壹付方面负责人表示：\"随着费率改革的正式推行，使得无卡支付、扫码支付进入了全国爆发期。目前，扫码支付市场也激战正酣，越来越多的企业、代理商以及个体都在进军移动支付战场抢滩分食。\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'split_text_to_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 執行句子級別切分\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m fragments = \u001b[43msplit_text_to_sentences\u001b[49m(example_text, min_length=\u001b[32m10\u001b[39m, max_length=\u001b[32m50\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🔪 切分結果 (共\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(fragments)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m個片段):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'split_text_to_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# 📋 句子級別切分示例演示\n",
    "print(\"🔍 句子級別切分功能演示\")\n",
    "\n",
    "# 示例文本（您提到的例子）\n",
    "example_text = \"\"\"如今，不管是在大型商超还是巷子里的小铺，随处可见各种扫码支付的身影。而随着扫码支付线下布局的逐渐广泛和已培育成熟的用户消费习惯，使得近期扫码支付风潮颇有横扫支付市场的势头。不管你是商家和个人都可以开通无卡支付！对此，优壹付方面负责人表示：\"随着费率改革的正式推行，使得无卡支付、扫码支付进入了全国爆发期。目前，扫码支付市场也激战正酣，越来越多的企业、代理商以及个体都在进军移动支付战场抢滩分食。\"\"\"\n",
    "\n",
    "print(f\"📝 原始文本 ({len(example_text)}字):\")\n",
    "print(f\"{example_text}\")\n",
    "print()\n",
    "\n",
    "# 執行句子級別切分\n",
    "fragments = split_text_to_sentences(example_text, min_length=10, max_length=50)\n",
    "\n",
    "print(f\"🔪 切分結果 (共{len(fragments)}個片段):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, fragment in enumerate(fragments, 1):\n",
    "    print(f\"片段 {i} ({len(fragment)}字): {fragment}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\n📊 切分統計:\")\n",
    "print(f\"  原始文本長度: {len(example_text)} 字\")\n",
    "print(f\"  生成片段數: {len(fragments)}\")\n",
    "print(f\"  平均片段長度: {sum(len(f) for f in fragments) / len(fragments):.1f} 字\")\n",
    "print(f\"  最短片段: {min(len(f) for f in fragments)} 字\")\n",
    "print(f\"  最長片段: {max(len(f) for f in fragments)} 字\")\n",
    "\n",
    "print(f\"\\n🎯 現在每個片段都可以獨立進行大陸用語分析！\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94844d",
   "metadata": {},
   "source": [
    "## 📁 從大文件挑選資料\n",
    "\n",
    "從指定的大文件中隨機挑選250筆資料進行評分處理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef2f5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 從大文件中挑選資料...\n",
      "📂 目標文件: /Users/edwardhuang/Documents/GitHub/LLM_Aug/data/CLUECorpusSmall.txt\n",
      "🎯 目標樣本數: 250\n",
      "📊 正在計算文件總行數...\n",
      "  已計算: 100,000 行\n",
      "  已計算: 200,000 行\n",
      "  已計算: 300,000 行\n",
      "  已計算: 400,000 行\n",
      "  已計算: 500,000 行\n",
      "  已計算: 600,000 行\n",
      "  已計算: 700,000 行\n",
      "  已計算: 800,000 行\n",
      "  已計算: 900,000 行\n",
      "  已計算: 1,000,000 行\n",
      "  已計算: 1,100,000 行\n",
      "  已計算: 1,200,000 行\n",
      "  已計算: 1,300,000 行\n",
      "  已計算: 1,400,000 行\n",
      "  已計算: 1,500,000 行\n",
      "  已計算: 1,600,000 行\n",
      "  已計算: 1,700,000 行\n",
      "  已計算: 1,800,000 行\n",
      "  已計算: 1,900,000 行\n",
      "  已計算: 2,000,000 行\n",
      "  已計算: 2,100,000 行\n",
      "  已計算: 2,200,000 行\n",
      "  已計算: 2,300,000 行\n",
      "  已計算: 2,400,000 行\n",
      "  已計算: 2,500,000 行\n",
      "  已計算: 2,600,000 行\n",
      "  已計算: 2,700,000 行\n",
      "  已計算: 2,800,000 行\n",
      "  已計算: 2,900,000 行\n",
      "  已計算: 3,000,000 行\n",
      "  已計算: 3,100,000 行\n",
      "  已計算: 3,200,000 行\n",
      "  已計算: 3,300,000 行\n",
      "  已計算: 3,400,000 行\n",
      "  已計算: 3,500,000 行\n",
      "  已計算: 3,600,000 行\n",
      "  已計算: 3,700,000 行\n",
      "  已計算: 3,800,000 行\n",
      "  已計算: 3,900,000 行\n",
      "  已計算: 4,000,000 行\n",
      "  已計算: 4,100,000 行\n",
      "  已計算: 4,200,000 行\n",
      "  已計算: 4,300,000 行\n",
      "  已計算: 4,400,000 行\n",
      "  已計算: 4,500,000 行\n",
      "  已計算: 4,600,000 行\n",
      "  已計算: 4,700,000 行\n",
      "  已計算: 4,800,000 行\n",
      "  已計算: 4,900,000 行\n",
      "  已計算: 5,000,000 行\n",
      "  已計算: 5,100,000 行\n",
      "  已計算: 5,200,000 行\n",
      "  已計算: 5,300,000 行\n",
      "  已計算: 5,400,000 行\n",
      "  已計算: 5,500,000 行\n",
      "  已計算: 5,600,000 行\n",
      "  已計算: 5,700,000 行\n",
      "  已計算: 5,800,000 行\n",
      "  已計算: 5,900,000 行\n",
      "  已計算: 6,000,000 行\n",
      "  已計算: 6,100,000 行\n",
      "  已計算: 6,200,000 行\n",
      "  已計算: 6,300,000 行\n",
      "  已計算: 6,400,000 行\n",
      "  已計算: 6,500,000 行\n",
      "  已計算: 6,600,000 行\n",
      "  已計算: 6,700,000 行\n",
      "  已計算: 6,800,000 行\n",
      "  已計算: 6,900,000 行\n",
      "  已計算: 7,000,000 行\n",
      "  已計算: 7,100,000 行\n",
      "  已計算: 7,200,000 行\n",
      "  已計算: 7,300,000 行\n",
      "  已計算: 7,400,000 行\n",
      "  已計算: 7,500,000 行\n",
      "  已計算: 7,600,000 行\n",
      "  已計算: 7,700,000 行\n",
      "  已計算: 7,800,000 行\n",
      "  已計算: 7,900,000 行\n",
      "  已計算: 8,000,000 行\n",
      "  已計算: 8,100,000 行\n",
      "  已計算: 8,200,000 行\n",
      "  已計算: 8,300,000 行\n",
      "  已計算: 8,400,000 行\n",
      "  已計算: 8,500,000 行\n",
      "  已計算: 8,600,000 行\n",
      "  已計算: 8,700,000 行\n",
      "  已計算: 8,800,000 行\n",
      "  已計算: 8,900,000 行\n",
      "  已計算: 9,000,000 行\n",
      "  已計算: 9,100,000 行\n",
      "  已計算: 9,200,000 行\n",
      "  已計算: 9,300,000 行\n",
      "  已計算: 9,400,000 行\n",
      "  已計算: 9,500,000 行\n",
      "  已計算: 9,600,000 行\n",
      "  已計算: 9,700,000 行\n",
      "  已計算: 9,800,000 行\n",
      "  已計算: 9,900,000 行\n",
      "  已計算: 10,000,000 行\n",
      "  已計算: 10,100,000 行\n",
      "  已計算: 10,200,000 行\n",
      "  已計算: 10,300,000 行\n",
      "  已計算: 10,400,000 行\n",
      "  已計算: 10,500,000 行\n",
      "  已計算: 10,600,000 行\n",
      "  已計算: 10,700,000 行\n",
      "  已計算: 10,800,000 行\n",
      "  已計算: 10,900,000 行\n",
      "  已計算: 11,000,000 行\n",
      "  已計算: 11,100,000 行\n",
      "  已計算: 11,200,000 行\n",
      "  已計算: 11,300,000 行\n",
      "  已計算: 11,400,000 行\n",
      "  已計算: 11,500,000 行\n",
      "  已計算: 11,600,000 行\n",
      "  已計算: 11,700,000 行\n",
      "  已計算: 11,800,000 行\n",
      "  已計算: 11,900,000 行\n",
      "  已計算: 12,000,000 行\n",
      "  已計算: 12,100,000 行\n",
      "  已計算: 12,200,000 行\n",
      "  已計算: 12,300,000 行\n",
      "  已計算: 12,400,000 行\n",
      "  已計算: 12,500,000 行\n",
      "  已計算: 12,600,000 行\n",
      "  已計算: 12,700,000 行\n",
      "  已計算: 12,800,000 行\n",
      "  已計算: 12,900,000 行\n",
      "  已計算: 13,000,000 行\n",
      "  已計算: 13,100,000 行\n",
      "  已計算: 13,200,000 行\n",
      "  已計算: 13,300,000 行\n",
      "  已計算: 13,400,000 行\n",
      "  已計算: 13,500,000 行\n",
      "  已計算: 13,600,000 行\n",
      "  已計算: 13,700,000 行\n",
      "  已計算: 13,800,000 行\n",
      "  已計算: 13,900,000 行\n",
      "  已計算: 14,000,000 行\n",
      "  已計算: 14,100,000 行\n",
      "  已計算: 14,200,000 行\n",
      "  已計算: 14,300,000 行\n",
      "  已計算: 14,400,000 行\n",
      "  已計算: 14,500,000 行\n",
      "  已計算: 14,600,000 行\n",
      "  已計算: 14,700,000 行\n",
      "  已計算: 14,800,000 行\n",
      "  已計算: 14,900,000 行\n",
      "  已計算: 15,000,000 行\n",
      "  已計算: 15,100,000 行\n",
      "  已計算: 15,200,000 行\n",
      "  已計算: 15,300,000 行\n",
      "  已計算: 15,400,000 行\n",
      "  已計算: 15,500,000 行\n",
      "  已計算: 15,600,000 行\n",
      "  已計算: 15,700,000 行\n",
      "  已計算: 15,800,000 行\n",
      "  已計算: 15,900,000 行\n",
      "  已計算: 16,000,000 行\n",
      "  已計算: 16,100,000 行\n",
      "  已計算: 16,200,000 行\n",
      "  已計算: 16,300,000 行\n",
      "  已計算: 16,400,000 行\n",
      "  已計算: 16,500,000 行\n",
      "  已計算: 16,600,000 行\n",
      "📈 文件總行數: 16,608,422\n",
      "🎲 隨機挑選 250 行...\n",
      "✅ 已生成 250 個隨機行號\n",
      "📖 正在讀取選中的行...\n",
      "  已挑選: 50 筆\n",
      "  已挑選: 100 筆\n",
      "  已挑選: 150 筆\n",
      "  已挑選: 200 筆\n",
      "  已挑選: 250 筆\n",
      "\n",
      "✅ 資料挑選完成！\n",
      "📊 成功挑選: 250 筆資料\n",
      "❌ 處理過程中發生錯誤: name 'pd' is not defined\n",
      "💡 請檢查文件路徑是否正確\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 📁 從大文件中挑選250筆資料進行評分\n",
    "print(\"🔍 從大文件中挑選資料...\")\n",
    "\n",
    "import random\n",
    "\n",
    "# 文件路徑\n",
    "file_path = \"/Users/edwardhuang/Documents/GitHub/LLM_Aug/data/CLUECorpusSmall.txt\"\n",
    "target_samples = 250\n",
    "\n",
    "print(f\"📂 目標文件: {file_path}\")\n",
    "print(f\"🎯 目標樣本數: {target_samples}\")\n",
    "\n",
    "def count_lines_in_file(file_path):\n",
    "    \"\"\"計算文件總行數\"\"\"\n",
    "    print(\"📊 正在計算文件總行數...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            if count % 100000 == 0:\n",
    "                print(f\"  已計算: {count:,} 行\")\n",
    "        return count\n",
    "\n",
    "def sample_lines_from_large_file(file_path, num_samples, total_lines=None):\n",
    "    \"\"\"從大文件中隨機挑選指定數量的行\"\"\"\n",
    "    \n",
    "    # 如果沒有提供總行數，先計算\n",
    "    if total_lines is None:\n",
    "        total_lines = count_lines_in_file(file_path)\n",
    "    \n",
    "    print(f\"📈 文件總行數: {total_lines:,}\")\n",
    "    print(f\"🎲 隨機挑選 {num_samples} 行...\")\n",
    "    \n",
    "    # 生成隨機行號\n",
    "    if num_samples >= total_lines:\n",
    "        print(\"⚠️  目標樣本數大於等於總行數，將返回所有行\")\n",
    "        selected_lines = list(range(total_lines))\n",
    "    else:\n",
    "        selected_lines = sorted(random.sample(range(total_lines), num_samples))\n",
    "    \n",
    "    print(f\"✅ 已生成 {len(selected_lines)} 個隨機行號\")\n",
    "    \n",
    "    # 讀取選中的行\n",
    "    selected_data = []\n",
    "    current_line = 0\n",
    "    next_target_idx = 0\n",
    "    \n",
    "    print(\"📖 正在讀取選中的行...\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if next_target_idx < len(selected_lines) and current_line == selected_lines[next_target_idx]:\n",
    "                # 清理文本\n",
    "                clean_text = line.strip()\n",
    "                if clean_text:  # 只保留非空行\n",
    "                    selected_data.append({\n",
    "                        'line_number': current_line,\n",
    "                        'text': clean_text,\n",
    "                        'text_length': len(clean_text)\n",
    "                    })\n",
    "                next_target_idx += 1\n",
    "                \n",
    "                # 顯示進度\n",
    "                if len(selected_data) % 50 == 0:\n",
    "                    print(f\"  已挑選: {len(selected_data)} 筆\")\n",
    "                \n",
    "                # 如果已經找到所有目標行，可以提前結束\n",
    "                if next_target_idx >= len(selected_lines):\n",
    "                    break\n",
    "            \n",
    "            current_line += 1\n",
    "    \n",
    "    return selected_data\n",
    "\n",
    "try:\n",
    "    # 設定隨機種子以確保可重現性\n",
    "    random.seed(42)\n",
    "    \n",
    "    # 挑選資料\n",
    "    selected_data = sample_lines_from_large_file(file_path, target_samples)\n",
    "    \n",
    "    print(f\"\\n✅ 資料挑選完成！\")\n",
    "    print(f\"📊 成功挑選: {len(selected_data)} 筆資料\")\n",
    "    \n",
    "    if selected_data:\n",
    "        # 轉換為DataFrame\n",
    "        selected_df = pd.DataFrame(selected_data)\n",
    "        \n",
    "        # 顯示基本統計\n",
    "        print(f\"\\n📈 文本長度統計:\")\n",
    "        length_stats = selected_df['text_length'].describe()\n",
    "        for stat_name, value in length_stats.items():\n",
    "            print(f\"  {stat_name}: {value:.1f}\")\n",
    "        \n",
    "        # 顯示前幾筆範例\n",
    "        print(f\"\\n📝 前5筆資料範例:\")\n",
    "        for i in range(min(5, len(selected_df))):\n",
    "            row = selected_df.iloc[i]\n",
    "            text = row['text']\n",
    "            preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            print(f\"第{i+1}筆 (行號:{row['line_number']}, {row['text_length']}字): {preview}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # 儲存挑選的資料\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        save_dir = \"selected_data\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # 儲存為多種格式\n",
    "        base_filename = f\"{save_dir}/selected_250_samples_{timestamp}\"\n",
    "        \n",
    "        csv_filename = f\"{base_filename}.csv\"\n",
    "        json_filename = f\"{base_filename}.json\"\n",
    "        parquet_filename = f\"{base_filename}.parquet\"\n",
    "        \n",
    "        selected_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "        selected_df.to_json(json_filename, orient='records', force_ascii=False, indent=2)\n",
    "        selected_df.to_parquet(parquet_filename, index=False)\n",
    "        \n",
    "        print(f\"\\n💾 資料已儲存:\")\n",
    "        print(f\"  📄 CSV: {csv_filename}\")\n",
    "        print(f\"  📋 JSON: {json_filename}\")\n",
    "        print(f\"  📦 Parquet: {parquet_filename}\")\n",
    "        \n",
    "        # 檔案大小\n",
    "        for filename in [csv_filename, json_filename, parquet_filename]:\n",
    "            if os.path.exists(filename):\n",
    "                size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "                print(f\"    {os.path.basename(filename)}: {size_mb:.2f} MB\")\n",
    "        \n",
    "        # 設定全域變數\n",
    "        globals()['selected_large_file_df'] = selected_df\n",
    "        \n",
    "        print(f\"\\n🎉 250筆資料挑選完成！\")\n",
    "        print(f\"📋 可用變數: selected_large_file_df\")\n",
    "        print(f\"🎯 現在可以使用這些資料進行句子切分和大陸用語評分！\")\n",
    "        \n",
    "        # 長度分布分析\n",
    "        print(f\"\\n📊 文本長度分布:\")\n",
    "        length_ranges = [\n",
    "            (0, 50, \"短文本\"),\n",
    "            (50, 100, \"中短文本\"),\n",
    "            (100, 200, \"中等文本\"),\n",
    "            (200, 500, \"長文本\"),\n",
    "            (500, 1000, \"很長文本\"),\n",
    "            (1000, float('inf'), \"超長文本\")\n",
    "        ]\n",
    "        \n",
    "        for min_len, max_len, desc in length_ranges:\n",
    "            if max_len == float('inf'):\n",
    "                count = len(selected_df[selected_df['text_length'] >= min_len])\n",
    "            else:\n",
    "                count = len(selected_df[(selected_df['text_length'] >= min_len) & \n",
    "                                      (selected_df['text_length'] < max_len)])\n",
    "            percentage = count / len(selected_df) * 100\n",
    "            print(f\"  {desc} ({min_len}+字): {count} 筆 ({percentage:.1f}%)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ 沒有成功挑選到任何資料\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 處理過程中發生錯誤: {str(e)}\")\n",
    "    print(\"💡 請檢查文件路徑是否正確\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f041f7a",
   "metadata": {},
   "source": [
    "## 🔄 處理挑選的250筆資料\n",
    "\n",
    "對挑選的資料進行句子級別切分和大陸用語評分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c88753f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 開始處理挑選的250筆資料...\n",
      "❌ 沒有找到挑選的資料\n",
      "💡 請先執行前面的資料挑選步驟\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 🔄 處理挑選的250筆資料 - 句子切分 + 大陸用語評分\n",
    "print(\"🚀 開始處理挑選的250筆資料...\")\n",
    "\n",
    "if 'selected_large_file_df' in globals() and selected_large_file_df is not None:\n",
    "    print(f\"✅ 找到挑選的資料: {len(selected_large_file_df)} 筆\")\n",
    "    \n",
    "    # Step 1: 句子級別切分\n",
    "    print(f\"\\n🔪 步驟1: 執行句子級別切分...\")\n",
    "    \n",
    "    selected_split_df = process_text_splitting(\n",
    "        df=selected_large_file_df,\n",
    "        text_column='text',\n",
    "        min_length=10,\n",
    "        max_length=50\n",
    "    )\n",
    "    \n",
    "    if not selected_split_df.empty:\n",
    "        print(f\"\\n📊 切分結果:\")\n",
    "        print(f\"  原始文本: {len(selected_large_file_df)} 筆\")\n",
    "        print(f\"  切分後句子片段: {len(selected_split_df)} 筆\")\n",
    "        print(f\"  平均每文本產生: {len(selected_split_df)/len(selected_large_file_df):.1f} 個片段\")\n",
    "        \n",
    "        # 顯示切分範例\n",
    "        print(f\"\\n📝 切分範例 (前3個原始文本):\")\n",
    "        for orig_idx in selected_split_df['original_index'].unique()[:3]:\n",
    "            fragments = selected_split_df[selected_split_df['original_index'] == orig_idx]\n",
    "            original_text = selected_large_file_df.iloc[orig_idx]['text']\n",
    "            \n",
    "            print(f\"\\n原始文本 #{orig_idx}: {original_text[:100]}{'...' if len(original_text) > 100 else ''}\")\n",
    "            print(f\"切分為 {len(fragments)} 個片段:\")\n",
    "            \n",
    "            for i, (_, row) in enumerate(fragments.iterrows()):\n",
    "                print(f\"  片段{i+1} ({row['fragment_length']}字): {row['text']}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # Step 2: 大陸用語評分\n",
    "        print(f\"\\n🎯 步驟2: 執行大陸用語評分...\")\n",
    "        \n",
    "        # 設定評分參數\n",
    "        SAMPLE_SIZE = min(100, len(selected_split_df))  # 從切分後的片段中取樣評分\n",
    "        THRESHOLD = 3  # 評分閾值\n",
    "        \n",
    "        print(f\"📊 評分參數:\")\n",
    "        print(f\"  評分樣本數: {SAMPLE_SIZE}\")\n",
    "        print(f\"  篩選閾值: {THRESHOLD}/5\")\n",
    "        \n",
    "        # 執行評分\n",
    "        results, authentic_results, generic_results = process_dataset(\n",
    "            df=selected_split_df,\n",
    "            text_col='text',\n",
    "            sample_size=SAMPLE_SIZE,\n",
    "            threshold=THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # 統計結果\n",
    "        print(f\"\\n📈 評分結果統計:\")\n",
    "        print(f\"  總評分數: {len(results)}\")\n",
    "        print(f\"  ✅ 真正大陸用語: {len(authentic_results)} 筆\")\n",
    "        print(f\"  🗑️ 通用簡體中文: {len(generic_results)} 筆\")\n",
    "        if len(results) > 0:\n",
    "            print(f\"  📊 大陸用語比例: {len(authentic_results)/len(results)*100:.1f}%\")\n",
    "        \n",
    "        # 顯示高質量範例\n",
    "        if authentic_results:\n",
    "            print(f\"\\n🏆 高質量大陸用語片段範例:\")\n",
    "            for i, r in enumerate(authentic_results[:5]):\n",
    "                print(f\"第{i+1}名 (得分:{r['scores']['總分']}/5): {r['text']}\")\n",
    "                if r['features']['mainland_terms']:\n",
    "                    print(f\"  大陸特有詞彙: {', '.join(r['features']['mainland_terms'])}\")\n",
    "                print(\"-\" * 60)\n",
    "        \n",
    "        # 儲存最終結果\n",
    "        print(f\"\\n💾 儲存最終結果...\")\n",
    "        full_df, auth_df = save_results(results, authentic_results, generic_results)\n",
    "        \n",
    "        # 創建完整處理結果摘要\n",
    "        summary_data = {\n",
    "            \"processing_summary\": {\n",
    "                \"original_samples\": len(selected_large_file_df),\n",
    "                \"split_fragments\": len(selected_split_df),\n",
    "                \"evaluated_fragments\": len(results),\n",
    "                \"high_quality_mainland\": len(authentic_results),\n",
    "                \"generic_chinese\": len(generic_results),\n",
    "                \"mainland_percentage\": len(authentic_results)/len(results)*100 if len(results) > 0 else 0\n",
    "            },\n",
    "            \"fragment_length_stats\": {\n",
    "                \"mean\": selected_split_df['fragment_length'].mean(),\n",
    "                \"min\": selected_split_df['fragment_length'].min(),\n",
    "                \"max\": selected_split_df['fragment_length'].max(),\n",
    "                \"median\": selected_split_df['fragment_length'].median()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 儲存處理摘要\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        summary_file = f\"processing_summary_{timestamp}.json\"\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"📋 處理摘要已儲存: {summary_file}\")\n",
    "        \n",
    "        # 設定全域變數\n",
    "        globals()['selected_split_df'] = selected_split_df\n",
    "        globals()['selected_mainland_results'] = results\n",
    "        globals()['selected_authentic_data'] = auth_df\n",
    "        \n",
    "        print(f\"\\n🎉 250筆資料完整處理流程完成！\")\n",
    "        print(f\"📊 最終統計:\")\n",
    "        print(f\"  原始挑選資料: {len(selected_large_file_df)} 筆\")\n",
    "        print(f\"  句子切分後: {len(selected_split_df)} 個片段\")\n",
    "        print(f\"  大陸用語評分: {len(results)} 個片段\")\n",
    "        print(f\"  高質量大陸用語: {len(authentic_results)} 個片段\")\n",
    "        print(f\"\\n📋 可用變數:\")\n",
    "        print(f\"  selected_large_file_df: 原始挑選的250筆資料\")\n",
    "        print(f\"  selected_split_df: 句子切分後的片段資料\")\n",
    "        print(f\"  selected_mainland_results: 大陸用語評分結果\")\n",
    "        print(f\"  selected_authentic_data: 高質量大陸用語片段\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ 句子切分失敗，無法進行後續處理\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 沒有找到挑選的資料\")\n",
    "    print(\"💡 請先執行前面的資料挑選步驟\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bea9398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 修正資料設定...\n",
      "✅ 找到 selected_data: 250 筆\n",
      "📊 DataFrame 建立成功: (250, 3)\n",
      "📋 欄位: ['line_number', 'text', 'text_length']\n",
      "\n",
      "📝 前3筆資料:\n",
      "第1筆: 身为一个单身小攻是什么体验？镜像问题身为一个单身小受是一种怎样的体验？1.看到长得瘦，腿很好看的男生就会一直盯着看，然后就想去摸两把。2.发誓看到喜欢的一定要去...\n",
      "第2筆: 朋友替偶庆生~~正巧在FOXTOWN吃完饭，路过这家店~~便进去了~~看了之后觉得环境比旁边的夜色好~~感觉不错正好搞活动，订了个套餐可以做个卡座~~不错不错·...\n",
      "第3筆: 错误太多影响阅读心情\n",
      "\n",
      "🔄 開始執行完整處理流程...\n",
      "\n",
      "🔪 步驟1: 執行句子級別切分...\n",
      "📊 開始處理文本切分...\n",
      "  原始資料筆數: 250\n",
      "  文本欄位: text\n",
      "  句子片段長度範圍: 10-50 字\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "切分進度: 100%|██████████| 250/250 [00:00<00:00, 263.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 文本切分完成！\n",
      "📈 切分統計:\n",
      "  處理文本數: 240\n",
      "  生成句子片段數: 2153\n",
      "  平均每文本片段數: 9.0\n",
      "\n",
      "📏 片段長度統計:\n",
      "  平均長度: 31.4 字\n",
      "  最短片段: 10 字\n",
      "  最長片段: 293 字\n",
      "  中位數長度: 31.0 字\n",
      "\n",
      "📊 片段長度分布:\n",
      "  短片段 (10-20字): 514 個 (23.9%)\n",
      "  中短片段 (20-30字): 509 個 (23.6%)\n",
      "  中等片段 (30-40字): 516 個 (24.0%)\n",
      "  長片段 (40-50字): 518 個 (24.1%)\n",
      "  超長片段 (50-100字): 84 個 (3.9%)\n",
      "\n",
      "📊 切分結果:\n",
      "  原始文本: 250 筆\n",
      "  切分後句子片段: 2153 筆\n",
      "  平均每文本產生: 8.6 個片段\n",
      "\n",
      "📝 切分範例 (第1個原始文本):\n",
      "原始文本: 身为一个单身小攻是什么体验？镜像问题身为一个单身小受是一种怎样的体验？1.看到长得瘦，腿很好看的男生就会一直盯着看，然后就想去摸两把。2.发誓看到喜欢的一定要去要联系方式，但是从来都没勇气。3.长期用手。真的巨想谈恋爱了，但是为啥在成都作为一个攻还会单身，我真的没想通。我也不矮啊也有181。也不胖啊我日，思来想去是太宅了，很多人都以为我是直男。可能真的太直了。\n",
      "切分為 7 個片段:\n",
      "  片段1 (14字): 身为一个单身小攻是什么体验？\n",
      "  片段2 (21字): 镜像问题身为一个单身小受是一种怎样的体验？\n",
      "  片段3 (32字): 1.看到长得瘦，腿很好看的男生就会一直盯着看，然后就想去摸两把。\n",
      "  片段4 (28字): 2.发誓看到喜欢的一定要去要联系方式，但是从来都没勇气。\n",
      "  片段5 (33字): 真的巨想谈恋爱了，但是为啥在成都作为一个攻还会单身，我真的没想通。\n",
      "  片段6 (11字): 我也不矮啊也有181。\n",
      "  片段7 (27字): 也不胖啊我日，思来想去是太宅了，很多人都以为我是直男。\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🎯 步驟2: 執行大陸用語評分...\n",
      "📊 評分參數:\n",
      "  評分樣本數: 30\n",
      "  篩選閾值: 3/5\n",
      "📊 處理資料集：2153 筆記錄\n",
      "📝 文本欄位：text\n",
      "🎯 樣本大小：30\n",
      "⚖️ 篩選閾值：3/5\n",
      "\n",
      "🔄 開始 Ollama 推論...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  第1筆: 1/5 - 通用簡體中文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:   3%|▎         | 1/30 [00:15<07:25, 15.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  第2筆: 3/5 - 真正大陸用語\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:   7%|▋         | 2/30 [00:26<06:07, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  第3筆: 1/5 - 通用簡體中文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:  67%|██████▋   | 20/30 [03:25<01:36,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  第21筆: 1/5 - 通用簡體中文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度: 100%|██████████| 30/30 [04:54<00:00,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 評分結果統計:\n",
      "  總評分數: 30\n",
      "  ✅ 真正大陸用語: 1 筆\n",
      "  🗑️ 通用簡體中文: 29 筆\n",
      "  📊 大陸用語比例: 3.3%\n",
      "\n",
      "🏆 高質量大陸用語片段範例:\n",
      "第1名 (得分:3/5): 住在他们家于是品尝了一下他们的自助晚餐，天庭的创意是不错，咋一看也很漂亮，但不可否认有点旧了。\n",
      "  大陸特有詞彙: 咋\n",
      "------------------------------------------------------------\n",
      "\n",
      "💾 儲存處理結果...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# 儲存結果\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m💾 儲存處理結果...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m full_df, auth_df = \u001b[43msave_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthentic_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneric_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# 設定全域變數\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[\u001b[33m'\u001b[39m\u001b[33mselected_large_file_df\u001b[39m\u001b[33m'\u001b[39m] = selected_large_file_df\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 149\u001b[39m, in \u001b[36msave_results\u001b[39m\u001b[34m(results, authentic_texts, generic_texts)\u001b[39m\n\u001b[32m    139\u001b[39m row = {\n\u001b[32m    140\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m: r[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    141\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtext_length\u001b[39m\u001b[33m'\u001b[39m: r[\u001b[33m'\u001b[39m\u001b[33mtext_length\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    145\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmainland_terms\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join(r[\u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmainland_terms\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    146\u001b[39m }\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# 添加切分相關欄位（如果存在）\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m original_row = \u001b[43mavailable_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m[r[\u001b[33m'\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33msource_type\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m original_row:\n\u001b[32m    151\u001b[39m     row[\u001b[33m'\u001b[39m\u001b[33msource_type\u001b[39m\u001b[33m'\u001b[39m] = original_row[\u001b[33m'\u001b[39m\u001b[33msource_type\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "# 🔧 修正資料設定並執行完整處理流程\n",
    "print(\"🔧 修正資料設定...\")\n",
    "\n",
    "# 檢查並設定DataFrame\n",
    "if 'selected_data' in globals() and selected_data is not None:\n",
    "    print(f\"✅ 找到 selected_data: {len(selected_data)} 筆\")\n",
    "    \n",
    "    # 轉換為DataFrame\n",
    "    selected_large_file_df = pd.DataFrame(selected_data)\n",
    "    \n",
    "    print(f\"📊 DataFrame 建立成功: {selected_large_file_df.shape}\")\n",
    "    print(f\"📋 欄位: {list(selected_large_file_df.columns)}\")\n",
    "    \n",
    "    # 顯示前幾筆範例\n",
    "    print(f\"\\n📝 前3筆資料:\")\n",
    "    for i in range(min(3, len(selected_large_file_df))):\n",
    "        row = selected_large_file_df.iloc[i]\n",
    "        text = row['text']\n",
    "        preview = text[:80] + \"...\" if len(text) > 80 else text\n",
    "        print(f\"第{i+1}筆: {preview}\")\n",
    "    \n",
    "    print(f\"\\n🔄 開始執行完整處理流程...\")\n",
    "    \n",
    "    # Step 1: 句子級別切分\n",
    "    print(f\"\\n🔪 步驟1: 執行句子級別切分...\")\n",
    "    \n",
    "    selected_split_df = process_text_splitting(\n",
    "        df=selected_large_file_df,\n",
    "        text_column='text',\n",
    "        min_length=10,\n",
    "        max_length=50\n",
    "    )\n",
    "    \n",
    "    if not selected_split_df.empty:\n",
    "        print(f\"\\n📊 切分結果:\")\n",
    "        print(f\"  原始文本: {len(selected_large_file_df)} 筆\")\n",
    "        print(f\"  切分後句子片段: {len(selected_split_df)} 筆\")\n",
    "        print(f\"  平均每文本產生: {len(selected_split_df)/len(selected_large_file_df):.1f} 個片段\")\n",
    "        \n",
    "        # 顯示切分範例\n",
    "        print(f\"\\n📝 切分範例 (第1個原始文本):\")\n",
    "        first_orig_idx = selected_split_df['original_index'].iloc[0]\n",
    "        fragments = selected_split_df[selected_split_df['original_index'] == first_orig_idx]\n",
    "        original_text = selected_large_file_df.iloc[first_orig_idx]['text']\n",
    "        \n",
    "        print(f\"原始文本: {original_text}\")\n",
    "        print(f\"切分為 {len(fragments)} 個片段:\")\n",
    "        \n",
    "        for i, (_, row) in enumerate(fragments.iterrows()):\n",
    "            print(f\"  片段{i+1} ({row['fragment_length']}字): {row['text']}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Step 2: 大陸用語評分 (取較小的樣本數進行演示)\n",
    "        print(f\"\\n🎯 步驟2: 執行大陸用語評分...\")\n",
    "        \n",
    "        # 設定評分參數 - 減少樣本數以加快處理\n",
    "        SAMPLE_SIZE = min(30, len(selected_split_df))  # 取30個片段進行評分演示\n",
    "        THRESHOLD = 3  # 評分閾值\n",
    "        \n",
    "        print(f\"📊 評分參數:\")\n",
    "        print(f\"  評分樣本數: {SAMPLE_SIZE}\")\n",
    "        print(f\"  篩選閾值: {THRESHOLD}/5\")\n",
    "        \n",
    "        # 執行評分\n",
    "        results, authentic_results, generic_results = process_dataset(\n",
    "            df=selected_split_df,\n",
    "            text_col='text',\n",
    "            sample_size=SAMPLE_SIZE,\n",
    "            threshold=THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # 統計結果\n",
    "        print(f\"\\n📈 評分結果統計:\")\n",
    "        print(f\"  總評分數: {len(results)}\")\n",
    "        print(f\"  ✅ 真正大陸用語: {len(authentic_results)} 筆\")\n",
    "        print(f\"  🗑️ 通用簡體中文: {len(generic_results)} 筆\")\n",
    "        if len(results) > 0:\n",
    "            print(f\"  📊 大陸用語比例: {len(authentic_results)/len(results)*100:.1f}%\")\n",
    "        \n",
    "        # 顯示高質量範例\n",
    "        if authentic_results:\n",
    "            print(f\"\\n🏆 高質量大陸用語片段範例:\")\n",
    "            for i, r in enumerate(authentic_results[:3]):\n",
    "                print(f\"第{i+1}名 (得分:{r['scores']['總分']}/5): {r['text']}\")\n",
    "                if r['features']['mainland_terms']:\n",
    "                    print(f\"  大陸特有詞彙: {', '.join(r['features']['mainland_terms'])}\")\n",
    "                print(\"-\" * 60)\n",
    "        \n",
    "        # 儲存結果\n",
    "        print(f\"\\n💾 儲存處理結果...\")\n",
    "        full_df, auth_df = save_results(results, authentic_results, generic_results)\n",
    "        \n",
    "        # 設定全域變數\n",
    "        globals()['selected_large_file_df'] = selected_large_file_df\n",
    "        globals()['selected_split_df'] = selected_split_df\n",
    "        globals()['selected_mainland_results'] = results\n",
    "        globals()['selected_authentic_data'] = auth_df\n",
    "        \n",
    "        print(f\"\\n🎉 250筆資料完整處理流程完成！\")\n",
    "        print(f\"📊 最終統計:\")\n",
    "        print(f\"  原始挑選資料: {len(selected_large_file_df)} 筆\")\n",
    "        print(f\"  句子切分後: {len(selected_split_df)} 個片段\")\n",
    "        print(f\"  大陸用語評分: {len(results)} 個片段\")\n",
    "        print(f\"  高質量大陸用語: {len(authentic_results)} 個片段\")\n",
    "        \n",
    "        print(f\"\\n📋 可用變數:\")\n",
    "        print(f\"  selected_large_file_df: 原始挑選的250筆資料\")\n",
    "        print(f\"  selected_split_df: 句子切分後的片段資料\")\n",
    "        print(f\"  selected_mainland_results: 大陸用語評分結果\")\n",
    "        if auth_df is not None:\n",
    "            print(f\"  selected_authentic_data: 高質量大陸用語片段 ({len(auth_df)} 筆)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ 句子切分失敗，無法進行後續處理\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 沒有找到 selected_data\")\n",
    "    print(\"💡 請先執行前面的資料挑選步驟\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ece484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 簡化版結果總結和儲存\n",
    "print(\"🎯 整理和儲存最終結果...\")\n",
    "\n",
    "# 確保有評分結果\n",
    "if 'results' in locals() and 'authentic_results' in locals():\n",
    "    print(f\"✅ 找到評分結果\")\n",
    "    \n",
    "    # 創建完整結果DataFrame\n",
    "    full_results_data = []\n",
    "    for r in results:\n",
    "        row = {\n",
    "            'text': r['text'],\n",
    "            'text_length': r['text_length'],\n",
    "            'category': r['category'],\n",
    "            'success': r['success'],\n",
    "            'authenticity_score': r['features']['authenticity_score'],\n",
    "            'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "        }\n",
    "        \n",
    "        if r['scores']:\n",
    "            row.update({f'score_{k}': v for k, v in r['scores'].items()})\n",
    "        full_results_data.append(row)\n",
    "    \n",
    "    full_results_df = pd.DataFrame(full_results_data)\n",
    "    \n",
    "    # 創建高質量大陸用語DataFrame\n",
    "    if authentic_results:\n",
    "        authentic_data = []\n",
    "        for r in authentic_results:\n",
    "            auth_row = {\n",
    "                'text': r['text'],\n",
    "                'total_score': r['scores']['總分'],\n",
    "                'mainland_terms': ','.join(r['features']['mainland_terms']),\n",
    "                'text_length': r['text_length'],\n",
    "                'category': r['category']\n",
    "            }\n",
    "            \n",
    "            # 添加詳細評分\n",
    "            if r['scores']:\n",
    "                for key, value in r['scores'].items():\n",
    "                    if key != '總分':\n",
    "                        auth_row[f'score_{key}'] = value\n",
    "            \n",
    "            authentic_data.append(auth_row)\n",
    "        \n",
    "        authentic_df = pd.DataFrame(authentic_data)\n",
    "    else:\n",
    "        authentic_df = pd.DataFrame()\n",
    "    \n",
    "    # 儲存結果\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = \"evaluation_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # 儲存完整結果\n",
    "    full_results_file = f\"{results_dir}/full_evaluation_results_{timestamp}.csv\"\n",
    "    full_results_df.to_csv(full_results_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 儲存高質量結果\n",
    "    if not authentic_df.empty:\n",
    "        authentic_file = f\"{results_dir}/high_quality_mainland_texts_{timestamp}.csv\"\n",
    "        authentic_df.to_csv(authentic_file, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        authentic_json = f\"{results_dir}/high_quality_mainland_texts_{timestamp}.json\"\n",
    "        authentic_df.to_json(authentic_json, orient='records', force_ascii=False, indent=2)\n",
    "    \n",
    "    # 創建處理摘要\n",
    "    summary = {\n",
    "        \"processing_timestamp\": timestamp,\n",
    "        \"source_file\": \"/Users/edwardhuang/Documents/GitHub/LLM_Aug/data/CLUECorpusSmall.txt\",\n",
    "        \"original_samples\": len(selected_large_file_df),\n",
    "        \"split_fragments\": len(selected_split_df),\n",
    "        \"evaluated_fragments\": len(results),\n",
    "        \"high_quality_mainland\": len(authentic_results),\n",
    "        \"generic_chinese\": len(generic_results),\n",
    "        \"mainland_percentage\": len(authentic_results)/len(results)*100 if len(results) > 0 else 0,\n",
    "        \"fragment_length_stats\": {\n",
    "            \"mean\": float(selected_split_df['fragment_length'].mean()),\n",
    "            \"min\": int(selected_split_df['fragment_length'].min()),\n",
    "            \"max\": int(selected_split_df['fragment_length'].max()),\n",
    "            \"median\": float(selected_split_df['fragment_length'].median())\n",
    "        },\n",
    "        \"evaluation_sample_size\": len(results),\n",
    "        \"threshold_used\": 3\n",
    "    }\n",
    "    \n",
    "    # 儲存摘要\n",
    "    summary_file = f\"{results_dir}/processing_summary_{timestamp}.json\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 結果已成功儲存:\")\n",
    "    print(f\"  📊 完整評分結果: {full_results_file}\")\n",
    "    if not authentic_df.empty:\n",
    "        print(f\"  🏆 高質量大陸用語: {authentic_file}\")\n",
    "        print(f\"  📋 JSON格式: {authentic_json}\")\n",
    "    print(f\"  📋 處理摘要: {summary_file}\")\n",
    "    \n",
    "    # 顯示檔案大小\n",
    "    print(f\"\\n📁 檔案大小:\")\n",
    "    for filename in [full_results_file, authentic_file if not authentic_df.empty else None, summary_file]:\n",
    "        if filename and os.path.exists(filename):\n",
    "            size_kb = os.path.getsize(filename) / 1024\n",
    "            print(f\"  {os.path.basename(filename)}: {size_kb:.2f} KB\")\n",
    "    \n",
    "    # 設定全域變數\n",
    "    globals()['selected_large_file_df'] = selected_large_file_df\n",
    "    globals()['selected_split_df'] = selected_split_df\n",
    "    globals()['selected_mainland_results'] = results\n",
    "    globals()['selected_authentic_data'] = authentic_df\n",
    "    globals()['evaluation_summary'] = summary\n",
    "    \n",
    "    print(f\"\\n🎉 250筆資料評分處理完成！\")\n",
    "    print(f\"\\n📊 最終統計摘要:\")\n",
    "    print(f\"  📂 原始大文件: CLUECorpusSmall.txt\")\n",
    "    print(f\"  🎲 隨機挑選: {summary['original_samples']} 筆資料\")\n",
    "    print(f\"  🔪 句子切分: {summary['split_fragments']} 個片段\")\n",
    "    print(f\"  🎯 評分處理: {summary['evaluated_fragments']} 個片段\")\n",
    "    print(f\"  ✅ 高質量大陸用語: {summary['high_quality_mainland']} 個片段\")\n",
    "    print(f\"  📈 大陸用語比例: {summary['mainland_percentage']:.1f}%\")\n",
    "    \n",
    "    if authentic_results:\n",
    "        print(f\"\\n🏆 發現的高質量大陸用語片段:\")\n",
    "        for i, r in enumerate(authentic_results):\n",
    "            print(f\"  {i+1}. (得分 {r['scores']['總分']}/5): {r['text']}\")\n",
    "            if r['features']['mainland_terms']:\n",
    "                print(f\"     大陸特有詞彙: {', '.join(r['features']['mainland_terms'])}\")\n",
    "    \n",
    "    print(f\"\\n📋 可用變數:\")\n",
    "    print(f\"  selected_large_file_df: 原始250筆資料\")\n",
    "    print(f\"  selected_split_df: 切分後的句子片段\")\n",
    "    print(f\"  selected_mainland_results: 評分結果\")\n",
    "    print(f\"  selected_authentic_data: 高質量大陸用語\")\n",
    "    print(f\"  evaluation_summary: 處理摘要\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 沒有找到評分結果\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
