{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008ebe94",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d9d4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "import os\n",
    "import ollama\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712a89a",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c0fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== è³‡æ–™è¼‰å…¥é¸é … ===\n",
      "ä½¿ç”¨æœ¬åœ°æª”æ¡ˆ: True\n",
      "ä½¿ç”¨ä¸²æµæ¨¡å¼: False\n",
      "ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†: False\n",
      "\n",
      "ğŸ“ å¾æœ¬åœ°æª”æ¡ˆè®€å–è³‡æ–™...\n",
      "æ‰¾åˆ°çš„æª”æ¡ˆ:\n",
      "  CSV æª”æ¡ˆ: 1 å€‹\n",
      "  JSON æª”æ¡ˆ: 1 å€‹\n",
      "  Parquet æª”æ¡ˆ: 1 å€‹\n",
      "\n",
      "ğŸ“Š è®€å–æœ€æ–°çš„ Parquet æª”æ¡ˆ: saved_datasets/clue_corpus_small_20250901_085816.parquet\n",
      "\n",
      "âœ… è³‡æ–™è¼‰å…¥æˆåŠŸï¼\n",
      "ğŸ“Š è³‡æ–™å½¢ç‹€: (1000, 1)\n",
      "ğŸ“‹ æ¬„ä½åç¨±: ['text']\n",
      "\n",
      "ğŸ“ˆ æ–‡æœ¬é•·åº¦çµ±è¨ˆ:\n",
      "count     1000.000000\n",
      "mean       300.899000\n",
      "std        785.763282\n",
      "min          5.000000\n",
      "25%         38.000000\n",
      "50%        105.500000\n",
      "75%        274.000000\n",
      "max      17020.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "ğŸ“ å‰ 3 ç­†è³‡æ–™ç¯„ä¾‹:\n",
      "ç¯„ä¾‹ 1 (132 å­—ç¬¦): 130çœŸæ˜¯ä½©æœè¿™å®¶åº—å¼€è¿™ä¹ˆä¹…ã€‚å°½ç®¡é—¨é¢å·²ç»å°äº†ä¸€åœˆï¼Œä½†è¿˜æ˜¯å¼€ç€ä¸å®¹æ˜“å•Šã€‚æˆ‘ä»¬ä¸å®¹æ˜“ï¼Œè€æ¿ä¹Ÿä¸å®¹æ˜“ã€‚è‡ªåŠ©é¤ï¼Œä½ å¯ä»¥åƒå¾—æ¯”å¹³æ—¶å¤šï¼Œä½†å†³ä¸èƒ½æµªè´¹ã€‚æƒ³åƒå›20å…ƒï¼Œé‚£æ˜¯ä¸å¯èƒ½çš„ï¼Œæ‰€ä»¥è¿˜æ˜¯ä¸è¦å»äº†ã€‚èœçœŸçš„å¾ˆä¸€èˆ¬ï¼Œ...\n",
      "--------------------------------------------------------------------------------\n",
      "ç¯„ä¾‹ 2 (8 å­—ç¬¦): é€è´§é€Ÿåº¦å¥‡æ…¢æ— æ¯”\n",
      "--------------------------------------------------------------------------------\n",
      "ç¯„ä¾‹ 3 (12 å­—ç¬¦): è¿™æ˜¯è‡ªå·±ç”¨è¿‡æœ€å¥½çš„ç”¨çš„äº†\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ¯ è³‡æ–™å·²æº–å‚™å°±ç·’ï¼Œå¯ç”¨æ–¼å¾ŒçºŒçš„ LLM è©•åˆ†è™•ç†ï¼\n"
     ]
    }
   ],
   "source": [
    "# è³‡æ–™è®€å–é¸é …\n",
    "# æ‚¨å¯ä»¥é¸æ“‡ä»¥ä¸‹ä»»ä¸€ç¨®æ–¹å¼ä¾†è¼‰å…¥è³‡æ–™ï¼š\n",
    "\n",
    "# é¸é … 1: å¾å·²å„²å­˜çš„æœ¬åœ°æª”æ¡ˆè®€å– (æ¨è–¦ï¼Œé€Ÿåº¦å¿«)\n",
    "use_local_files = True\n",
    "\n",
    "# é¸é … 2: å¾ Hugging Face ç›´æ¥ä¸²æµè¼‰å…¥ (éœ€è¦ç¶²è·¯é€£ç·š)\n",
    "use_streaming = False\n",
    "\n",
    "# é¸é … 3: ä¸‹è¼‰å®Œæ•´è³‡æ–™é›† (æª”æ¡ˆå¾ˆå¤§ï¼Œä¸æ¨è–¦)\n",
    "use_full_download = False\n",
    "\n",
    "print(\"=== è³‡æ–™è¼‰å…¥é¸é … ===\")\n",
    "print(f\"ä½¿ç”¨æœ¬åœ°æª”æ¡ˆ: {use_local_files}\")\n",
    "print(f\"ä½¿ç”¨ä¸²æµæ¨¡å¼: {use_streaming}\")\n",
    "print(f\"ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†: {use_full_download}\")\n",
    "\n",
    "# è³‡æ–™è¼‰å…¥\n",
    "if use_local_files:\n",
    "    print(\"\\nğŸ“ å¾æœ¬åœ°æª”æ¡ˆè®€å–è³‡æ–™...\")\n",
    "    \n",
    "    # æª¢æŸ¥å·²å„²å­˜çš„æª”æ¡ˆ\n",
    "    save_dir = \"saved_datasets\"\n",
    "    \n",
    "    if os.path.exists(save_dir):\n",
    "        import glob\n",
    "        \n",
    "        # å°‹æ‰¾å¯ç”¨çš„æª”æ¡ˆ\n",
    "        csv_files = glob.glob(f\"{save_dir}/*.csv\")\n",
    "        json_files = glob.glob(f\"{save_dir}/*.json\")\n",
    "        parquet_files = glob.glob(f\"{save_dir}/*.parquet\")\n",
    "        \n",
    "        print(f\"æ‰¾åˆ°çš„æª”æ¡ˆ:\")\n",
    "        print(f\"  CSV æª”æ¡ˆ: {len(csv_files)} å€‹\")\n",
    "        print(f\"  JSON æª”æ¡ˆ: {len(json_files)} å€‹\")\n",
    "        print(f\"  Parquet æª”æ¡ˆ: {len(parquet_files)} å€‹\")\n",
    "        \n",
    "        # å„ªå…ˆä½¿ç”¨ Parquet æª”æ¡ˆ (æœ€é«˜æ•ˆ)\n",
    "        if parquet_files:\n",
    "            latest_file = max(parquet_files, key=os.path.getctime)\n",
    "            print(f\"\\nğŸ“Š è®€å–æœ€æ–°çš„ Parquet æª”æ¡ˆ: {latest_file}\")\n",
    "            df = pd.read_parquet(latest_file)\n",
    "            \n",
    "        # å…¶æ¬¡ä½¿ç”¨ CSV æª”æ¡ˆ\n",
    "        elif csv_files:\n",
    "            latest_file = max(csv_files, key=os.path.getctime)\n",
    "            print(f\"\\nğŸ“Š è®€å–æœ€æ–°çš„ CSV æª”æ¡ˆ: {latest_file}\")\n",
    "            df = pd.read_csv(latest_file)\n",
    "            \n",
    "        # æœ€å¾Œä½¿ç”¨ JSON æª”æ¡ˆ\n",
    "        elif json_files:\n",
    "            latest_file = max(json_files, key=os.path.getctime)\n",
    "            print(f\"\\nğŸ“Š è®€å–æœ€æ–°çš„ JSON æª”æ¡ˆ: {latest_file}\")\n",
    "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ æ²’æœ‰æ‰¾åˆ°å·²å„²å­˜çš„è³‡æ–™æª”æ¡ˆ\")\n",
    "            print(\"è«‹å…ˆåŸ·è¡Œè³‡æ–™ä¸‹è¼‰å’Œå„²å­˜çš„ç¨‹å¼ç¢¼\")\n",
    "            df = None\n",
    "    else:\n",
    "        print(\"âŒ æ‰¾ä¸åˆ° saved_datasets ç›®éŒ„\")\n",
    "        print(\"è«‹å…ˆåŸ·è¡Œè³‡æ–™ä¸‹è¼‰å’Œå„²å­˜çš„ç¨‹å¼ç¢¼\")\n",
    "        df = None\n",
    "\n",
    "elif use_streaming:\n",
    "    print(\"\\nğŸŒ å¾ Hugging Face ä¸²æµè¼‰å…¥è³‡æ–™...\")\n",
    "    \n",
    "    # ä½¿ç”¨ä¸²æµæ¨¡å¼è¼‰å…¥è³‡æ–™é›†\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\", streaming=True)\n",
    "    \n",
    "    # è¨­å®šè¦è¼‰å…¥çš„æ¨£æœ¬æ•¸é‡\n",
    "    num_samples = 1000\n",
    "    print(f\"è¼‰å…¥å‰ {num_samples} ç­†è³‡æ–™...\")\n",
    "    \n",
    "    # æ”¶é›†è³‡æ–™\n",
    "    sample_data = []\n",
    "    for i, example in enumerate(dataset['train']):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        sample_data.append(example)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  å·²è¼‰å…¥ {i + 1} ç­†è³‡æ–™...\")\n",
    "    \n",
    "    # è½‰æ›ç‚º DataFrame\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    \n",
    "elif use_full_download:\n",
    "    print(\"\\nâ¬‡ï¸ ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†...\")\n",
    "    print(\"è­¦å‘Šï¼šé€™å°‡ä¸‹è¼‰ 13.7GB çš„è³‡æ–™ï¼Œå¯èƒ½éœ€è¦å¾ˆé•·æ™‚é–“\")\n",
    "    \n",
    "    # ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\")\n",
    "    df = dataset['train'].to_pandas()\n",
    "\n",
    "else:\n",
    "    print(\"âŒ æ²’æœ‰é¸æ“‡ä»»ä½•è³‡æ–™è¼‰å…¥é¸é …\")\n",
    "    df = None\n",
    "\n",
    "# é¡¯ç¤ºè³‡æ–™è³‡è¨Š\n",
    "if df is not None:\n",
    "    print(f\"\\nâœ… è³‡æ–™è¼‰å…¥æˆåŠŸï¼\")\n",
    "    print(f\"ğŸ“Š è³‡æ–™å½¢ç‹€: {df.shape}\")\n",
    "    print(f\"ğŸ“‹ æ¬„ä½åç¨±: {list(df.columns)}\")\n",
    "    \n",
    "    # é¡¯ç¤ºåŸºæœ¬çµ±è¨ˆ\n",
    "    if 'text' in df.columns: # type: ignore\n",
    "        df['text_length'] = df['text'].str.len() # type: ignore\n",
    "        print(f\"\\nğŸ“ˆ æ–‡æœ¬é•·åº¦çµ±è¨ˆ:\")\n",
    "        print(df['text_length'].describe()) # type: ignore\n",
    "        \n",
    "        # é¡¯ç¤ºå‰å¹¾ç­†è³‡æ–™ç¯„ä¾‹\n",
    "        print(f\"\\nğŸ“ å‰ 3 ç­†è³‡æ–™ç¯„ä¾‹:\")\n",
    "        for i in range(min(3, len(df))): # type: ignore\n",
    "            text = df.iloc[i]['text']\n",
    "            # é¡¯ç¤ºå‰100å€‹å­—ç¬¦\n",
    "            preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            print(f\"ç¯„ä¾‹ {i+1} ({len(text)} å­—ç¬¦): {preview}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ è³‡æ–™å·²æº–å‚™å°±ç·’ï¼Œå¯ç”¨æ–¼å¾ŒçºŒçš„ LLM è©•åˆ†è™•ç†ï¼\")\n",
    "else:\n",
    "    print(\"\\nâŒ è³‡æ–™è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥è¨­å®šä¸¦é‡æ–°åŸ·è¡Œ\")\n",
    "\n",
    "# å„²å­˜åˆ°å…¨åŸŸè®Šæ•¸ä¾›å¾ŒçºŒä½¿ç”¨\n",
    "globals()['dataset_df'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525fac3d",
   "metadata": {},
   "source": [
    "## LLM AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eb918cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ é–‹å§‹ä½¿ç”¨ Ollama GPT-OSS 20B é€²è¡Œè³‡æ–™æ“´å¢...\n",
      "ğŸ“‹ è¨­å®šåƒæ•¸:\n",
      "  æ¨¡å‹: gpt-oss:20b\n",
      "  æ¯å€‹æ–‡æœ¬æ“´å¢æ•¸é‡: 3\n",
      "  è™•ç†æ–‡æœ¬æ•¸é‡: 50\n",
      "\n",
      "ğŸ” æª¢æŸ¥ Ollama æ¨¡å‹...\n",
      "âŒ Ollama é€£æ¥éŒ¯èª¤: 'name'\n",
      "è«‹ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œ\n",
      "å¯ä»¥å˜—è©¦åœ¨çµ‚ç«¯æ©ŸåŸ·è¡Œ: ollama serve\n",
      "\n",
      "ğŸ”„ é–‹å§‹è™•ç† 50 å€‹æ–‡æœ¬...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ“´å¢é€²åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [36:07<00:00, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… è³‡æ–™æ“´å¢å®Œæˆï¼\n",
      "ğŸ“Š æ“´å¢çµ±è¨ˆ:\n",
      "  åŸå§‹æ–‡æœ¬: 50 ç­†\n",
      "  æ“´å¢æ–‡æœ¬: 150 ç­†\n",
      "  ç¸½è¨ˆ: 200 ç­†\n",
      "  æ“´å¢å€ç‡: 4.0x\n",
      "\n",
      "ğŸ“ˆ è³‡æ–™ä¾†æºåˆ†å¸ƒ:\n",
      "source\n",
      "original        50\n",
      "augmented_v1    50\n",
      "augmented_v2    50\n",
      "augmented_v3    50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“ æ“´å¢ç¯„ä¾‹:\n",
      "\n",
      "ORIGINAL:\n",
      "  130çœŸæ˜¯ä½©æœè¿™å®¶åº—å¼€è¿™ä¹ˆä¹…ã€‚å°½ç®¡é—¨é¢å·²ç»å°äº†ä¸€åœˆï¼Œä½†è¿˜æ˜¯å¼€ç€ä¸å®¹æ˜“å•Šã€‚æˆ‘ä»¬ä¸å®¹æ˜“ï¼Œè€æ¿ä¹Ÿä¸å®¹æ˜“ã€‚è‡ªåŠ©é¤ï¼Œä½ å¯ä»¥åƒå¾—æ¯”å¹³æ—¶å¤šï¼Œä½†å†³ä¸èƒ½æµªè´¹ã€‚æƒ³åƒå›20å…ƒï¼Œé‚£æ˜¯ä¸å¯èƒ½çš„ï¼Œæ‰€ä»¥è¿˜æ˜¯ä¸è¦å»äº†ã€‚èœçœŸçš„å¾ˆä¸€èˆ¬ï¼Œæ´—å¹²å‡€å°±å¥½å•¦ã€‚ä»€ä¹ˆéƒ½è¦å¦å¤–ä»˜é’±ï¼Œä¸€å®šè¦æƒ³å¥½ï¼Œåˆ«çš„ä¸å«ï¼Œåªåƒè‡ªåŠ©ã€‚\n",
      "\n",
      "AUGMENTED_V1:\n",
      "  çœŸè®©äººä½©æœè¿™å®¶åº—èƒ½ç»è¥è¿™ä¹ˆä¹…ã€‚è™½ç„¶é—¨é¢ç¼©å°äº†ä¸€åœˆï¼Œå´ä¾æ—§ç»´æŒè¥ä¸šå¹¶ä¸ç®€å•ã€‚æˆ‘ä»¬å’Œè€æ¿éƒ½ä¸å®¹æ˜“ã€‚è‡ªåŠ©é¤å¯ä»¥åƒå¾—æ¯”å¹³æ—¶å¤šï¼Œä½†ç»ä¸èƒ½æµªè´¹ã€‚æƒ³è¦é€€å›20å…ƒï¼Œæ ¹æœ¬ä¸å¯èƒ½ï¼Œè¿˜æ˜¯åˆ«å»å§ã€‚èœå“çœŸçš„å¾ˆæ™®é€šï¼Œå¹²å‡€å°±è¡Œã€‚æ‰€æœ‰ä¸œè¥¿éƒ½è¦é¢å¤–æ”¶è´¹ï¼ŒåŠ¡å¿…ä¸‰æ€ï¼Œåˆ«ç‚¹åˆ«çš„ï¼Œåªåƒè‡ªåŠ©ã€‚\n",
      "\n",
      "AUGMENTED_V2:\n",
      "  æˆ‘æ·±æ„Ÿä½©æœè¿™å®¶åº—ç»è¥è‡³ä»Šã€‚å°½ç®¡é—¨é¢å·²ç•¥æ˜¾ç®€çº¦ï¼Œä½†ä»ä¸æ˜“ç»´æŒã€‚æˆ‘ä»¬ä¸è€æ¿åŒæ ·é¢ä¸´è¯¸å¤šå›°éš¾ã€‚è‡ªåŠ©é¤å¯æ¯”å¹³æ—¶å¤šé£Ÿï¼Œä½†åŠ¡å¿…é¿å…æµªè´¹ã€‚è‹¥æœŸæœ›é€€å›20å…ƒï¼Œææ€•æ— æ³•å®ç°ï¼Œå»ºè®®ä¸å†å‰å¾€ã€‚èœå“è´¨é‡ä¸€èˆ¬ï¼Œåªéœ€æ´—å‡€åé£Ÿç”¨ã€‚æ‰€æœ‰é¡¹ç›®å‡éœ€é¢å¤–ä»˜è´¹ï¼Œè¯·åŠ¡å¿…ä¸‰æ€ï¼Œè‹¥åªæƒ³åƒè‡ªåŠ©ï¼Œè¯·è‡ªè¡Œå†³å®šã€‚\n",
      "\n",
      "AUGMENTED_V3:\n",
      "  è¿™å®¶åº—å¼€äº†è¿™ä¹ˆä¹…ï¼ŒçœŸè®©äººä½©æœã€‚é—¨é¢è™½å·²å˜å°ï¼Œä½†ä»ä¸æ˜“ç»è¥ã€‚æˆ‘ä»¬å’Œè€æ¿éƒ½ä¸å®¹æ˜“ã€‚è‡ªåŠ©é¤å¯ä»¥æ¯”å¹³æ—¶åƒå¾—å¤šï¼Œä½†ä¸€å®šä¸èƒ½æµªè´¹ã€‚æƒ³è¦å›20å…ƒæ˜¯ä¸å¯èƒ½çš„ï¼Œè¿˜æ˜¯åˆ«å»äº†ã€‚èœå“å¾ˆä¸€èˆ¬ï¼Œæ´—å¹²å‡€å°±è¡Œã€‚æ‰€æœ‰ä¸œè¥¿éƒ½è¦å¦å¤–ä»˜è´¹ï¼Œä¸€å®šè¦æƒ³æ¸…æ¥šï¼Œåˆ«å«åˆ«çš„èœï¼Œåªåƒè‡ªåŠ©ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "NDFrame.to_json() got an unexpected keyword argument 'ensure_ascii'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 198\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# JSON æ ¼å¼\u001b[39;00m\n\u001b[32m    197\u001b[39m json_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m \u001b[43maugmented_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrecords\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# Parquet æ ¼å¼\u001b[39;00m\n\u001b[32m    201\u001b[39m parquet_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: NDFrame.to_json() got an unexpected keyword argument 'ensure_ascii'"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ Ollama GPT-OSS 20B é€²è¡Œè³‡æ–™æ“´å¢\n",
    "\n",
    "print(\"ğŸš€ é–‹å§‹ä½¿ç”¨ Ollama GPT-OSS 20B é€²è¡Œè³‡æ–™æ“´å¢...\")\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦æœ‰è¼‰å…¥çš„è³‡æ–™é›†\n",
    "if 'dataset_df' not in globals() or dataset_df is None:\n",
    "    print(\"âŒ æ²’æœ‰æ‰¾åˆ°è³‡æ–™é›†ï¼Œè«‹å…ˆåŸ·è¡Œ GET DATA éƒ¨åˆ†\")\n",
    "else:\n",
    "    # è¨­å®šåƒæ•¸\n",
    "    model_name = \"gpt-oss:20b\"  # Ollama æ¨¡å‹åç¨±\n",
    "    num_augmentations_per_text = 3  # æ¯å€‹åŸå§‹æ–‡æœ¬ç”Ÿæˆçš„æ“´å¢æ•¸é‡\n",
    "    max_texts_to_process = 50  # è™•ç†çš„æ–‡æœ¬æ•¸é‡ (å¯èª¿æ•´)\n",
    "    \n",
    "    print(f\"ğŸ“‹ è¨­å®šåƒæ•¸:\")\n",
    "    print(f\"  æ¨¡å‹: {model_name}\")\n",
    "    print(f\"  æ¯å€‹æ–‡æœ¬æ“´å¢æ•¸é‡: {num_augmentations_per_text}\")\n",
    "    print(f\"  è™•ç†æ–‡æœ¬æ•¸é‡: {max_texts_to_process}\")\n",
    "    \n",
    "    # æª¢æŸ¥ Ollama é€£æ¥\n",
    "    try:\n",
    "        # æ¸¬è©¦ Ollama é€£æ¥\n",
    "        available_models = ollama.list()\n",
    "        print(f\"\\nğŸ” æª¢æŸ¥ Ollama æ¨¡å‹...\")\n",
    "        model_found = any(model_name in model['name'] for model in available_models['models'])\n",
    "        \n",
    "        if not model_found:\n",
    "            print(f\"âš ï¸  æ¨¡å‹ {model_name} æœªæ‰¾åˆ°ï¼Œå˜—è©¦æ‹‰å–æ¨¡å‹...\")\n",
    "            print(f\"æ­£åœ¨ä¸‹è¼‰ {model_name}ï¼Œé€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“...\")\n",
    "            ollama.pull(model_name)\n",
    "            print(f\"âœ… æ¨¡å‹ {model_name} ä¸‹è¼‰å®Œæˆ\")\n",
    "        else:\n",
    "            print(f\"âœ… æ¨¡å‹ {model_name} å·²å°±ç·’\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ollama é€£æ¥éŒ¯èª¤: {e}\")\n",
    "        print(\"è«‹ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œ\")\n",
    "        print(\"å¯ä»¥å˜—è©¦åœ¨çµ‚ç«¯æ©ŸåŸ·è¡Œ: ollama serve\")\n",
    "        \n",
    "    # å®šç¾©è³‡æ–™æ“´å¢çš„æç¤ºæ¨¡æ¿\n",
    "    augmentation_prompts = [\n",
    "        # æç¤º1: æ”¹å¯«ä¿æŒåŸæ„\n",
    "        \"\"\"è«‹å°‡ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬é€²è¡Œæ”¹å¯«ï¼Œä¿æŒåŸæ„ä½†ä½¿ç”¨ä¸åŒçš„è¡¨é”æ–¹å¼ï¼š\n",
    "\n",
    "åŸæ–‡æœ¬ï¼š{text}\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "1. ä¿æŒæ–‡æœ¬çš„æ ¸å¿ƒæ„æ€ä¸è®Š\n",
    "2. ä½¿ç”¨ä¸åŒçš„è©å½™å’Œå¥å¼\n",
    "3. ä¿æŒç°¡é«”ä¸­æ–‡\n",
    "4. è¼¸å‡ºæ ¼å¼åªéœ€è¦æ”¹å¯«å¾Œçš„æ–‡æœ¬ï¼Œä¸è¦å…¶ä»–èªªæ˜\n",
    "\n",
    "æ”¹å¯«æ–‡æœ¬ï¼š\"\"\",\n",
    "\n",
    "        # æç¤º2: é¢¨æ ¼è½‰æ›\n",
    "        \"\"\"è«‹å°‡ä»¥ä¸‹æ–‡æœ¬è½‰æ›ç‚ºæ›´æ­£å¼/æ›´å£èªçš„è¡¨é”æ–¹å¼ï¼š\n",
    "\n",
    "åŸæ–‡æœ¬ï¼š{text}\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "1. å¦‚æœåŸæ–‡æ¯”è¼ƒå£èªï¼Œè«‹è½‰ç‚ºæ­£å¼è¡¨é”\n",
    "2. å¦‚æœåŸæ–‡æ¯”è¼ƒæ­£å¼ï¼Œè«‹è½‰ç‚ºå£èªè¡¨é”\n",
    "3. ä¿æŒæ–‡æœ¬çš„ä¸»è¦è³‡è¨Š\n",
    "4. ä½¿ç”¨ç°¡é«”ä¸­æ–‡\n",
    "5. è¼¸å‡ºæ ¼å¼åªéœ€è¦è½‰æ›å¾Œçš„æ–‡æœ¬ï¼Œä¸è¦å…¶ä»–èªªæ˜\n",
    "\n",
    "è½‰æ›æ–‡æœ¬ï¼š\"\"\",\n",
    "\n",
    "        # æç¤º3: å¥å¼é‡çµ„\n",
    "        \"\"\"è«‹é‡æ–°çµ„ç¹”ä»¥ä¸‹æ–‡æœ¬çš„å¥å­çµæ§‹ï¼Œä½†ä¿æŒç›¸åŒçš„å«ç¾©ï¼š\n",
    "\n",
    "åŸæ–‡æœ¬ï¼š{text}\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "1. é‡æ–°å®‰æ’å¥å­é †åºæˆ–çµæ§‹\n",
    "2. å¯ä»¥å°‡é•·å¥æ‹†åˆ†æˆ–çŸ­å¥åˆä½µ\n",
    "3. ä¿æŒåŸå§‹å«ç¾©\n",
    "4. ä½¿ç”¨ç°¡é«”ä¸­æ–‡\n",
    "5. è¼¸å‡ºæ ¼å¼åªéœ€è¦é‡çµ„å¾Œçš„æ–‡æœ¬ï¼Œä¸è¦å…¶ä»–èªªæ˜\n",
    "\n",
    "é‡çµ„æ–‡æœ¬ï¼š\"\"\"\n",
    "    ]\n",
    "    \n",
    "    # è³‡æ–™æ“´å¢å‡½æ•¸\n",
    "    def augment_text(original_text, prompt_template):\n",
    "        \"\"\"ä½¿ç”¨ Ollama æ“´å¢å–®å€‹æ–‡æœ¬\"\"\"\n",
    "        try:\n",
    "            # æº–å‚™æç¤º\n",
    "            full_prompt = prompt_template.format(text=original_text)\n",
    "            \n",
    "            # èª¿ç”¨ Ollama\n",
    "            response = ollama.generate(\n",
    "                model=model_name,\n",
    "                prompt=full_prompt,\n",
    "                options={\n",
    "                    'temperature': 0.7,\n",
    "                    'top_p': 0.9,\n",
    "                    'max_tokens': 1000\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # æå–ç”Ÿæˆçš„æ–‡æœ¬\n",
    "            augmented_text = response['response'].strip()\n",
    "            \n",
    "            # æ¸…ç†æ–‡æœ¬ (ç§»é™¤å¯èƒ½çš„å‰ç¶´)\n",
    "            augmented_text = re.sub(r'^(æ”¹å¯«æ–‡æœ¬ï¼š|è½‰æ›æ–‡æœ¬ï¼š|é‡çµ„æ–‡æœ¬ï¼š)', '', augmented_text).strip()\n",
    "            \n",
    "            return augmented_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ æ–‡æœ¬æ“´å¢éŒ¯èª¤: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # é–‹å§‹è³‡æ–™æ“´å¢\n",
    "    augmented_data = []\n",
    "    original_data = []\n",
    "    \n",
    "    # æº–å‚™åŸå§‹è³‡æ–™\n",
    "    texts_to_process = dataset_df['text'].head(max_texts_to_process).tolist()\n",
    "    \n",
    "    print(f\"\\nğŸ”„ é–‹å§‹è™•ç† {len(texts_to_process)} å€‹æ–‡æœ¬...\")\n",
    "    \n",
    "    # ä½¿ç”¨é€²åº¦æ¢\n",
    "    with tqdm(total=len(texts_to_process) * num_augmentations_per_text, desc=\"æ“´å¢é€²åº¦\") as pbar:\n",
    "        \n",
    "        for idx, original_text in enumerate(texts_to_process):\n",
    "            # æ·»åŠ åŸå§‹æ–‡æœ¬\n",
    "            original_data.append({\n",
    "                'text': original_text,\n",
    "                'source': 'original',\n",
    "                'original_idx': idx\n",
    "            })\n",
    "            \n",
    "            # å°æ¯å€‹æ–‡æœ¬é€²è¡Œå¤šæ¬¡æ“´å¢\n",
    "            for aug_idx in range(num_augmentations_per_text):\n",
    "                # é¸æ“‡æç¤ºæ¨¡æ¿ (è¼ªæµä½¿ç”¨)\n",
    "                prompt_idx = aug_idx % len(augmentation_prompts)\n",
    "                prompt = augmentation_prompts[prompt_idx]\n",
    "                \n",
    "                # é€²è¡Œæ“´å¢\n",
    "                augmented_text = augment_text(original_text, prompt)\n",
    "                \n",
    "                if augmented_text and len(augmented_text.strip()) > 0:\n",
    "                    augmented_data.append({\n",
    "                        'text': augmented_text,\n",
    "                        'source': f'augmented_v{aug_idx + 1}',\n",
    "                        'original_idx': idx,\n",
    "                        'augmentation_method': f'prompt_{prompt_idx + 1}'\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"âš ï¸ ç¬¬ {idx} å€‹æ–‡æœ¬çš„ç¬¬ {aug_idx + 1} æ¬¡æ“´å¢å¤±æ•—\")\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # æ·»åŠ å°å»¶é²é¿å…éè¼‰\n",
    "                time.sleep(0.1)\n",
    "    \n",
    "    # åˆä½µåŸå§‹è³‡æ–™å’Œæ“´å¢è³‡æ–™\n",
    "    all_data = original_data + augmented_data\n",
    "    \n",
    "    # å‰µå»ºæ–°çš„ DataFrame\n",
    "    augmented_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    print(f\"\\nâœ… è³‡æ–™æ“´å¢å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“Š æ“´å¢çµ±è¨ˆ:\")\n",
    "    print(f\"  åŸå§‹æ–‡æœ¬: {len(original_data)} ç­†\")\n",
    "    print(f\"  æ“´å¢æ–‡æœ¬: {len(augmented_data)} ç­†\")\n",
    "    print(f\"  ç¸½è¨ˆ: {len(all_data)} ç­†\")\n",
    "    print(f\"  æ“´å¢å€ç‡: {len(all_data) / len(original_data):.1f}x\")\n",
    "    \n",
    "    # é¡¯ç¤ºè³‡æ–™ä¾†æºåˆ†å¸ƒ\n",
    "    print(f\"\\nğŸ“ˆ è³‡æ–™ä¾†æºåˆ†å¸ƒ:\")\n",
    "    print(augmented_df['source'].value_counts())\n",
    "    \n",
    "    # é¡¯ç¤ºç¯„ä¾‹\n",
    "    print(f\"\\nğŸ“ æ“´å¢ç¯„ä¾‹:\")\n",
    "    example_idx = 0\n",
    "    original_example = augmented_df[augmented_df['original_idx'] == example_idx]\n",
    "    \n",
    "    for i, row in original_example.iterrows():\n",
    "        print(f\"\\n{row['source'].upper()}:\")\n",
    "        text_preview = row['text'][:150] + \"...\" if len(row['text']) > 150 else row['text']\n",
    "        print(f\"  {text_preview}\")\n",
    "    \n",
    "    # å„²å­˜æ“´å¢å¾Œçš„è³‡æ–™é›†\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    augmented_dir = \"augmented_datasets\"\n",
    "    os.makedirs(augmented_dir, exist_ok=True)\n",
    "    \n",
    "    # å„²å­˜ç‚ºå¤šç¨®æ ¼å¼\n",
    "    base_filename = f\"{augmented_dir}/clue_corpus_augmented_{timestamp}\"\n",
    "    \n",
    "    # CSV æ ¼å¼\n",
    "    csv_filename = f\"{base_filename}.csv\"\n",
    "    augmented_df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    # JSON æ ¼å¼\n",
    "    json_filename = f\"{base_filename}.json\"\n",
    "    augmented_df.to_json(json_filename, orient='records', ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Parquet æ ¼å¼\n",
    "    parquet_filename = f\"{base_filename}.parquet\"\n",
    "    augmented_df.to_parquet(parquet_filename, index=False)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ æ“´å¢è³‡æ–™é›†å·²å„²å­˜:\")\n",
    "    print(f\"  CSV: {csv_filename}\")\n",
    "    print(f\"  JSON: {json_filename}\")\n",
    "    print(f\"  Parquet: {parquet_filename}\")\n",
    "    \n",
    "    # æª”æ¡ˆå¤§å°\n",
    "    for filename in [csv_filename, json_filename, parquet_filename]:\n",
    "        size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"  {os.path.basename(filename)}: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # å„²å­˜åˆ°å…¨åŸŸè®Šæ•¸\n",
    "    globals()['augmented_dataset_df'] = augmented_df\n",
    "    \n",
    "    print(f\"\\nğŸ¯ æ“´å¢è³‡æ–™é›†å·²æº–å‚™å°±ç·’ï¼Œå¯ç”¨æ–¼è¨“ç·´å’Œè©•ä¼°ï¼\")\n",
    "    print(f\"è®Šæ•¸åç¨±: augmented_dataset_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa3e48ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ ä¿®æ­£å„²å­˜æ ¼å¼ä¸¦å®Œæˆè³‡æ–™é›†å„²å­˜...\n",
      "âŒ æ‰¾ä¸åˆ°æ“´å¢è³‡æ–™é›†è®Šæ•¸ï¼Œè«‹é‡æ–°åŸ·è¡Œæ“´å¢ç¨‹åº\n"
     ]
    }
   ],
   "source": [
    "# ä¿®æ­£å„²å­˜å•é¡Œä¸¦å®Œæˆæ“´å¢è³‡æ–™é›†å„²å­˜\n",
    "print(\"ğŸ”§ ä¿®æ­£å„²å­˜æ ¼å¼ä¸¦å®Œæˆè³‡æ–™é›†å„²å­˜...\")\n",
    "\n",
    "if 'augmented_dataset_df' in globals():\n",
    "    # é‡æ–°å„²å­˜\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    augmented_dir = \"augmented_datasets\"\n",
    "    os.makedirs(augmented_dir, exist_ok=True)\n",
    "    \n",
    "    base_filename = f\"{augmented_dir}/clue_corpus_augmented_{timestamp}\"\n",
    "    \n",
    "    # CSV æ ¼å¼\n",
    "    csv_filename = f\"{base_filename}.csv\"\n",
    "    augmented_dataset_df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    # JSON æ ¼å¼ (ä¿®æ­£ç‰ˆæœ¬)\n",
    "    json_filename = f\"{base_filename}.json\"\n",
    "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(augmented_dataset_df.to_dict('records'), f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Parquet æ ¼å¼\n",
    "    parquet_filename = f\"{base_filename}.parquet\"\n",
    "    augmented_dataset_df.to_parquet(parquet_filename, index=False)\n",
    "    \n",
    "    print(f\"âœ… æ“´å¢è³‡æ–™é›†å„²å­˜å®Œæˆ:\")\n",
    "    print(f\"  CSV: {csv_filename}\")\n",
    "    print(f\"  JSON: {json_filename}\")\n",
    "    print(f\"  Parquet: {parquet_filename}\")\n",
    "    \n",
    "    # æª”æ¡ˆå¤§å°çµ±è¨ˆ\n",
    "    print(f\"\\nğŸ“ æª”æ¡ˆå¤§å°:\")\n",
    "    for filename in [csv_filename, json_filename, parquet_filename]:\n",
    "        if os.path.exists(filename):\n",
    "            size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "            print(f\"  {os.path.basename(filename)}: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # æœ€çµ‚çµ±è¨ˆ\n",
    "    print(f\"\\nğŸ“Š æœ€çµ‚æ“´å¢çµ±è¨ˆ:\")\n",
    "    print(f\"  è³‡æ–™é›†å½¢ç‹€: {augmented_dataset_df.shape}\")\n",
    "    print(f\"  æ¬„ä½: {list(augmented_dataset_df.columns)}\")\n",
    "    \n",
    "    # å“è³ªæª¢æŸ¥\n",
    "    print(f\"\\nğŸ” è³‡æ–™å“è³ªæª¢æŸ¥:\")\n",
    "    empty_texts = augmented_dataset_df['text'].str.strip().eq('').sum()\n",
    "    duplicate_texts = augmented_dataset_df.duplicated(subset=['text']).sum()\n",
    "    avg_length = augmented_dataset_df['text'].str.len().mean()\n",
    "    \n",
    "    print(f\"  ç©ºç™½æ–‡æœ¬: {empty_texts} ç­†\")\n",
    "    print(f\"  é‡è¤‡æ–‡æœ¬: {duplicate_texts} ç­†\")\n",
    "    print(f\"  å¹³å‡æ–‡æœ¬é•·åº¦: {avg_length:.1f} å­—ç¬¦\")\n",
    "    \n",
    "    # æ“´å¢æ–¹æ³•çµ±è¨ˆ\n",
    "    if 'augmentation_method' in augmented_dataset_df.columns:\n",
    "        print(f\"\\nğŸ“ˆ æ“´å¢æ–¹æ³•åˆ†å¸ƒ:\")\n",
    "        method_counts = augmented_dataset_df['augmentation_method'].value_counts()\n",
    "        for method, count in method_counts.items():\n",
    "            print(f\"  {method}: {count} ç­†\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ è³‡æ–™æ“´å¢å°ˆæ¡ˆå®Œæˆï¼\")\n",
    "    print(f\"ğŸ¯ åŸå§‹è³‡æ–™é›†: 50 ç­† â†’ æ“´å¢è³‡æ–™é›†: {len(augmented_dataset_df)} ç­†\")\n",
    "    print(f\"ğŸ“ˆ æ“´å¢å€ç‡: {len(augmented_dataset_df) / 50:.1f}x\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ æ‰¾ä¸åˆ°æ“´å¢è³‡æ–™é›†è®Šæ•¸ï¼Œè«‹é‡æ–°åŸ·è¡Œæ“´å¢ç¨‹åº\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73723d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æª¢æŸ¥æ“´å¢è³‡æ–™ç‹€æ…‹...\n",
      "ğŸ“‚ æ‰¾åˆ°æ“´å¢è³‡æ–™æª”æ¡ˆ: augmented_datasets/clue_corpus_augmented_20250901_094627.csv\n",
      "âœ… æˆåŠŸè¼‰å…¥æ“´å¢è³‡æ–™é›†\n",
      "\n",
      "ğŸ“Š æ“´å¢è³‡æ–™é›†çµ±è¨ˆ:\n",
      "  è³‡æ–™å½¢ç‹€: (200, 4)\n",
      "  æ¬„ä½: ['text', 'source', 'original_idx', 'augmentation_method']\n",
      "\n",
      "ğŸ“ˆ è³‡æ–™ä¾†æºåˆ†å¸ƒ:\n",
      "source\n",
      "original        50\n",
      "augmented_v1    50\n",
      "augmented_v2    50\n",
      "augmented_v3    50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“ æ–‡æœ¬é•·åº¦çµ±è¨ˆ:\n",
      "count     200.000000\n",
      "mean      327.555000\n",
      "std       423.366262\n",
      "min         8.000000\n",
      "25%        61.500000\n",
      "50%       224.500000\n",
      "75%       403.250000\n",
      "max      2468.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "ğŸ“ æ“´å¢ç¯„ä¾‹å°æ¯”:\n",
      "\n",
      "ORIGINAL:\n",
      "  130çœŸæ˜¯ä½©æœè¿™å®¶åº—å¼€è¿™ä¹ˆä¹…ã€‚å°½ç®¡é—¨é¢å·²ç»å°äº†ä¸€åœˆï¼Œä½†è¿˜æ˜¯å¼€ç€ä¸å®¹æ˜“å•Šã€‚æˆ‘ä»¬ä¸å®¹æ˜“ï¼Œè€æ¿ä¹Ÿä¸å®¹æ˜“ã€‚è‡ªåŠ©é¤ï¼Œä½ å¯ä»¥åƒå¾—æ¯”å¹³æ—¶å¤šï¼Œä½†å†³ä¸èƒ½æµªè´¹ã€‚æƒ³åƒå›20å…ƒï¼Œé‚£æ˜¯ä¸å¯èƒ½çš„ï¼Œæ‰€ä»¥è¿˜æ˜¯ä¸è¦å»äº†ã€‚èœçœŸçš„å¾ˆä¸€èˆ¬ï¼Œ...\n",
      "\n",
      "AUGMENTED_V1:\n",
      "  çœŸè®©äººä½©æœè¿™å®¶åº—èƒ½ç»è¥è¿™ä¹ˆä¹…ã€‚è™½ç„¶é—¨é¢ç¼©å°äº†ä¸€åœˆï¼Œå´ä¾æ—§ç»´æŒè¥ä¸šå¹¶ä¸ç®€å•ã€‚æˆ‘ä»¬å’Œè€æ¿éƒ½ä¸å®¹æ˜“ã€‚è‡ªåŠ©é¤å¯ä»¥åƒå¾—æ¯”å¹³æ—¶å¤šï¼Œä½†ç»ä¸èƒ½æµªè´¹ã€‚æƒ³è¦é€€å›20å…ƒï¼Œæ ¹æœ¬ä¸å¯èƒ½ï¼Œè¿˜æ˜¯åˆ«å»å§ã€‚èœå“çœŸçš„å¾ˆæ™®é€šï¼Œå¹²å‡€å°±è¡Œã€‚æ‰€...\n",
      "\n",
      "AUGMENTED_V2:\n",
      "  æˆ‘æ·±æ„Ÿä½©æœè¿™å®¶åº—ç»è¥è‡³ä»Šã€‚å°½ç®¡é—¨é¢å·²ç•¥æ˜¾ç®€çº¦ï¼Œä½†ä»ä¸æ˜“ç»´æŒã€‚æˆ‘ä»¬ä¸è€æ¿åŒæ ·é¢ä¸´è¯¸å¤šå›°éš¾ã€‚è‡ªåŠ©é¤å¯æ¯”å¹³æ—¶å¤šé£Ÿï¼Œä½†åŠ¡å¿…é¿å…æµªè´¹ã€‚è‹¥æœŸæœ›é€€å›20å…ƒï¼Œææ€•æ— æ³•å®ç°ï¼Œå»ºè®®ä¸å†å‰å¾€ã€‚èœå“è´¨é‡ä¸€èˆ¬ï¼Œåªéœ€æ´—å‡€åé£Ÿç”¨ã€‚...\n",
      "\n",
      "AUGMENTED_V3:\n",
      "  è¿™å®¶åº—å¼€äº†è¿™ä¹ˆä¹…ï¼ŒçœŸè®©äººä½©æœã€‚é—¨é¢è™½å·²å˜å°ï¼Œä½†ä»ä¸æ˜“ç»è¥ã€‚æˆ‘ä»¬å’Œè€æ¿éƒ½ä¸å®¹æ˜“ã€‚è‡ªåŠ©é¤å¯ä»¥æ¯”å¹³æ—¶åƒå¾—å¤šï¼Œä½†ä¸€å®šä¸èƒ½æµªè´¹ã€‚æƒ³è¦å›20å…ƒæ˜¯ä¸å¯èƒ½çš„ï¼Œè¿˜æ˜¯åˆ«å»äº†ã€‚èœå“å¾ˆä¸€èˆ¬ï¼Œæ´—å¹²å‡€å°±è¡Œã€‚æ‰€æœ‰ä¸œè¥¿éƒ½è¦å¦å¤–ä»˜è´¹ï¼Œ...\n",
      "\n",
      "ğŸ” è³‡æ–™å“è³ªæª¢æŸ¥:\n",
      "  ç©ºç™½æ–‡æœ¬: 0 ç­†\n",
      "  é‡è¤‡æ–‡æœ¬: 1 ç­†\n",
      "\n",
      "ğŸ¯ æ“´å¢æ•ˆæœç¸½çµ:\n",
      "  åŸå§‹æ–‡æœ¬: 50 ç­†\n",
      "  æ“´å¢æ–‡æœ¬: 150 ç­†\n",
      "  ç¸½è¨ˆ: 200 ç­†\n",
      "  æ“´å¢å€ç‡: 4.0x\n",
      "\n",
      "âœ… æ“´å¢è³‡æ–™é›†å·²è¼‰å…¥ä¸¦æº–å‚™å°±ç·’ï¼\n",
      "è®Šæ•¸åç¨±: augmented_dataset_df\n"
     ]
    }
   ],
   "source": [
    "# æª¢æŸ¥ä¸¦è®€å–å·²å­˜åœ¨çš„æ“´å¢è³‡æ–™ (å¦‚æœæœ‰çš„è©±)\n",
    "print(\"ğŸ” æª¢æŸ¥æ“´å¢è³‡æ–™ç‹€æ…‹...\")\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦æœ‰ç¾æœ‰çš„æ“´å¢è³‡æ–™æª”æ¡ˆ\n",
    "augmented_dir = \"augmented_datasets\"\n",
    "if os.path.exists(augmented_dir):\n",
    "    import glob\n",
    "    \n",
    "    # å°‹æ‰¾æœ€æ–°çš„æ“´å¢è³‡æ–™æª”æ¡ˆ\n",
    "    parquet_files = glob.glob(f\"{augmented_dir}/*.parquet\")\n",
    "    csv_files = glob.glob(f\"{augmented_dir}/*.csv\")\n",
    "    \n",
    "    if parquet_files:\n",
    "        # ä½¿ç”¨æœ€æ–°çš„ parquet æª”æ¡ˆ\n",
    "        latest_file = max(parquet_files, key=os.path.getctime)\n",
    "        print(f\"ğŸ“‚ æ‰¾åˆ°æ“´å¢è³‡æ–™æª”æ¡ˆ: {latest_file}\")\n",
    "        augmented_dataset_df = pd.read_parquet(latest_file)\n",
    "        print(f\"âœ… æˆåŠŸè¼‰å…¥æ“´å¢è³‡æ–™é›†\")\n",
    "        \n",
    "    elif csv_files:\n",
    "        # ä½¿ç”¨æœ€æ–°çš„ CSV æª”æ¡ˆ\n",
    "        latest_file = max(csv_files, key=os.path.getctime)\n",
    "        print(f\"ğŸ“‚ æ‰¾åˆ°æ“´å¢è³‡æ–™æª”æ¡ˆ: {latest_file}\")\n",
    "        augmented_dataset_df = pd.read_csv(latest_file)\n",
    "        print(f\"âœ… æˆåŠŸè¼‰å…¥æ“´å¢è³‡æ–™é›†\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ æ²’æœ‰æ‰¾åˆ°æ“´å¢è³‡æ–™æª”æ¡ˆ\")\n",
    "        augmented_dataset_df = None\n",
    "else:\n",
    "    print(\"âŒ æ“´å¢è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨\")\n",
    "    augmented_dataset_df = None\n",
    "\n",
    "# å¦‚æœæˆåŠŸè¼‰å…¥ï¼Œé¡¯ç¤ºçµ±è¨ˆè³‡è¨Š\n",
    "if augmented_dataset_df is not None:\n",
    "    print(f\"\\nğŸ“Š æ“´å¢è³‡æ–™é›†çµ±è¨ˆ:\")\n",
    "    print(f\"  è³‡æ–™å½¢ç‹€: {augmented_dataset_df.shape}\")\n",
    "    print(f\"  æ¬„ä½: {list(augmented_dataset_df.columns)}\")\n",
    "    \n",
    "    # è³‡æ–™ä¾†æºåˆ†å¸ƒ\n",
    "    if 'source' in augmented_dataset_df.columns:\n",
    "        print(f\"\\nğŸ“ˆ è³‡æ–™ä¾†æºåˆ†å¸ƒ:\")\n",
    "        print(augmented_dataset_df['source'].value_counts())\n",
    "    \n",
    "    # æ–‡æœ¬é•·åº¦çµ±è¨ˆ\n",
    "    augmented_dataset_df['text_length'] = augmented_dataset_df['text'].str.len()\n",
    "    print(f\"\\nğŸ“ æ–‡æœ¬é•·åº¦çµ±è¨ˆ:\")\n",
    "    print(augmented_dataset_df['text_length'].describe())\n",
    "    \n",
    "    # é¡¯ç¤ºä¸€äº›ç¯„ä¾‹\n",
    "    print(f\"\\nğŸ“ æ“´å¢ç¯„ä¾‹å°æ¯”:\")\n",
    "    \n",
    "    # æ‰¾ä¸€å€‹åŸå§‹æ–‡æœ¬å’Œå…¶æ“´å¢ç‰ˆæœ¬\n",
    "    if 'original_idx' in augmented_dataset_df.columns:\n",
    "        sample_idx = 0\n",
    "        sample_data = augmented_dataset_df[augmented_dataset_df['original_idx'] == sample_idx]\n",
    "        \n",
    "        for i, row in sample_data.iterrows():\n",
    "            source = row['source']\n",
    "            text = row['text']\n",
    "            preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            print(f\"\\n{source.upper()}:\")\n",
    "            print(f\"  {preview}\")\n",
    "    \n",
    "    # å“è³ªæª¢æŸ¥\n",
    "    print(f\"\\nğŸ” è³‡æ–™å“è³ªæª¢æŸ¥:\")\n",
    "    empty_texts = augmented_dataset_df['text'].str.strip().eq('').sum()\n",
    "    duplicate_texts = augmented_dataset_df.duplicated(subset=['text']).sum()\n",
    "    \n",
    "    print(f\"  ç©ºç™½æ–‡æœ¬: {empty_texts} ç­†\")\n",
    "    print(f\"  é‡è¤‡æ–‡æœ¬: {duplicate_texts} ç­†\")\n",
    "    \n",
    "    # è¨ˆç®—æ“´å¢æ•ˆæœ\n",
    "    original_count = (augmented_dataset_df['source'] == 'original').sum()\n",
    "    augmented_count = len(augmented_dataset_df) - original_count\n",
    "    \n",
    "    print(f\"\\nğŸ¯ æ“´å¢æ•ˆæœç¸½çµ:\")\n",
    "    print(f\"  åŸå§‹æ–‡æœ¬: {original_count} ç­†\")\n",
    "    print(f\"  æ“´å¢æ–‡æœ¬: {augmented_count} ç­†\")\n",
    "    print(f\"  ç¸½è¨ˆ: {len(augmented_dataset_df)} ç­†\")\n",
    "    print(f\"  æ“´å¢å€ç‡: {len(augmented_dataset_df) / original_count:.1f}x\")\n",
    "    \n",
    "    # å„²å­˜åˆ°å…¨åŸŸè®Šæ•¸\n",
    "    globals()['augmented_dataset_df'] = augmented_dataset_df\n",
    "    \n",
    "    print(f\"\\nâœ… æ“´å¢è³‡æ–™é›†å·²è¼‰å…¥ä¸¦æº–å‚™å°±ç·’ï¼\")\n",
    "    print(f\"è®Šæ•¸åç¨±: augmented_dataset_df\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâŒ ç„¡æ³•è¼‰å…¥æ“´å¢è³‡æ–™é›†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4357b53",
   "metadata": {},
   "source": [
    "## ğŸ¯ æœ€çµ‚ç‰ˆå¤§é™¸ç”¨èªè­˜åˆ¥èˆ‡ç¯©é¸ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2c275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å•Ÿå‹•æœ€çµ‚ç‰ˆå¤§é™¸ç”¨èªè­˜åˆ¥ç³»çµ±...\n",
      "============================================================\n",
      "âœ… ä½¿ç”¨ æ“´å¢è³‡æ–™é›†ï¼Œå…± 200 ç­†è¨˜éŒ„\n",
      "\n",
      "ğŸ¯ é–‹å§‹åŸ·è¡Œå¤§é™¸ç”¨èªç¯©é¸...\n",
      "ğŸ“Š è™•ç†è³‡æ–™é›†ï¼š200 ç­†è¨˜éŒ„\n",
      "ğŸ“ æ–‡æœ¬æ¬„ä½ï¼štext\n",
      "ğŸ¯ æ¨£æœ¬å¤§å°ï¼š50\n",
      "âš–ï¸ ç¯©é¸é–¾å€¼ï¼š3/5\n",
      "\n",
      "ğŸ”„ é–‹å§‹ Ollama æ¨è«–...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ¨è«–é€²åº¦:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ç¬¬1ç­†: 1/5 - é€šç”¨ç°¡é«”ä¸­æ–‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ¨è«–é€²åº¦:   2%|â–         | 1/50 [00:17<14:39, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ç¬¬2ç­†: å¤±æ•— - é€šç”¨ç°¡é«”ä¸­æ–‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ¨è«–é€²åº¦:   4%|â–         | 2/50 [07:34<3:31:27, 264.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ç¬¬3ç­†: 1/5 - é€šç”¨ç°¡é«”ä¸­æ–‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ¨è«–é€²åº¦:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [14:44<13:54, 27.80s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ç¬¬21ç­†: 1/5 - é€šç”¨ç°¡é«”ä¸­æ–‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ¨è«–é€²åº¦:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [34:07<03:49, 17.62s/it]   "
     ]
    }
   ],
   "source": [
    "# ğŸ¯ æœ€çµ‚ç‰ˆå¤§é™¸ç”¨èªè­˜åˆ¥èˆ‡ç¯©é¸ç³»çµ± - ä½¿ç”¨ Ollama æ¨è«–ä¸¦å„²å­˜çµæœ\n",
    "print(\"ğŸš€ å•Ÿå‹•æœ€çµ‚ç‰ˆå¤§é™¸ç”¨èªè­˜åˆ¥ç³»çµ±...\")\n",
    "\n",
    "# å®šç¾©å¤§é™¸ç‰¹æœ‰è©å½™åº«\n",
    "mainland_terms = {\n",
    "    \"è¨ˆç®—æ©Ÿ\": [\"é›»è…¦\"], \"è»Ÿä»¶\": [\"è»Ÿé«”\"], \"ç¡¬ä»¶\": [\"ç¡¬é«”\"], \"ç¶²çµ¡\": [\"ç¶²è·¯\"], \n",
    "    \"æ•¸æ“š\": [\"è³‡æ–™\"], \"ç¨‹åº\": [\"ç¨‹å¼\"], \"ä¿¡æ¯\": [\"è³‡è¨Š\"], \"å‡ºç§Ÿè»Š\": [\"è¨ˆç¨‹è»Š\"],\n",
    "    \"å…¬äº¤è»Š\": [\"å…¬è»Š\"], \"åœ°éµ\": [\"æ·é‹\"], \"è³ªé‡\": [\"å“è³ª\"], \"æœå‹™å“¡\": [\"æœå‹™ç”Ÿ\"],\n",
    "    \"åœŸè±†\": [\"é¦¬éˆ´è–¯\"], \"è¥¿ç´…æŸ¿\": [\"ç•ªèŒ„\"], \"æå®š\": [\"å®Œæˆ\"], \"æŒº\": [\"å¾ˆ\"],\n",
    "    \"å’‹\": [\"æ€éº¼\"], \"å•¥\": [\"ä»€éº¼\"], \"å¾®ä¿¡\": [\"\"], \"æ”¯ä»˜å¯¶\": [\"\"], \"æ·˜å¯¶\": [\"\"]\n",
    "}\n",
    "\n",
    "# å¤§é™¸èªæ³•æ¨¡å¼\n",
    "mainland_patterns = [r\"æŒº.*çš„\", r\"è ».*çš„\", r\".*å¾—å¾ˆ\", r\"å’‹.*\", r\"å•¥.*\"]\n",
    "\n",
    "def analyze_features(text):\n",
    "    \"\"\"å¿«é€Ÿç‰¹å¾µåˆ†æ\"\"\"\n",
    "    mainland_count = sum(1 for term in mainland_terms if term in text)\n",
    "    pattern_count = sum(1 for pattern in mainland_patterns if re.search(pattern, text))\n",
    "    return {\n",
    "        \"mainland_terms\": [term for term in mainland_terms if term in text],\n",
    "        \"pattern_matches\": pattern_count,\n",
    "        \"authenticity_score\": mainland_count + pattern_count\n",
    "    }\n",
    "\n",
    "def mainland_score_ollama(text, model=\"gpt-oss:20b\"):\n",
    "    \"\"\"ä½¿ç”¨ Ollama è©•åˆ†å¤§é™¸ç”¨èªç‰¹å¾µ\"\"\"\n",
    "    prompt = f\"\"\"è©•ä¼°æ–‡æœ¬çš„å¤§é™¸ç”¨èªç‰¹å¾µï¼Œæ¯é …0æˆ–1åˆ†ï¼š\n",
    "\n",
    "æ–‡æœ¬ï¼š{text}\n",
    "\n",
    "è©•åˆ†æ¨™æº–ï¼š\n",
    "1. å¤§é™¸ç‰¹æœ‰è©å½™ï¼šè¨ˆç®—æ©Ÿã€è»Ÿä»¶ã€å‡ºç§Ÿè»Šã€åœ°éµç­‰\n",
    "2. å¤§é™¸èªæ³•ç¿’æ…£ï¼šæŒº...çš„ã€è »...çš„ã€å’‹æ¨£ç­‰  \n",
    "3. å¤§é™¸å£èªè¡¨é”ï¼šæå®šã€æ•´ã€å¼„ç­‰\n",
    "4. é¿å…ç¹é«”ç”¨èªï¼šä¸å«é›»è…¦ã€è»Ÿé«”ã€è³‡æ–™ç­‰\n",
    "5. æ•´é«”å¤§é™¸åŒ–ç¨‹åº¦ï¼šç¶œåˆè©•ä¼°\n",
    "\n",
    "è«‹æŒ‰æ ¼å¼å›ç­”ï¼š\n",
    "å¤§é™¸ç‰¹æœ‰è©å½™:0\n",
    "å¤§é™¸èªæ³•ç¿’æ…£:0\n",
    "å¤§é™¸å£èªè¡¨é”:0\n",
    "é¿å…ç¹é«”ç”¨èª:1\n",
    "æ•´é«”å¤§é™¸åŒ–ç¨‹åº¦:0\n",
    "ç¸½åˆ†:1\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options={'temperature': 0.1, 'max_tokens': 100}\n",
    "        )\n",
    "        \n",
    "        # è§£æå›æ‡‰\n",
    "        scores = {}\n",
    "        total = 0\n",
    "        categories = [\"å¤§é™¸ç‰¹æœ‰è©å½™\", \"å¤§é™¸èªæ³•ç¿’æ…£\", \"å¤§é™¸å£èªè¡¨é”\", \"é¿å…ç¹é«”ç”¨èª\", \"æ•´é«”å¤§é™¸åŒ–ç¨‹åº¦\"]\n",
    "        \n",
    "        for line in response['response'].split('\\n'):\n",
    "            for cat in categories:\n",
    "                if cat in line:\n",
    "                    match = re.search(r'[ï¼š:]\\s*([01])', line)\n",
    "                    if match:\n",
    "                        score = int(match.group(1))\n",
    "                        scores[cat] = score\n",
    "                        total += score\n",
    "        \n",
    "        if len(scores) == 5:\n",
    "            scores[\"ç¸½åˆ†\"] = total\n",
    "            return scores, response['response']\n",
    "        return None, response['response']\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def process_dataset(df, text_col='text', sample_size=100, threshold=3):\n",
    "    \"\"\"è™•ç†è³‡æ–™é›†é€²è¡Œå¤§é™¸ç”¨èªç¯©é¸\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“Š è™•ç†è³‡æ–™é›†ï¼š{len(df)} ç­†è¨˜éŒ„\")\n",
    "    print(f\"ğŸ“ æ–‡æœ¬æ¬„ä½ï¼š{text_col}\")\n",
    "    print(f\"ğŸ¯ æ¨£æœ¬å¤§å°ï¼š{sample_size}\")\n",
    "    print(f\"âš–ï¸ ç¯©é¸é–¾å€¼ï¼š{threshold}/5\")\n",
    "    \n",
    "    # å–æ¨£æœ¬\n",
    "    sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    texts = sample_df[text_col].tolist()\n",
    "    \n",
    "    # åŸ·è¡Œæ¨è«–\n",
    "    results = []\n",
    "    authentic_texts = []\n",
    "    generic_texts = []\n",
    "    \n",
    "    print(f\"\\nğŸ”„ é–‹å§‹ Ollama æ¨è«–...\")\n",
    "    \n",
    "    for i, text in enumerate(tqdm(texts, desc=\"æ¨è«–é€²åº¦\")):\n",
    "        # ç‰¹å¾µåˆ†æ\n",
    "        features = analyze_features(text)\n",
    "        \n",
    "        # Ollama è©•åˆ†\n",
    "        scores, response = mainland_score_ollama(text)\n",
    "        \n",
    "        result = {\n",
    "            'index': i,\n",
    "            'text': text,\n",
    "            'text_length': len(text),\n",
    "            'features': features,\n",
    "            'scores': scores,\n",
    "            'response': response,\n",
    "            'success': scores is not None\n",
    "        }\n",
    "        \n",
    "        # åˆ†é¡\n",
    "        if scores and scores.get(\"ç¸½åˆ†\", 0) >= threshold:\n",
    "            authentic_texts.append(result)\n",
    "            category = \"çœŸæ­£å¤§é™¸ç”¨èª\"\n",
    "        else:\n",
    "            generic_texts.append(result)\n",
    "            category = \"é€šç”¨ç°¡é«”ä¸­æ–‡\"\n",
    "        \n",
    "        result['category'] = category\n",
    "        results.append(result)\n",
    "        \n",
    "        # é¡¯ç¤ºé€²åº¦\n",
    "        if i % 20 == 0 or i < 3:\n",
    "            score_str = f\"{scores['ç¸½åˆ†']}/5\" if scores else \"å¤±æ•—\"\n",
    "            print(f\"  ç¬¬{i+1}ç­†: {score_str} - {category}\")\n",
    "        \n",
    "        time.sleep(0.2)  # æ§åˆ¶è«‹æ±‚é »ç‡\n",
    "    \n",
    "    return results, authentic_texts, generic_texts\n",
    "\n",
    "def save_results(results, authentic_texts, generic_texts):\n",
    "    \"\"\"å„²å­˜ç¯©é¸çµæœ\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. å®Œæ•´çµæœ\n",
    "    full_data = []\n",
    "    for r in results:\n",
    "        row = {\n",
    "            'text': r['text'],\n",
    "            'text_length': r['text_length'],\n",
    "            'category': r['category'],\n",
    "            'success': r['success'],\n",
    "            'authenticity_score': r['features']['authenticity_score'],\n",
    "            'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "        }\n",
    "        if r['scores']:\n",
    "            row.update({f'score_{k}': v for k, v in r['scores'].items()})\n",
    "        full_data.append(row)\n",
    "    \n",
    "    full_df = pd.DataFrame(full_data)\n",
    "    full_file = f\"mainland_filtering_complete_{timestamp}.csv\"\n",
    "    full_df.to_csv(full_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. é«˜è³ªé‡å¤§é™¸ç”¨èªæ•¸æ“š\n",
    "    if authentic_texts:\n",
    "        authentic_data = [{\n",
    "            'text': r['text'],\n",
    "            'total_score': r['scores']['ç¸½åˆ†'],\n",
    "            'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "        } for r in authentic_texts]\n",
    "        \n",
    "        auth_df = pd.DataFrame(authentic_data)\n",
    "        auth_csv = f\"authentic_mainland_texts_{timestamp}.csv\"\n",
    "        auth_json = f\"authentic_mainland_texts_{timestamp}.json\"\n",
    "        \n",
    "        auth_df.to_csv(auth_csv, index=False, encoding='utf-8-sig')\n",
    "        auth_df.to_json(auth_json, orient='records', force_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"ğŸ’¾ å„²å­˜å®Œæˆ:\")\n",
    "        print(f\"  ğŸ“„ å®Œæ•´çµæœ: {full_file}\")\n",
    "        print(f\"  âœ… é«˜è³ªé‡æ•¸æ“š: {auth_csv}\")\n",
    "        print(f\"  ğŸ“‹ JSONæ ¼å¼: {auth_json}\")\n",
    "        \n",
    "        return full_df, auth_df\n",
    "    \n",
    "    return full_df, None\n",
    "\n",
    "# ä¸»è¦åŸ·è¡Œæµç¨‹\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æª¢æŸ¥å¯ç”¨è³‡æ–™é›†\n",
    "available_data = None\n",
    "text_column = 'text'\n",
    "\n",
    "if 'augmented_dataset_df' in locals() and augmented_dataset_df is not None:\n",
    "    available_data = augmented_dataset_df\n",
    "    source_name = \"æ“´å¢è³‡æ–™é›†\"\n",
    "elif 'dataset_df' in locals() and dataset_df is not None:\n",
    "    available_data = dataset_df  \n",
    "    source_name = \"åŸå§‹è³‡æ–™é›†\"\n",
    "\n",
    "if available_data is not None:\n",
    "    print(f\"âœ… ä½¿ç”¨ {source_name}ï¼Œå…± {len(available_data)} ç­†è¨˜éŒ„\")\n",
    "    \n",
    "    # åŸ·è¡Œç¯©é¸ï¼ˆå¯èª¿æ•´åƒæ•¸ï¼‰\n",
    "    SAMPLE_SIZE = 50    # è™•ç†æ¨£æœ¬æ•¸é‡\n",
    "    THRESHOLD = 3       # ç¯©é¸é–¾å€¼\n",
    "    \n",
    "    print(f\"\\nğŸ¯ é–‹å§‹åŸ·è¡Œå¤§é™¸ç”¨èªç¯©é¸...\")\n",
    "    results, authentic_results, generic_results = process_dataset(\n",
    "        df=available_data,\n",
    "        text_col=text_column,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        threshold=THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # çµ±è¨ˆçµæœ\n",
    "    print(f\"\\nğŸ“Š ç¯©é¸çµæœçµ±è¨ˆ:\")\n",
    "    print(f\"  âœ… çœŸæ­£å¤§é™¸ç”¨èª: {len(authentic_results)} ç­†\")\n",
    "    print(f\"  ğŸ—‘ï¸ é€šç”¨ç°¡é«”ä¸­æ–‡: {len(generic_results)} ç­†\")\n",
    "    print(f\"  ğŸ“ˆ ç¯©é¸ç‡: {len(authentic_results)/len(results)*100:.1f}%\")\n",
    "    \n",
    "    # é¡¯ç¤ºç¯„ä¾‹\n",
    "    if authentic_results:\n",
    "        print(f\"\\nğŸ“ é«˜è³ªé‡å¤§é™¸ç”¨èªç¯„ä¾‹:\")\n",
    "        for i, r in enumerate(authentic_results[:3]):\n",
    "            preview = r['text'][:60] + \"...\" if len(r['text']) > 60 else r['text']\n",
    "            print(f\"  {i+1}. (å¾—åˆ†:{r['scores']['ç¸½åˆ†']}) {preview}\")\n",
    "    \n",
    "    # å„²å­˜çµæœ\n",
    "    print(f\"\\nğŸ’¾ å„²å­˜çµæœ...\")\n",
    "    full_df, auth_df = save_results(results, authentic_results, generic_results)\n",
    "    \n",
    "    # è¨­å®šå…¨åŸŸè®Šæ•¸\n",
    "    globals()['mainland_filtering_results'] = results\n",
    "    globals()['authentic_mainland_data'] = auth_df\n",
    "    \n",
    "    print(f\"\\nğŸ‰ å¤§é™¸ç”¨èªè­˜åˆ¥èˆ‡ç¯©é¸å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“‹ å¯ç”¨è®Šæ•¸: mainland_filtering_results, authentic_mainland_data\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„è³‡æ–™é›†\")\n",
    "    print(\"ğŸ’¡ è«‹å…ˆåŸ·è¡Œå‰é¢çš„è³‡æ–™è¼‰å…¥æˆ–æ“´å¢æ­¥é©Ÿ\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3199cb7",
   "metadata": {},
   "source": [
    "## ğŸ§ª 30ç­†çœŸå¯¦è³‡æ–™æ¸¬è©¦è…³æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff5f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª 30ç­†çœŸå¯¦è³‡æ–™æ¸¬è©¦è…³æœ¬ - å¤§é™¸ç”¨èªè­˜åˆ¥ç³»çµ±é©—è­‰\n",
    "print(\"ğŸ§ª å•Ÿå‹•30ç­†çœŸå¯¦è³‡æ–™æ¸¬è©¦...\")\n",
    "\n",
    "# æ¸¬è©¦ç”¨çš„30ç­†çœŸå¯¦è³‡æ–™ - åŒ…å«å¤§é™¸ç”¨èªå’Œé€šç”¨ç°¡é«”ä¸­æ–‡\n",
    "test_data = [\n",
    "    # æ˜é¡¯å¤§é™¸ç”¨èª (1-10)\n",
    "    \"æˆ‘çš„è¨ˆç®—æ©Ÿè»Ÿä»¶å‡ºäº†å•é¡Œï¼Œéœ€è¦é‡æ–°å®‰è£ç³»çµ±ã€‚\",\n",
    "    \"æ‰“è»Šå»æ©Ÿå ´æŒºæ–¹ä¾¿çš„ï¼Œå‡ºç§Ÿè»Šå¾ˆå¤šã€‚\",\n",
    "    \"å’‹æ¨£ï¼Ÿé€™å€‹è³ªé‡é‚„å¯ä»¥å—ï¼Ÿ\",\n",
    "    \"æ•¸æ“šåº«çš„ç¡¬ä»¶é…ç½®éœ€è¦å‡ç´šäº†ã€‚\",\n",
    "    \"åœ°éµç«™è£¡çš„æœå‹™å“¡æ…‹åº¦è »å¥½çš„ã€‚\",\n",
    "    \"ç”¨å¾®ä¿¡æ”¯ä»˜å¾ˆæ–¹ä¾¿ï¼Œæ¯”ç¾é‡‘å¿«å¤šäº†ã€‚\",\n",
    "    \"æ™šé£¯æƒ³åƒåœŸè±†ç‡‰ç‰›è‚‰ï¼Œå†ä¾†å€‹è¥¿ç´…æŸ¿é›è›‹æ¹¯ã€‚\",\n",
    "    \"ç¶²çµ¡é€£æ¥æœ‰å•é¡Œï¼Œç¨‹åºé‹è¡Œä¸äº†ã€‚\",\n",
    "    \"é€™äº‹å…’æå®šäº†å—ï¼Ÿåˆ¥å¿˜äº†ç™¼ä¿¡æ¯çµ¦æˆ‘ã€‚\",\n",
    "    \"å•¥æ™‚å€™èƒ½åˆ°ï¼Ÿå…¬äº¤è»Šå µè»Šäº†ã€‚\",\n",
    "    \n",
    "    # ä¸­ç­‰å¤§é™¸ç‰¹å¾µ (11-20)\n",
    "    \"ä»Šå¤©å¤©æ°£æŒºä¸éŒ¯çš„ï¼Œé©åˆå‡ºå»èµ°èµ°ã€‚\",\n",
    "    \"é€™å€‹åƒ¹æ ¼è »åˆç†çš„ï¼Œæ€§åƒ¹æ¯”å¾ˆé«˜ã€‚\",\n",
    "    \"è¾¦å…¬å®¤çš„é›»è…¦éœ€è¦å®‰è£æ–°è»Ÿä»¶ã€‚\",\n",
    "    \"æ·˜å¯¶ä¸Šè²·æ±è¥¿å¾ˆæ–¹ä¾¿ï¼Œå¿«éä¹Ÿå¿«ã€‚\",\n",
    "    \"åœ°ä¸‹åœè»Šå ´çš„è»Šä½æŒºç·Šå¼µçš„ã€‚\",\n",
    "    \"é€™å€‹å•é¡Œæä¸å®šï¼Œéœ€è¦è«‹æ•™å°ˆå®¶ã€‚\",\n",
    "    \"æ”¯ä»˜å¯¶è½‰è³¬å¾ˆå®‰å…¨ï¼Œæ‰‹çºŒè²»ä¹Ÿä½ã€‚\",\n",
    "    \"è³ªé‡æ§åˆ¶åšå¾—ä¸éŒ¯ï¼Œç”¢å“å¾ˆç©©å®šã€‚\",\n",
    "    \"æ•¸æ“šåˆ†æçš„çµæœæŒºæœ‰æ„æ€çš„ã€‚\",\n",
    "    \"ç¶²ä¸Šè³¼ç‰©è¶Šä¾†è¶Šæµè¡Œäº†ã€‚\",\n",
    "    \n",
    "    # é€šç”¨ç°¡é«”ä¸­æ–‡ (21-30) \n",
    "    \"ä»Šå¤©çš„æœƒè­°è¨è«–äº†å¾ˆå¤šé‡è¦è­°é¡Œã€‚\",\n",
    "    \"å­¸ç¿’æ–°æŠ€è¡“éœ€è¦æŒçºŒçš„åŠªåŠ›å’Œå¯¦è¸ã€‚\",\n",
    "    \"é€™æœ¬æ›¸çš„å…§å®¹å¾ˆè±å¯Œï¼Œå€¼å¾—ä»”ç´°é–±è®€ã€‚\",\n",
    "    \"é‹å‹•å°èº«é«”å¥åº·éå¸¸é‡è¦ã€‚\",\n",
    "    \"æ™‚é–“ç®¡ç†æ˜¯æé«˜å·¥ä½œæ•ˆç‡çš„é—œéµã€‚\",\n",
    "    \"ç’°å¢ƒä¿è­·æ˜¯æˆ‘å€‘å…±åŒçš„è²¬ä»»ã€‚\",\n",
    "    \"ç§‘æŠ€ç™¼å±•æ”¹è®Šäº†æˆ‘å€‘çš„ç”Ÿæ´»æ–¹å¼ã€‚\",\n",
    "    \"æ•™è‚²æ˜¯ç¤¾æœƒé€²æ­¥çš„é‡è¦æ¨å‹•åŠ›ã€‚\",\n",
    "    \"è—è¡“å‰µä½œéœ€è¦éˆæ„Ÿå’ŒæŠ€å·§çš„çµåˆã€‚\",\n",
    "    \"åœ˜éšŠåˆä½œèƒ½å¤ å‰µé€ æ›´å¤§çš„åƒ¹å€¼ã€‚\"\n",
    "]\n",
    "\n",
    "# æ‰‹å‹•æ¨™è¨»çš„æ¨™æº–ç­”æ¡ˆ (ç”¨æ–¼è©•ä¼°æº–ç¢ºæ€§)\n",
    "ground_truth = [\n",
    "    # æ˜é¡¯å¤§é™¸ç”¨èª (1-10) - æ‡‰è©²å¾—åˆ† >= 3\n",
    "    True, True, True, True, True, True, True, True, True, True,\n",
    "    # ä¸­ç­‰å¤§é™¸ç‰¹å¾µ (11-20) - å¯èƒ½å¾—åˆ† 2-3\n",
    "    True, True, False, True, False, False, True, False, False, False,\n",
    "    # é€šç”¨ç°¡é«”ä¸­æ–‡ (21-30) - æ‡‰è©²å¾—åˆ† <= 2\n",
    "    False, False, False, False, False, False, False, False, False, False\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“Š æ¸¬è©¦è³‡æ–™çµ±è¨ˆ:\")\n",
    "print(f\"  ç¸½æ¸¬è©¦æ¨£æœ¬: {len(test_data)} ç­†\")\n",
    "print(f\"  é æœŸå¤§é™¸ç”¨èª: {sum(ground_truth)} ç­†\")\n",
    "print(f\"  é æœŸé€šç”¨ä¸­æ–‡: {len(ground_truth) - sum(ground_truth)} ç­†\")\n",
    "\n",
    "# é‡è¤‡ä½¿ç”¨ç¾æœ‰çš„åˆ†æå‡½æ•¸\n",
    "def analyze_features(text):\n",
    "    \"\"\"å¿«é€Ÿç‰¹å¾µåˆ†æ\"\"\"\n",
    "    mainland_terms = {\n",
    "        \"è¨ˆç®—æ©Ÿ\": [\"é›»è…¦\"], \"è»Ÿä»¶\": [\"è»Ÿé«”\"], \"ç¡¬ä»¶\": [\"ç¡¬é«”\"], \"ç¶²çµ¡\": [\"ç¶²è·¯\"], \n",
    "        \"æ•¸æ“š\": [\"è³‡æ–™\"], \"ç¨‹åº\": [\"ç¨‹å¼\"], \"ä¿¡æ¯\": [\"è³‡è¨Š\"], \"å‡ºç§Ÿè»Š\": [\"è¨ˆç¨‹è»Š\"],\n",
    "        \"å…¬äº¤è»Š\": [\"å…¬è»Š\"], \"åœ°éµ\": [\"æ·é‹\"], \"è³ªé‡\": [\"å“è³ª\"], \"æœå‹™å“¡\": [\"æœå‹™ç”Ÿ\"],\n",
    "        \"åœŸè±†\": [\"é¦¬éˆ´è–¯\"], \"è¥¿ç´…æŸ¿\": [\"ç•ªèŒ„\"], \"æå®š\": [\"å®Œæˆ\"], \"æŒº\": [\"å¾ˆ\"],\n",
    "        \"å’‹\": [\"æ€éº¼\"], \"å•¥\": [\"ä»€éº¼\"], \"å¾®ä¿¡\": [\"\"], \"æ”¯ä»˜å¯¶\": [\"\"], \"æ·˜å¯¶\": [\"\"]\n",
    "    }\n",
    "    \n",
    "    mainland_patterns = [r\"æŒº.*çš„\", r\"è ».*çš„\", r\".*å¾—å¾ˆ\", r\"å’‹.*\", r\"å•¥.*\"]\n",
    "    \n",
    "    mainland_count = sum(1 for term in mainland_terms if term in text)\n",
    "    pattern_count = sum(1 for pattern in mainland_patterns if re.search(pattern, text))\n",
    "    found_terms = [term for term in mainland_terms if term in text]\n",
    "    \n",
    "    return {\n",
    "        \"mainland_terms\": found_terms,\n",
    "        \"pattern_matches\": pattern_count,\n",
    "        \"authenticity_score\": mainland_count + pattern_count\n",
    "    }\n",
    "\n",
    "def mainland_score_ollama_test(text, model=\"gpt-oss:20b\"):\n",
    "    \"\"\"æ¸¬è©¦ç”¨çš„ Ollama è©•åˆ†å‡½æ•¸\"\"\"\n",
    "    prompt = f\"\"\"è©•ä¼°ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬çš„å¤§é™¸ç”¨èªç‰¹å¾µï¼ŒæŒ‰ç…§5å€‹æ¨™æº–å„çµ¦0æˆ–1åˆ†ï¼š\n",
    "\n",
    "æ–‡æœ¬ï¼š{text}\n",
    "\n",
    "è©•åˆ†æ¨™æº–ï¼š\n",
    "1. å¤§é™¸ç‰¹æœ‰è©å½™ (è¨ˆç®—æ©Ÿã€è»Ÿä»¶ã€å‡ºç§Ÿè»Šã€åœ°éµç­‰): 0/1åˆ†\n",
    "2. å¤§é™¸èªæ³•ç¿’æ…£ (æŒº...çš„ã€è »...çš„ã€å’‹æ¨£ç­‰): 0/1åˆ†  \n",
    "3. å¤§é™¸å£èªè¡¨é” (æå®šã€æ•´ã€å¼„ç­‰): 0/1åˆ†\n",
    "4. é¿å…ç¹é«”ç”¨èª (ä¸å«é›»è…¦ã€è»Ÿé«”ã€è³‡æ–™ç­‰): 0/1åˆ†\n",
    "5. æ•´é«”å¤§é™¸åŒ–ç¨‹åº¦ (ç¶œåˆåˆ¤æ–·): 0/1åˆ†\n",
    "\n",
    "è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼Œæ¯è¡Œä¸€å€‹åˆ†æ•¸ï¼š\n",
    "å¤§é™¸ç‰¹æœ‰è©å½™:0\n",
    "å¤§é™¸èªæ³•ç¿’æ…£:0\n",
    "å¤§é™¸å£èªè¡¨é”:0\n",
    "é¿å…ç¹é«”ç”¨èª:0\n",
    "æ•´é«”å¤§é™¸åŒ–ç¨‹åº¦:0\n",
    "ç¸½åˆ†:0\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options={'temperature': 0.1, 'max_tokens': 150}\n",
    "        )\n",
    "        \n",
    "        # è§£æå›æ‡‰\n",
    "        scores = {}\n",
    "        categories = [\"å¤§é™¸ç‰¹æœ‰è©å½™\", \"å¤§é™¸èªæ³•ç¿’æ…£\", \"å¤§é™¸å£èªè¡¨é”\", \"é¿å…ç¹é«”ç”¨èª\", \"æ•´é«”å¤§é™¸åŒ–ç¨‹åº¦\"]\n",
    "        \n",
    "        response_text = response['response']\n",
    "        \n",
    "        for cat in categories:\n",
    "            pattern = rf\"{cat}[ï¼š:]\\s*([01])\"\n",
    "            match = re.search(pattern, response_text)\n",
    "            if match:\n",
    "                scores[cat] = int(match.group(1))\n",
    "        \n",
    "        # è¨ˆç®—ç¸½åˆ†\n",
    "        if len(scores) == 5:\n",
    "            total = sum(scores.values())\n",
    "            scores[\"ç¸½åˆ†\"] = total\n",
    "            return scores, response_text, True\n",
    "        else:\n",
    "            return None, response_text, False\n",
    "            \n",
    "    except Exception as e:\n",
    "        return None, str(e), False\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦\n",
    "print(f\"\\nğŸ”„ é–‹å§‹åŸ·è¡Œæ¸¬è©¦...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_results = []\n",
    "correct_predictions = 0\n",
    "total_processed = 0\n",
    "\n",
    "# æ¸¬è©¦æ¯ä¸€ç­†è³‡æ–™\n",
    "for i, text in enumerate(tqdm(test_data, desc=\"æ¸¬è©¦é€²åº¦\")):\n",
    "    print(f\"\\nç¬¬ {i+1} ç­†æ¸¬è©¦:\")\n",
    "    print(f\"æ–‡æœ¬: {text[:50]}{'...' if len(text) > 50 else ''}\")\n",
    "    \n",
    "    # ç‰¹å¾µåˆ†æ\n",
    "    features = analyze_features(text)\n",
    "    print(f\"ç‰¹å¾µ: {features['mainland_terms']} (æ¨¡å¼:{features['pattern_matches']})\")\n",
    "    \n",
    "    # Ollama è©•åˆ†\n",
    "    scores, response, success = mainland_score_ollama_test(text)\n",
    "    \n",
    "    if success and scores:\n",
    "        total_score = scores[\"ç¸½åˆ†\"]\n",
    "        predicted_mainland = total_score >= 3  # é–¾å€¼ç‚º3\n",
    "        actual_mainland = ground_truth[i]\n",
    "        \n",
    "        # åˆ¤æ–·é æ¸¬æ˜¯å¦æ­£ç¢º\n",
    "        is_correct = predicted_mainland == actual_mainland\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        print(f\"è©•åˆ†: {total_score}/5 â†’ {'å¤§é™¸ç”¨èª' if predicted_mainland else 'é€šç”¨ä¸­æ–‡'}\")\n",
    "        print(f\"å¯¦éš›: {'å¤§é™¸ç”¨èª' if actual_mainland else 'é€šç”¨ä¸­æ–‡'} â†’ {'âœ…' if is_correct else 'âŒ'}\")\n",
    "        \n",
    "        # è©³ç´°åˆ†æ•¸\n",
    "        score_details = \" | \".join([f\"{k}:{v}\" for k, v in scores.items() if k != \"ç¸½åˆ†\"])\n",
    "        print(f\"è©³ç´°: {score_details}\")\n",
    "        \n",
    "        total_processed += 1\n",
    "        \n",
    "    else:\n",
    "        print(f\"âŒ è©•åˆ†å¤±æ•—: {response}\")\n",
    "        predicted_mainland = None\n",
    "        is_correct = False\n",
    "    \n",
    "    # å„²å­˜çµæœ\n",
    "    result = {\n",
    "        'index': i,\n",
    "        'text': text,\n",
    "        'features': features,\n",
    "        'scores': scores,\n",
    "        'predicted_mainland': predicted_mainland,\n",
    "        'actual_mainland': ground_truth[i],\n",
    "        'correct': is_correct,\n",
    "        'success': success\n",
    "    }\n",
    "    test_results.append(result)\n",
    "    \n",
    "    time.sleep(0.2)  # æ§åˆ¶è«‹æ±‚é »ç‡\n",
    "\n",
    "# è¨ˆç®—æ¸¬è©¦çµæœçµ±è¨ˆ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š æ¸¬è©¦çµæœçµ±è¨ˆ:\")\n",
    "\n",
    "if total_processed > 0:\n",
    "    accuracy = correct_predictions / total_processed * 100\n",
    "    print(f\"âœ… ç¸½é«”æº–ç¢ºç‡: {accuracy:.1f}% ({correct_predictions}/{total_processed})\")\n",
    "    \n",
    "    # åˆ†é¡çµ±è¨ˆ\n",
    "    true_positives = sum(1 for r in test_results if r['predicted_mainland'] and r['actual_mainland'] and r['success'])\n",
    "    true_negatives = sum(1 for r in test_results if not r['predicted_mainland'] and not r['actual_mainland'] and r['success'])\n",
    "    false_positives = sum(1 for r in test_results if r['predicted_mainland'] and not r['actual_mainland'] and r['success'])\n",
    "    false_negatives = sum(1 for r in test_results if not r['predicted_mainland'] and r['actual_mainland'] and r['success'])\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ åˆ†é¡è©³ç´°çµ±è¨ˆ:\")\n",
    "    print(f\"  çœŸé™½æ€§ (æ­£ç¢ºè­˜åˆ¥å¤§é™¸ç”¨èª): {true_positives}\")\n",
    "    print(f\"  çœŸé™°æ€§ (æ­£ç¢ºè­˜åˆ¥é€šç”¨ä¸­æ–‡): {true_negatives}\")\n",
    "    print(f\"  å‡é™½æ€§ (èª¤åˆ¤ç‚ºå¤§é™¸ç”¨èª): {false_positives}\")\n",
    "    print(f\"  å‡é™°æ€§ (éºæ¼å¤§é™¸ç”¨èª): {false_negatives}\")\n",
    "    \n",
    "    # è¨ˆç®—ç²¾ç¢ºç‡å’Œå¬å›ç‡\n",
    "    if (true_positives + false_positives) > 0:\n",
    "        precision = true_positives / (true_positives + false_positives) * 100\n",
    "        print(f\"  ç²¾ç¢ºç‡: {precision:.1f}%\")\n",
    "    \n",
    "    if (true_positives + false_negatives) > 0:\n",
    "        recall = true_positives / (true_positives + false_negatives) * 100\n",
    "        print(f\"  å¬å›ç‡: {recall:.1f}%\")\n",
    "\n",
    "# åˆ†æéŒ¯èª¤æ¡ˆä¾‹\n",
    "print(f\"\\nğŸ” éŒ¯èª¤æ¡ˆä¾‹åˆ†æ:\")\n",
    "errors = [r for r in test_results if not r['correct'] and r['success']]\n",
    "\n",
    "if errors:\n",
    "    print(f\"ç™¼ç¾ {len(errors)} å€‹éŒ¯èª¤æ¡ˆä¾‹:\")\n",
    "    for err in errors[:5]:  # åªé¡¯ç¤ºå‰5å€‹éŒ¯èª¤\n",
    "        text_preview = err['text'][:40] + \"...\" if len(err['text']) > 40 else err['text']\n",
    "        predicted = \"å¤§é™¸\" if err['predicted_mainland'] else \"é€šç”¨\"\n",
    "        actual = \"å¤§é™¸\" if err['actual_mainland'] else \"é€šç”¨\"\n",
    "        score = err['scores']['ç¸½åˆ†'] if err['scores'] else \"N/A\"\n",
    "        print(f\"  âŒ é æ¸¬:{predicted} | å¯¦éš›:{actual} | åˆ†æ•¸:{score} | {text_preview}\")\n",
    "else:\n",
    "    print(\"ğŸ‰ æ²’æœ‰ç™¼ç¾éŒ¯èª¤æ¡ˆä¾‹ï¼\")\n",
    "\n",
    "# å„²å­˜æ¸¬è©¦çµæœ\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "test_results_file = f\"test_results_30samples_{timestamp}.csv\"\n",
    "\n",
    "# æº–å‚™å„²å­˜çš„è³‡æ–™\n",
    "save_data = []\n",
    "for r in test_results:\n",
    "    row = {\n",
    "        'text': r['text'],\n",
    "        'predicted_mainland': r['predicted_mainland'],\n",
    "        'actual_mainland': r['actual_mainland'],\n",
    "        'correct': r['correct'],\n",
    "        'authenticity_score': r['features']['authenticity_score'],\n",
    "        'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "    }\n",
    "    \n",
    "    if r['scores']:\n",
    "        row.update({f'score_{k.replace(\":\", \"_\")}': v for k, v in r['scores'].items()})\n",
    "    \n",
    "    save_data.append(row)\n",
    "\n",
    "# å„²å­˜ç‚ºCSV\n",
    "test_df = pd.DataFrame(save_data)\n",
    "test_df.to_csv(test_results_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\nğŸ’¾ æ¸¬è©¦çµæœå·²å„²å­˜: {test_results_file}\")\n",
    "print(f\"ğŸ“ æª”æ¡ˆå¤§å°: {os.path.getsize(test_results_file)/1024:.1f} KB\")\n",
    "\n",
    "# å„²å­˜åˆ°å…¨åŸŸè®Šæ•¸\n",
    "globals()['test_results_30'] = test_results\n",
    "globals()['test_dataframe_30'] = test_df\n",
    "\n",
    "print(f\"\\nğŸ¯ 30ç­†çœŸå¯¦è³‡æ–™æ¸¬è©¦å®Œæˆï¼\")\n",
    "print(f\"ğŸ“‹ è®Šæ•¸: test_results_30, test_dataframe_30\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
