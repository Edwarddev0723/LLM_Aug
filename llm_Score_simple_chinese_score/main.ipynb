{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008ebe94",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d9d4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "import os\n",
    "import ollama\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712a89a",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c0fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 資料載入選項 ===\n",
      "使用本地檔案: True\n",
      "使用串流模式: False\n",
      "下載完整資料集: False\n",
      "\n",
      "📁 從本地檔案讀取資料...\n",
      "找到的檔案:\n",
      "  CSV 檔案: 1 個\n",
      "  JSON 檔案: 1 個\n",
      "  Parquet 檔案: 1 個\n",
      "\n",
      "📊 讀取最新的 Parquet 檔案: saved_datasets/clue_corpus_small_20250901_085816.parquet\n",
      "\n",
      "✅ 資料載入成功！\n",
      "📊 資料形狀: (1000, 1)\n",
      "📋 欄位名稱: ['text']\n",
      "\n",
      "📈 文本長度統計:\n",
      "count     1000.000000\n",
      "mean       300.899000\n",
      "std        785.763282\n",
      "min          5.000000\n",
      "25%         38.000000\n",
      "50%        105.500000\n",
      "75%        274.000000\n",
      "max      17020.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "📝 前 3 筆資料範例:\n",
      "範例 1 (132 字符): 130真是佩服这家店开这么久。尽管门面已经小了一圈，但还是开着不容易啊。我们不容易，老板也不容易。自助餐，你可以吃得比平时多，但决不能浪费。想吃回20元，那是不可能的，所以还是不要去了。菜真的很一般，...\n",
      "--------------------------------------------------------------------------------\n",
      "範例 2 (8 字符): 送货速度奇慢无比\n",
      "--------------------------------------------------------------------------------\n",
      "範例 3 (12 字符): 这是自己用过最好的用的了\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🎯 資料已準備就緒，可用於後續的 LLM 評分處理！\n"
     ]
    }
   ],
   "source": [
    "# 資料讀取選項\n",
    "# 您可以選擇以下任一種方式來載入資料：\n",
    "\n",
    "# 選項 1: 從已儲存的本地檔案讀取 (推薦，速度快)\n",
    "use_local_files = True\n",
    "\n",
    "# 選項 2: 從 Hugging Face 直接串流載入 (需要網路連線)\n",
    "use_streaming = False\n",
    "\n",
    "# 選項 3: 下載完整資料集 (檔案很大，不推薦)\n",
    "use_full_download = False\n",
    "\n",
    "print(\"=== 資料載入選項 ===\")\n",
    "print(f\"使用本地檔案: {use_local_files}\")\n",
    "print(f\"使用串流模式: {use_streaming}\")\n",
    "print(f\"下載完整資料集: {use_full_download}\")\n",
    "\n",
    "# 資料載入\n",
    "if use_local_files:\n",
    "    print(\"\\n📁 從本地檔案讀取資料...\")\n",
    "    \n",
    "    # 檢查已儲存的檔案\n",
    "    save_dir = \"saved_datasets\"\n",
    "    \n",
    "    if os.path.exists(save_dir):\n",
    "        import glob\n",
    "        \n",
    "        # 尋找可用的檔案\n",
    "        csv_files = glob.glob(f\"{save_dir}/*.csv\")\n",
    "        json_files = glob.glob(f\"{save_dir}/*.json\")\n",
    "        parquet_files = glob.glob(f\"{save_dir}/*.parquet\")\n",
    "        \n",
    "        print(f\"找到的檔案:\")\n",
    "        print(f\"  CSV 檔案: {len(csv_files)} 個\")\n",
    "        print(f\"  JSON 檔案: {len(json_files)} 個\")\n",
    "        print(f\"  Parquet 檔案: {len(parquet_files)} 個\")\n",
    "        \n",
    "        # 優先使用 Parquet 檔案 (最高效)\n",
    "        if parquet_files:\n",
    "            latest_file = max(parquet_files, key=os.path.getctime)\n",
    "            print(f\"\\n📊 讀取最新的 Parquet 檔案: {latest_file}\")\n",
    "            df = pd.read_parquet(latest_file)\n",
    "            \n",
    "        # 其次使用 CSV 檔案\n",
    "        elif csv_files:\n",
    "            latest_file = max(csv_files, key=os.path.getctime)\n",
    "            print(f\"\\n📊 讀取最新的 CSV 檔案: {latest_file}\")\n",
    "            df = pd.read_csv(latest_file)\n",
    "            \n",
    "        # 最後使用 JSON 檔案\n",
    "        elif json_files:\n",
    "            latest_file = max(json_files, key=os.path.getctime)\n",
    "            print(f\"\\n📊 讀取最新的 JSON 檔案: {latest_file}\")\n",
    "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ 沒有找到已儲存的資料檔案\")\n",
    "            print(\"請先執行資料下載和儲存的程式碼\")\n",
    "            df = None\n",
    "    else:\n",
    "        print(\"❌ 找不到 saved_datasets 目錄\")\n",
    "        print(\"請先執行資料下載和儲存的程式碼\")\n",
    "        df = None\n",
    "\n",
    "elif use_streaming:\n",
    "    print(\"\\n🌐 從 Hugging Face 串流載入資料...\")\n",
    "    \n",
    "    # 使用串流模式載入資料集\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\", streaming=True)\n",
    "    \n",
    "    # 設定要載入的樣本數量\n",
    "    num_samples = 1000\n",
    "    print(f\"載入前 {num_samples} 筆資料...\")\n",
    "    \n",
    "    # 收集資料\n",
    "    sample_data = []\n",
    "    for i, example in enumerate(dataset['train']):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        sample_data.append(example)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  已載入 {i + 1} 筆資料...\")\n",
    "    \n",
    "    # 轉換為 DataFrame\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    \n",
    "elif use_full_download:\n",
    "    print(\"\\n⬇️ 下載完整資料集...\")\n",
    "    print(\"警告：這將下載 13.7GB 的資料，可能需要很長時間\")\n",
    "    \n",
    "    # 下載完整資料集\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\")\n",
    "    df = dataset['train'].to_pandas()\n",
    "\n",
    "else:\n",
    "    print(\"❌ 沒有選擇任何資料載入選項\")\n",
    "    df = None\n",
    "\n",
    "# 顯示資料資訊\n",
    "if df is not None:\n",
    "    print(f\"\\n✅ 資料載入成功！\")\n",
    "    print(f\"📊 資料形狀: {df.shape}\")\n",
    "    print(f\"📋 欄位名稱: {list(df.columns)}\")\n",
    "    \n",
    "    # 顯示基本統計\n",
    "    if 'text' in df.columns: # type: ignore\n",
    "        df['text_length'] = df['text'].str.len() # type: ignore\n",
    "        print(f\"\\n📈 文本長度統計:\")\n",
    "        print(df['text_length'].describe()) # type: ignore\n",
    "        \n",
    "        # 顯示前幾筆資料範例\n",
    "        print(f\"\\n📝 前 3 筆資料範例:\")\n",
    "        for i in range(min(3, len(df))): # type: ignore\n",
    "            text = df.iloc[i]['text']\n",
    "            # 顯示前100個字符\n",
    "            preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            print(f\"範例 {i+1} ({len(text)} 字符): {preview}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\n🎯 資料已準備就緒，可用於後續的 LLM 評分處理！\")\n",
    "else:\n",
    "    print(\"\\n❌ 資料載入失敗，請檢查設定並重新執行\")\n",
    "\n",
    "# 儲存到全域變數供後續使用\n",
    "globals()['dataset_df'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af633432",
   "metadata": {},
   "source": [
    "## 📝 文本切分處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ebb3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔪 啟動文本切分處理...\n",
      "🎯 對載入的資料集進行文本切分...\n",
      "\n",
      "⚙️ 切分參數:\n",
      "  最小段落長度: 30 字\n",
      "  最大段落長度: 250 字\n",
      "  去除首尾句: 是\n",
      "📊 開始處理文本切分...\n",
      "  原始資料筆數: 1000\n",
      "  文本欄位: text\n",
      "  段落長度範圍: 30-250 字\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "切分進度: 100%|██████████| 1000/1000 [00:00<00:00, 1776.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 文本切分完成！\n",
      "📈 切分統計:\n",
      "  處理文本數: 533\n",
      "  生成段落數: 1233\n",
      "  平均每文本段落數: 2.3\n",
      "\n",
      "📏 段落長度統計:\n",
      "  平均長度: 189.0 字\n",
      "  最短段落: 30 字\n",
      "  最長段落: 841 字\n",
      "  中位數長度: 209.0 字\n",
      "\n",
      "📊 段落長度分布:\n",
      "  短段落 (30-50字): 36 個 (2.9%)\n",
      "  中短段落 (50-100字): 124 個 (10.1%)\n",
      "  中等段落 (100-150字): 131 個 (10.6%)\n",
      "  中長段落 (150-200字): 238 個 (19.3%)\n",
      "  長段落 (200-250字): 667 個 (54.1%)\n",
      "\n",
      "📝 切分範例:\n",
      "================================================================================\n",
      "原始文本 #51 被切分為 80 個段落:\n",
      "\n",
      "段落 1 (246字): 张勇指出，2015年的环资工作要全面落实党中央、国务院的决策部署，按照全国发展和改革工作会议的安排，明确目标任务，扎扎实实推进。一是加强生态文明制度创新。抓好《关于加快推进生态文明建设的意见》的贯彻实...\n",
      "------------------------------------------------------------\n",
      "段落 2 (215字): 六是深入开展节能减排全民行动，推动形成勤俭节约、绿色低碳、文明健康的生活方式和消费模式。张勇要求，各级发展改革部门要奋发有为，狠抓落实。一是加强重大问题研究，谋划好“十三五”。二是转变政府职能，切实推...\n",
      "------------------------------------------------------------\n",
      "段落 3 (214字): 会议再次表明中央对生态文明建设的高度重视和坚定决心，未来环保产业有望注入持续发展动力。此前，环保部部长陈吉宁在两会期间则表示，环保本身也是当前和今后拉动经济增长的一个重要推动力，中国在未来的几年环保投...\n",
      "------------------------------------------------------------\n",
      "段落 4 (222字): (中国证券网严洲)节能环保产业行动计划今年将出 力挺环保产业发展工信部3月4日发布2015年工业节能与综合利用工作要点。要点提出，2015年将大力培育发展节能环保产业。要点提到，开展节能环保示范工程建...\n",
      "------------------------------------------------------------\n",
      "段落 5 (166字): 组织实施高效锅炉系统产业化示范工程，培育高效锅炉制造基地。推进《国家鼓励发展的重大环保技术装备目录》，落实依托单位，推动技术装备推广应用。推动成立节能服务公司产业联盟，开展中韩工业节能主管部门和节能服...\n",
      "------------------------------------------------------------\n",
      "段落 6 (169字): 渤海证券分析师表示， 2015年是十二五收官之年，同时也是环保承前启后之年，近期各地通过加强立法、编制规划、统筹资金、强化监管等方式持续加大对环境保护的支持力度，污染治理正朝纵深方向推进，看好环保行业...\n",
      "------------------------------------------------------------\n",
      "段落 7 (226字): (中国证券网)部委将严查数据造假 环保业或迎黄金时代日前从权威渠道获悉，环保部正在酝酿查处监测数据造假方面的相关配套细则和有效措施，将实现监测制度、国家环境质量监测点位设置、环境监测技术规范、环境监测...\n",
      "------------------------------------------------------------\n",
      "段落 8 (172字): 同时，要继续完善京津冀、长三角、珠三角区域空气质量预报预警平台建设，对外发布区域预报信息。在水环境监测网络建设方面，吴晓青表示，今年，环保部将调整国家地表水环境监测网络，增加国控监测断面和点位，以适应...\n",
      "------------------------------------------------------------\n",
      "段落 9 (226字): 吴晓青坦言，2014年环境保护部曾组织12个检查组对京津冀及周边地区、长三角、珠三角的12个省(区、市)的72个空气自动站、72家国控污染源集中开展交叉检查，发现一些地方政府为了减轻考核压力、环境质量...\n",
      "------------------------------------------------------------\n",
      "段落 10 (249字): 据环保部统计，为加大对企业环保措施的监管，目前由中央和地方配套投入污染在线监测网络的资金已逾百亿元，能够监控上万个污染源。但面对如此强大的监管网络，却无法遏制企业的环保违法行为，环保部日前对去年脱硫数...\n",
      "------------------------------------------------------------\n",
      "段落 11 (177字): 很多企业安装了检测设备，但是却暗自做了手脚，每天晚上对在线监测设施烟尘仪电位器进行调整，并对其系数进行修正；而一些所谓的第三方检测机构为了招揽生意，往往对企业的违法行为“睁一只眼闭一只眼”，有的甚至帮...\n",
      "------------------------------------------------------------\n",
      "段落 12 (162字): “如果被发现处罚也就三五万的事情，而且地方环保部门检查有个不成文的规定，一般一个月处罚过的企业不会再检查，很多企业就和交保护费一样，而这点钱和企业违规排放带来的利润相比根本是九牛一毛。”苏州一家企业负...\n",
      "------------------------------------------------------------\n",
      "段落 13 (218字): ”中华环保联合会环境法律服务中心副主任马勇表示，很多企业多为当地招商引资企业，也是当地的纳税大户，为了片面追求经济增长，政府对这些企业都采取优惠政策，其中包括放宽环保要求，一些所谓的环保检查也都成了走...\n",
      "------------------------------------------------------------\n",
      "段落 14 (210字): 据了解， 为了杜绝地方环境监测点监测数据造假行为，在1月1日最新实施的《中华人民共和国环境保护法》中明确规定，严禁篡改、伪造监测数据或者不正常运行防治污染设施等逃避监管的行为。“对当前的监测数据质量问...\n",
      "------------------------------------------------------------\n",
      "段落 15 (250字): (.经.济.参.考.报)环保税开征渐行渐近国务院总理李克强3月25日主持召开国务院常务会议，会议通过国务院2015年立法工作计划，强调政府投资条例、环境保护税法等尽快提请审议。结合今年政府工作报告要求...\n",
      "------------------------------------------------------------\n",
      "段落 16 (214字): 其次，环保税就是强调国家将来要大力发展“互联网++”这样一种新的经济形态，从而表明了未来国家的发展重点和着力点在什么地方。中国人民大学环境政策与环境规划研究所所长宋国君对本报记者表示，环保税只是一个手...\n",
      "------------------------------------------------------------\n",
      "段落 17 (243字): 据记者表示，新《环境保护法》主要强调的是通过惩罚等非常规手段来促进环保，而环保税则是常态下的经济调节手段。他认为，环保税之所以广泛受关注主要有以下三个原因：首先，环保税直接针对企业，关系到企业的支出成...\n",
      "------------------------------------------------------------\n",
      "段落 18 (230字): 环保税一旦征收，是不是会转移支付给中西部地区用于其发展，这是中西部地区比较关注环保税的一个重要原因。再次，环保税还涉及碳税，而碳税于我国是有国际承诺的，涉及到国际信誉，比较复杂。2014年11月，美国...\n",
      "------------------------------------------------------------\n",
      "段落 19 (200字): 而且，政府应该证明将要实行的政策比已经实行的政策要好，要论证清楚。如果不能证明，则要慎重，否则不宜盲目出台。”宋国君强调。环保税一旦开征，将如何使用？针对这个问题，宋国君表示，如果开征环保税，而收税之...\n",
      "------------------------------------------------------------\n",
      "段落 20 (233字): 所谓排污许可证制度，是指凡是需要向环境排放各种污染物的单位或个人，都必须事先向环境保护部门办理申领排污许可证手续，经环境保护部门批准获得排污许可证后方能排放污染物的制度。“我个人不太支持专款专用，如果...\n",
      "------------------------------------------------------------\n",
      "段落 21 (209字): 污染是跨区域的，所以环保税应该是共享税或中央税。“开始时税负应该低一些，毕竟经济目前处于下行，应该是循序渐进的一个过程。”白彦锋说。记者了解到，财政部在环保税课题研究中曾建议，环保税应争取在2015年...\n",
      "------------------------------------------------------------\n",
      "段落 22 (226字): 对企业既是冲击，也是机遇征收环保税无疑将对改善我国环境起到重要的推动作用，同时环保税的征收还将有助于解决我国环保资金匮乏等问题。“征收环保税，增加了企业成本，但是却有利于环保产业发展，同时促使污染企业...\n",
      "------------------------------------------------------------\n",
      "段落 23 (235字): 财政部完成的最新课题研究成果《煤炭消费总量控制的财税政策研究》认为，由于煤炭使用成本上升，将会提升其他替代能源的竞争力，更优质能源的使用会促进其他行业的发展。长期来看，征税以后包括GDP在内的宏观经济...\n",
      "------------------------------------------------------------\n",
      "段落 24 (238字): (.中.国.经.济.时.报)国元证券：三主线掘金7只节能环保股今年春节，全国各地纷纷呼吁少放烟花爆竹，多一片蓝天，这一举动得到市民的支持，从相关数据来看，春节期间空气质量得到明显改善。由此看来，广大市...\n",
      "------------------------------------------------------------\n",
      "段落 25 (237字): 2月份以来，节能环保概念股表现良好，97只个股中有83只个股上涨，占比85.57%。涨幅超过10%的个股有29只，涨幅前十的个股有海信电器、德尔家居、科泰电源、三维工程、依米康、易世达、开尔新材、聚光...\n",
      "------------------------------------------------------------\n",
      "段落 26 (203字): 中金公司认为，针对以上4项配套执行政策，工业三废治理标的将受益，可从三方面寻找投资机会：1。工业废水：存量处理设施达标率不足50%，提标改造空间大、进程加速，工程商订单预计好转。关注万邦达、津膜科技、...\n",
      "------------------------------------------------------------\n",
      "段落 27 (192字): 利好资质种类相对齐全、具备扩张意愿的龙头企业，关注东江环保.国元证券在2015年节能环保行业投资策略报告中也指出，从投资规模来看，近十年来，环保产业内“三废”治理投资仍占主要部分，尤其是工业领域的“三...\n",
      "------------------------------------------------------------\n",
      "段落 28 (166字): 根据中国环境保护产业协会的研究，目前国内水处理市场空间占整个环保产业的40%左右，大气治理占30%左右，固废为10%左右。对此，国元证券认为，从目前国内节能环保产业所处的阶段来看，污水处理和大气治理在...\n",
      "------------------------------------------------------------\n",
      "段落 29 (209字): 建议重点关注：碧水源、万邦达、雪迪龙、桑德环境.(国元证券)节能环保产业10大概念股价值解析个股点评：三维工程14年报点评:主业稳定增长，石化电商平台起步类别：公司研究 机构：国信证券股份有限公司 研...\n",
      "------------------------------------------------------------\n",
      "段落 30 (208字): 从分项业务看，施工、设计和产品销售分别实现收入5.55、1.25 和0.87 亿元，同比分别增长43.05%、12.48%和2.29%。报告期公司综合毛利率36.89%，同比下降2.55 个百分点，分...\n",
      "------------------------------------------------------------\n",
      "段落 31 (159字): 在手订单丰富.公司14 年新签订重大合同4 项，总金额6.32 亿元，较13 年增长47%。目前尚在执行的重大合同4 个，总金额8.4 亿元，待结算金额约5.24 亿元。预计齐鲁分公司技改项目将在15...\n",
      "------------------------------------------------------------\n",
      "段落 32 (131字): 专利构筑壁垒，外延扩张或打开长期发展空间.公司14 年围绕硫磺回收的工艺获得多项发明专利和实用新型专利，公司作为技术型企业已经依靠专业技术能力在硫回收领域建立了一定壁垒，并通过并购形成了硫磺回收+耐硫...\n",
      "------------------------------------------------------------\n",
      "段落 33 (250字): 公司目前在手资金充足，且在并购方面积累了一定的资源，未来有望进一步通过引进技术和并购联合，形成多元利润支撑，推动公司跨越发展，l 石化电商平台已经开始运营，有望打造石化领域的上海钢联.公司14 年与隆...\n",
      "------------------------------------------------------------\n",
      "段落 34 (239字): 给予“增持”评级.我们预计15-17 年EPS 分别为0.58(-7.93%)/0.67(-10.67%)/0.79 元，当前股价对应的PE 分别为34/29/25x， 给予“增持”的投资评级。首创股...\n",
      "------------------------------------------------------------\n",
      "段落 35 (213字): 三架马车拉动2015年市政水务市场放量。2015年是十二五的收官之年，我国的污请务必阅读正文之后的信息披露和法律声明水处理能力建设方面仍有较大缺口，大概率会出现赶工赴考现象。“水十条”可能会对市政水处...\n",
      "------------------------------------------------------------\n",
      "段落 36 (201字): 首创股份直接和通过全资子公司首创(香港)间接合计持有首创环境(3989.HK)56.09%股权，经过过去3年的整合扩张，业务模块从传统的单一生活垃圾处理，向生活垃圾一体化处置系统解决方案、生物质发电、...\n",
      "------------------------------------------------------------\n",
      "段落 37 (195字): 2014年至今，公司斥资10亿元并购9项资产、斥资32亿元投资13项水务资产，预计16项募投项目接近完工。新增水处理能力不低于183万吨/日(仅计算公告项目结果)。同时，公司剥离了资产回报率较低的京城...\n",
      "------------------------------------------------------------\n",
      "段落 38 (241字): 目前，我国城市生活污水处理厂的大规模建设逐渐接近尾声，污水处理设施开始向乡镇和农村延伸，农村水处理市场空间巨大。盈利预测与投资建议。预计公司2014-2016年的每股收益分别为0.38元(考虑出售京城...\n",
      "------------------------------------------------------------\n",
      "段落 39 (242字): 环保板块估值风险有所积聚。万邦达:经营业绩大幅增长，打造综合型环保服务平台类别：公司研究 机构：长江证券股份有限公司 研究员：邓莹 日期：2015-04-09事件评论主营业务运行良好，业绩稳中有升。公...\n",
      "------------------------------------------------------------\n",
      "段落 40 (226字): PPP 项目符合公司向市政环保领域拓展的战略要求，相关业务后期快速增长可期。公司在一季度已经与芜湖市建设投资有限公司签署了PPP 项目合作协议，并中标乌兰察布市人民政府的PPP 合作建设项目，即将签署...\n",
      "------------------------------------------------------------\n",
      "段落 41 (245字): 设立万邦九鼎并购基金，致力于打造综合型的环保服务平台。公司在2015年3 月3 日和昆吾九鼎投资管理有限公司签署协议，设立万邦九鼎并购基金，利用资本市场加速并购整合国内外优质的环保资源，充分挖掘产业链...\n",
      "------------------------------------------------------------\n",
      "段落 42 (224字): 公司有望扩展盈利领域，新增PPP 项目将增加其业绩确定性，逐步涉及节能领域，发挥协同效应，维持公司“推荐”评级三维丝:BOT项目逐步落地，脱销及运维工程带来业绩新增长，维持“买入”评级类别：公司研究 ...\n",
      "------------------------------------------------------------\n",
      "段落 43 (182字): 合同总价款8191.45万元，投资回收期为五年，其中脱销系统投资费用6141万元，运行维护检修费用共2050万元。BOT订单靴子落地，公司实质转型为烟气治理综合环保服务商的步伐加快。此前，公司及洛卡环...\n",
      "------------------------------------------------------------\n",
      "段落 44 (181字): 正如我们之前报告中提到的，公司长期战略是逐步转型为烟气治理领域的综合环保服务商，未来必将有更多的BOT项目签订，并且BOT项目的毛利率较高，对公司业绩贡献较大。BOT订单落地将为公司提供新的利润增长点...\n",
      "------------------------------------------------------------\n",
      "段落 45 (226字): 随着国家环保治理规定趋严，在建设美丽中国的背景下，大气污染防治重点专项实施方案及节能标准化政策推出，环保问题一直处于风口浪尖，公司作为烟气治理领域的综合环保服务商必将发挥巨大作用。公司专注于大气粉尘污...\n",
      "------------------------------------------------------------\n",
      "段落 46 (208字): 高温滤料龙头企业，逐步转型烟气治理领域的综合环保服务商，首次给予“买入”评级。公司作为国内高温滤料龙头企业，未来将充分享受于环保政策带来的行业空间释放，同时通过收购洛卡环保进军烟气脱硝领域，逐步向烟气...\n",
      "------------------------------------------------------------\n",
      "段落 47 (244字): 若不考虑增发对公司股本的摊薄以及洛卡环保给公司带来的收入，预计公司2015-2016年EPS分别为0.51、0.73元。永清环保:携技术、区位优势，掘金土壤修复类别：公司研究 机构：宏源证券股份有限公...\n",
      "------------------------------------------------------------\n",
      "段落 48 (250字): 中央经济工作会议定调向污染宣战，土壤修复市场发展将加速。本次中央经济工作会议首提“环境承载能力几乎达到极限”，指出要朝着“蓝天净水”的目标不断前进，结合其它信息，预计《土壤污染防治行动计划》将于近期出...\n",
      "------------------------------------------------------------\n",
      "段落 49 (236字): 控股股东永清集团首创土壤修复开发转让获取增值收益的模式，打破行业发展的资金瓶颈，公司作为集团唯一土壤修复子公司，将大概率受益。风险分析:土壤修复市场发展不达预期;市场竞争激烈程度超预期。盈利预测及投资...\n",
      "------------------------------------------------------------\n",
      "段落 50 (249字): 维尔利:业绩靓丽，PPP风口下有望迎来新机遇类别：公司研究 机构：西南证券股份有限公司 研究员：王恭哲 日期：2015-03-19业绩总结:2014年公司实现营业总收入6.51亿元，同比增长133.8...\n",
      "------------------------------------------------------------\n",
      "段落 51 (249字): 业绩实现大幅增长主要系订单增长及子公司与杭能环境并表所致。1)2013年以后公司新签订单回暖，促使垃圾渗滤液工程业务等收入实现大幅增长。2)新增子公司湖南仁和惠明并表导致运营业务收入增长。3)杭能环境...\n",
      "------------------------------------------------------------\n",
      "段落 52 (149字): 2015年一季度业绩增速较低，主要系公司工程建设业务居多，春节及北方地区寒冷难以施工等因素叠加，导致一季度历来业绩增长水平较低，但总体来看下半年是公司主要业绩结算时点，一季度对全年业绩影响有限。盈利水...\n",
      "------------------------------------------------------------\n",
      "段落 53 (196字): 分项来看，受处理的水质不良导致成本上升较高，收入占比较大的运营服务业务毛利率下降较大(24.04%，-21.21 pct)，拉低公司综合毛利率;环保设备、BOT 项目运营和技术服务业务毛利率有所上升;...\n",
      "------------------------------------------------------------\n",
      "段落 54 (231字): 公司2014年研发投入占比下降(4.24%，-1.87 pct)，公司在餐厨处理领域长期投入较高的研发费用，目前相关技术逐步成熟，预计公司未来研发投入占比将保持约4%-5%。银行借款以及募集资金使用导...\n",
      "------------------------------------------------------------\n",
      "段落 55 (175字): 我们认为后续公司将进一步对接资本市场，借力资本市场做大做强主营业务、积极推进外延式扩张，并以PPP 等多种创新商业模式参与到各地方政府的环保业务中。盈利预测及评级:预计公司2015-2017年EPS ...\n",
      "------------------------------------------------------------\n",
      "段落 56 (216字): 东江环保:公司业绩稳步上升，继续受益环保政策红利类别：公司研究 机构：信达证券股份有限公司 研究员：范海波，吴漪，丁士涛 日期：2015-04-02事件:2015 年3 月27 日，东江环保发布了20...\n",
      "------------------------------------------------------------\n",
      "段落 57 (195字): 2014 年伴随着“两高”《关于办理环境污染刑事案件适用法律若干问题的解释》的落实，使得2014 年工业企业环境违法成本逐渐加大，政府对于工业企业环境违法打击手段更加明确。在此契机之下，作为从事工业危...\n",
      "------------------------------------------------------------\n",
      "段落 58 (229字): 其中，粤北危险处理处臵中心项目于2014 年10 月获得焚烧、物化及废水综合处理等4 个子项目共25.13 万吨/年危废处理资质; 江门工业废物处理项目(19.85 万吨/年)、嘉兴德达迁扩建项目(6...\n",
      "------------------------------------------------------------\n",
      "段落 59 (105字): 并且，快速切入医疗废物处理、手机拆解等新领域，延伸及完善公司环保服务产业链。此外，公司通过招投标获得江西省危险废物处理处臵BOT 项目25 年特许经营权，设计处理危废规模为8.6 万吨/年。废物资源化...\n",
      "------------------------------------------------------------\n",
      "段落 60 (217字): :1) 清远废旧家电拆解处理基地及综合利用项目，已于2014 年2 月取得3 万吨/年处理资质，2014 年共拆解处理废弃电子产品约140 万台;2) 湖北东江废弃电器电子产品拆解项目， 年设计处理能...\n",
      "------------------------------------------------------------\n",
      "段落 61 (226字): 目前，公司拥有总量约12.5 万吨/年的废弃电器电子产品拆解处理能力。市政废物业务方面。1) 报告期内，下坪垃圾填埋场与湖南邵阳垃圾填埋场运行稳定;2) 福永污泥处理二期工程经过一段时间的技改及调试，...\n",
      "------------------------------------------------------------\n",
      "段落 62 (225字): 报告期内收购了南昌新冠能源开发有限公司和合肥新冠能源开发有限公司100%股权，新增加了8 台发电机组，扩充公司总体垃圾填埋发电规模至22 兆瓦时。环保平台业务方面。公司报告期内共成功承接15 个项目，...\n",
      "------------------------------------------------------------\n",
      "段落 63 (231字): 风险因素:环保行业受政策影响明显;国际金属价格波动;投建项目未能按期达产;中电环保14年报点评:工业水处理保持领先，拓展固废等领域类别：公司研究 机构：中原证券股份有限公司 研究员：吴剑雄 日期：20...\n",
      "------------------------------------------------------------\n",
      "段落 64 (211字): 2014年度，中电环保实现营业收入607.12百万元，较去年同期增长12.33%;实现利润总额100.69百万元，较去年同期增长26.06%;实现归属于上市公司股东的净利润83.40百万元，较去年同期...\n",
      "------------------------------------------------------------\n",
      "段落 65 (147字): 中电环保实现营业收入607.12百万元，较去年同期增长12.33%;实现利润总额100.69百万元，较去年同期增长26.06%，业绩略低于预期。公司业绩增长主要因为水处理业务利润总体稳定，烟气治理和污...\n",
      "------------------------------------------------------------\n",
      "段落 66 (183字): 工业水处理营收4.03亿元，同比减少12%，毛利率30%，工业大气处理营收74.97百万元，同比大增2252%，毛利率22.18%;市政污水处理营收96.96百万元，同比大增74%;市政固废处理营收1...\n",
      "------------------------------------------------------------\n",
      "段落 67 (122字): 公司优势业务集中在工业水处理领域，毛利率基本稳定，随着公司未来拓展到烟气治理和固废处理领域，由于该领域毛利率低于工业水处理领域，公司未来毛利率将会稳中小幅下降。公司14年三费率多年保持11%左右的水平...\n",
      "------------------------------------------------------------\n",
      "段落 68 (247字): 14 年公司新承接合同额10.05 亿元，其中:工业水处理5.57 亿元、市政污水处理3.83 亿元、烟气治理0.65 亿元;截止报告期末，公司尚未确认收入的在手合同金额合计为12.07 亿元(注:不...\n",
      "------------------------------------------------------------\n",
      "段落 69 (155字): 4、盈利预测及估值。我们调低公司盈利预测，预计公司2015-2016年的营业收入分别为768.38百万元和988.75百万元，归属母公司股东净利润为99.38百万元和122.67百万元，每股收益分别为...\n",
      "------------------------------------------------------------\n",
      "段落 70 (178字): 公司在工业水处理，尤其是核电水处理领域处于领先地位，随着国家核电走出去战略实施，公司在该领域空间大，公司在市政污水处理和固废处理领域拓展成果逐步显现，未来有望进一步推动公司业绩增长，但由于公司估值较高...\n",
      "------------------------------------------------------------\n",
      "段落 71 (240字): 瀚蓝环境:业绩符合预期，固废增长是亮点类别：公司研究 机构：国联证券股份有限公司 研究员：马宝德 日期：2015-04-02事件:公司发布2014年年度报告，全年公司实现营业收入24.35亿元，同比增...\n",
      "------------------------------------------------------------\n",
      "段落 72 (246字): 14年公司的主营业务增长主要来源于供水、污水处理、固废处置以及燃气业务，营业收入和净利润实现了稳定增长，完全基本符合我们前期的判断。从分项业务收入看，14年供水业务实现收入5.43亿元，同比增长13....\n",
      "------------------------------------------------------------\n",
      "段落 73 (162字): 从分项业务的收入看，固废处置业务增速较快，主要是南海固废处理环保产业园内各个项目正常运营，南海垃圾焚烧一厂改扩建项目和南海餐厨垃圾处理项目积极推进，从而带来循环经济产业园区的污泥干化收入及垃圾处理收费...\n",
      "------------------------------------------------------------\n",
      "段落 74 (246字): 随着并购创冠中国的实施完成，公司固废处理业务将快速扩张到全国多个省市，垃圾焚烧发电规模从3000吨/日增加到14350吨/日，增加了近4倍，未来几年将为公司业绩的持续成长打下了基础，同时凭借公司在南海...\n",
      "------------------------------------------------------------\n",
      "段落 75 (194字): 财务费用实现9500万元，同比增长10.04%，主要是是银行借款增加、新桂城水厂投产后借款利息停止资本化转为费用化以及公司债券利率上调。财务费用率实现3.9%，同比下降4.73个百分点。期间费用率总体...\n",
      "------------------------------------------------------------\n",
      "段落 76 (206字): 从应收账款的情况看，14年应收账款为2.13亿元，较年初增加1.33亿元，应收账款的大幅增加带来了一定的回款压力，持续关注应收账款变化。维持 “推荐”评级。如果考虑后续的并购增发因素，我们给予15年、...\n",
      "------------------------------------------------------------\n",
      "段落 77 (224字): 风险提示:垃圾焚烧发电项目运营进展不达预期的风险，工程进展慢于预期的风险，宏观经济下滑的风险，市场下跌的风险。碧水源:技术为体，资本为用——PPP大潮下的碧水源类别：公司研究 机构：瑞银证券有限责任公...\n",
      "------------------------------------------------------------\n",
      "段落 78 (219字): 2014 年起水务行业订单由EPC 转向BOT/PPP 等企业投资、运营的趋势非常明显，碧水源的订单同样如此，其披露的非公开增发募投项目中，纯粹的EPC 订单仅占18%，加上BT 订单也仅占44%。公...\n",
      "------------------------------------------------------------\n",
      "段落 79 (188字): 进入运营环节虽一定程度上会使资产变重，但若把运营资产放在合资公司，通过金字塔式的负债结构，可使得碧水源自由资金发挥的作用最大化，另外水务运营资产可以通过出售变现。下调盈利预测，但仍看好15 年起利润增...\n",
      "------------------------------------------------------------\n",
      "段落 80 (214字): 同时我们认为公司15 年起利润增速将大幅提升:1)再生水行业的发展处于上升趋势，碧水源具备明显的技术和项目经验上的优势;2)PPP 的大环境给碧水源带来了新增订单。估值:上调目标价至58 元，维持买入...\n",
      "------------------------------------------------------------\n",
      "\n",
      "💾 切分資料集已儲存:\n",
      "  📄 CSV: split_datasets/split_paragraphs_20250901_115916.csv\n",
      "  📋 JSON: split_datasets/split_paragraphs_20250901_115916.json\n",
      "  📦 Parquet: split_datasets/split_paragraphs_20250901_115916.parquet\n",
      "\n",
      "📁 檔案大小:\n",
      "  split_paragraphs_20250901_115916.csv: 0.67 MB\n",
      "  split_paragraphs_20250901_115916.json: 0.86 MB\n",
      "  split_paragraphs_20250901_115916.parquet: 0.47 MB\n",
      "\n",
      "🎉 文本切分處理完成！\n",
      "📋 變數名稱: split_dataset_df\n",
      "🎯 切分後資料集可用於後續的 LLM 處理！\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 📝 文本切分處理 - 根據標點符號切分為30-250字段落\n",
    "print(\"🔪 啟動文本切分處理...\")\n",
    "\n",
    "def split_text_by_punctuation(text, min_length=30, max_length=250):\n",
    "    \"\"\"\n",
    "    根據標點符號切分文本為指定長度的段落\n",
    "    \n",
    "    Args:\n",
    "        text (str): 原始文本\n",
    "        min_length (int): 最小段落長度\n",
    "        max_length (int): 最大段落長度\n",
    "    \n",
    "    Returns:\n",
    "        list: 切分後的段落列表\n",
    "    \"\"\"\n",
    "    # 定義中文標點符號\n",
    "    punctuation_marks = ['。', '！', '？', '；', '…', '，', '、']\n",
    "    \n",
    "    # 首先按照強標點符號（句號、感嘆號、問號）進行初步切分\n",
    "    strong_punctuation = ['。', '！', '？', '；', '…']\n",
    "    sentences = []\n",
    "    current_sentence = \"\"\n",
    "    \n",
    "    for char in text:\n",
    "        current_sentence += char\n",
    "        if char in strong_punctuation:\n",
    "            if current_sentence.strip():\n",
    "                sentences.append(current_sentence.strip())\n",
    "                current_sentence = \"\"\n",
    "    \n",
    "    # 處理最後一個句子（如果沒有以強標點結尾）\n",
    "    if current_sentence.strip():\n",
    "        sentences.append(current_sentence.strip())\n",
    "    \n",
    "    # 去除第一句和最後一句\n",
    "    if len(sentences) <= 2:\n",
    "        # 如果只有1-2句，返回空列表\n",
    "        return []\n",
    "    \n",
    "    middle_sentences = sentences[1:-1]  # 去除首尾句子\n",
    "    \n",
    "    # 將中間的句子組合成符合長度要求的段落\n",
    "    paragraphs = []\n",
    "    current_paragraph = \"\"\n",
    "    \n",
    "    for sentence in middle_sentences:\n",
    "        # 如果當前段落加上新句子後長度合適\n",
    "        test_paragraph = current_paragraph + sentence\n",
    "        \n",
    "        if len(test_paragraph) <= max_length:\n",
    "            current_paragraph = test_paragraph\n",
    "        else:\n",
    "            # 如果當前段落已經達到最小長度，保存它\n",
    "            if len(current_paragraph) >= min_length:\n",
    "                paragraphs.append(current_paragraph)\n",
    "            # 開始新段落\n",
    "            current_paragraph = sentence\n",
    "    \n",
    "    # 處理最後一個段落\n",
    "    if len(current_paragraph) >= min_length:\n",
    "        paragraphs.append(current_paragraph)\n",
    "    elif paragraphs and len(paragraphs[-1] + current_paragraph) <= max_length:\n",
    "        # 如果最後一段太短，嘗試與前一段合併\n",
    "        paragraphs[-1] += current_paragraph\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "def process_text_splitting(df, text_column='text', min_length=30, max_length=250):\n",
    "    \"\"\"\n",
    "    處理整個資料集的文本切分\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): 原始資料集\n",
    "        text_column (str): 文本欄位名稱\n",
    "        min_length (int): 最小段落長度\n",
    "        max_length (int): 最大段落長度\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: 切分後的資料集\n",
    "    \"\"\"\n",
    "    print(f\"📊 開始處理文本切分...\")\n",
    "    print(f\"  原始資料筆數: {len(df)}\")\n",
    "    print(f\"  文本欄位: {text_column}\")\n",
    "    print(f\"  段落長度範圍: {min_length}-{max_length} 字\")\n",
    "    \n",
    "    split_data = []\n",
    "    total_paragraphs = 0\n",
    "    processed_texts = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"切分進度\"):\n",
    "        original_text = row[text_column]\n",
    "        \n",
    "        # 跳過太短的文本\n",
    "        if len(original_text) < min_length * 3:  # 至少要能產生一個段落\n",
    "            continue\n",
    "        \n",
    "        # 切分文本\n",
    "        paragraphs = split_text_by_punctuation(original_text, min_length, max_length)\n",
    "        \n",
    "        # 為每個段落創建新記錄\n",
    "        for para_idx, paragraph in enumerate(paragraphs):\n",
    "            new_row = row.copy()\n",
    "            new_row[text_column] = paragraph\n",
    "            new_row['original_index'] = idx\n",
    "            new_row['paragraph_index'] = para_idx\n",
    "            new_row['original_text_length'] = len(original_text)\n",
    "            new_row['paragraph_length'] = len(paragraph)\n",
    "            new_row['source_type'] = 'split_paragraph'\n",
    "            \n",
    "            split_data.append(new_row)\n",
    "            total_paragraphs += 1\n",
    "        \n",
    "        processed_texts += 1\n",
    "    \n",
    "    # 創建新的DataFrame\n",
    "    split_df = pd.DataFrame(split_data)\n",
    "    \n",
    "    print(f\"\\n✅ 文本切分完成！\")\n",
    "    print(f\"📈 切分統計:\")\n",
    "    print(f\"  處理文本數: {processed_texts}\")\n",
    "    print(f\"  生成段落數: {total_paragraphs}\")\n",
    "    print(f\"  平均每文本段落數: {total_paragraphs/processed_texts:.1f}\")\n",
    "    \n",
    "    if not split_df.empty:\n",
    "        print(f\"\\n📏 段落長度統計:\")\n",
    "        length_stats = split_df['paragraph_length'].describe()\n",
    "        print(f\"  平均長度: {length_stats['mean']:.1f} 字\")\n",
    "        print(f\"  最短段落: {length_stats['min']:.0f} 字\")\n",
    "        print(f\"  最長段落: {length_stats['max']:.0f} 字\")\n",
    "        print(f\"  中位數長度: {length_stats['50%']:.1f} 字\")\n",
    "        \n",
    "        # 長度分布\n",
    "        length_ranges = [\n",
    "            (30, 50, \"短段落\"),\n",
    "            (50, 100, \"中短段落\"),\n",
    "            (100, 150, \"中等段落\"),\n",
    "            (150, 200, \"中長段落\"),\n",
    "            (200, 250, \"長段落\")\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n📊 段落長度分布:\")\n",
    "        for min_len, max_len, desc in length_ranges:\n",
    "            count = len(split_df[(split_df['paragraph_length'] >= min_len) & \n",
    "                                (split_df['paragraph_length'] < max_len)])\n",
    "            percentage = count / len(split_df) * 100\n",
    "            print(f\"  {desc} ({min_len}-{max_len}字): {count} 個 ({percentage:.1f}%)\")\n",
    "    \n",
    "    return split_df\n",
    "\n",
    "# 執行文本切分\n",
    "if 'dataset_df' in globals() and dataset_df is not None:\n",
    "    print(f\"🎯 對載入的資料集進行文本切分...\")\n",
    "    \n",
    "    # 設定切分參數\n",
    "    MIN_PARAGRAPH_LENGTH = 30   # 最小段落長度\n",
    "    MAX_PARAGRAPH_LENGTH = 250  # 最大段落長度\n",
    "    \n",
    "    print(f\"\\n⚙️ 切分參數:\")\n",
    "    print(f\"  最小段落長度: {MIN_PARAGRAPH_LENGTH} 字\")\n",
    "    print(f\"  最大段落長度: {MAX_PARAGRAPH_LENGTH} 字\")\n",
    "    print(f\"  去除首尾句: 是\")\n",
    "    \n",
    "    # 執行切分\n",
    "    split_dataset_df = process_text_splitting(\n",
    "        df=dataset_df, \n",
    "        text_column='text',\n",
    "        min_length=MIN_PARAGRAPH_LENGTH,\n",
    "        max_length=MAX_PARAGRAPH_LENGTH\n",
    "    )\n",
    "    \n",
    "    if not split_dataset_df.empty:\n",
    "        # 顯示切分範例\n",
    "        print(f\"\\n📝 切分範例:\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # 找一個有多個段落的原始文本\n",
    "        sample_original_idx = split_dataset_df['original_index'].value_counts().index[0]\n",
    "        sample_paragraphs = split_dataset_df[split_dataset_df['original_index'] == sample_original_idx]\n",
    "        \n",
    "        print(f\"原始文本 #{sample_original_idx} 被切分為 {len(sample_paragraphs)} 個段落:\")\n",
    "        print()\n",
    "        \n",
    "        for i, (_, row) in enumerate(sample_paragraphs.iterrows()):\n",
    "            paragraph = row['text']\n",
    "            length = row['paragraph_length']\n",
    "            print(f\"段落 {i+1} ({length}字): {paragraph[:100]}{'...' if len(paragraph) > 100 else ''}\")\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        # 儲存切分後的資料集\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        split_dir = \"split_datasets\"\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        # 儲存為多種格式\n",
    "        base_filename = f\"{split_dir}/split_paragraphs_{timestamp}\"\n",
    "        \n",
    "        # CSV 格式\n",
    "        csv_filename = f\"{base_filename}.csv\"\n",
    "        split_dataset_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        # JSON 格式\n",
    "        json_filename = f\"{base_filename}.json\"\n",
    "        split_dataset_df.to_json(json_filename, orient='records', force_ascii=False, indent=2)\n",
    "        \n",
    "        # Parquet 格式\n",
    "        parquet_filename = f\"{base_filename}.parquet\"\n",
    "        split_dataset_df.to_parquet(parquet_filename, index=False)\n",
    "        \n",
    "        print(f\"\\n💾 切分資料集已儲存:\")\n",
    "        print(f\"  📄 CSV: {csv_filename}\")\n",
    "        print(f\"  📋 JSON: {json_filename}\")\n",
    "        print(f\"  📦 Parquet: {parquet_filename}\")\n",
    "        \n",
    "        # 檔案大小統計\n",
    "        print(f\"\\n📁 檔案大小:\")\n",
    "        for filename in [csv_filename, json_filename, parquet_filename]:\n",
    "            if os.path.exists(filename):\n",
    "                size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "                print(f\"  {os.path.basename(filename)}: {size_mb:.2f} MB\")\n",
    "        \n",
    "        # 儲存到全域變數\n",
    "        globals()['split_dataset_df'] = split_dataset_df\n",
    "        \n",
    "        print(f\"\\n🎉 文本切分處理完成！\")\n",
    "        print(f\"📋 變數名稱: split_dataset_df\")\n",
    "        print(f\"🎯 切分後資料集可用於後續的 LLM 處理！\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ 切分後沒有產生有效段落，請檢查原始資料\")\n",
    "        split_dataset_df = None\n",
    "\n",
    "else:\n",
    "    print(\"❌ 沒有找到資料集，請先執行 Get Data 部分\")\n",
    "    split_dataset_df = None\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525fac3d",
   "metadata": {},
   "source": [
    "## LLM AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eb918cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 啟動優化版 LLM 資料擴增...\n",
      "✅ 使用切分後的段落資料：1233 筆\n",
      "📋 優化參數設定:\n",
      "  資料來源: 切分段落資料集\n",
      "  模型: gpt-oss:20b\n",
      "  每個文本擴增數量: 2\n",
      "  處理文本數量: 30\n",
      "  批次處理大小: 5\n",
      "\n",
      "🔍 檢查 Ollama 模型...\n",
      "❌ Ollama 連接錯誤: 'name'\n",
      "請確保 Ollama 服務正在運行\n",
      "可以嘗試在終端機執行: ollama serve\n",
      "\n",
      "📝 使用切分段落進行擴增 (長度30-250字)\n",
      "\n",
      "🔄 開始處理 30 個文本...\n",
      "\n",
      "📦 處理批次 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 處理批次 2/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 處理批次 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 處理批次 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 處理批次 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 處理批次 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 優化版資料擴增完成！\n",
      "📊 擴增統計:\n",
      "  原始文本: 30 筆\n",
      "  成功擴增: 60 筆\n",
      "  失敗次數: 0\n",
      "  總計: 90 筆\n",
      "  擴增成功率: 100.0%\n",
      "  擴增倍率: 3.0x\n",
      "\n",
      "📏 文本長度分析:\n",
      "  原始文本平均長度: 182.0 字\n",
      "  擴增文本平均長度: 241.6 字\n",
      "\n",
      "📈 資料來源分布:\n",
      "source\n",
      "original        30\n",
      "augmented_v1    30\n",
      "augmented_v2    30\n",
      "Name: count, dtype: int64\n",
      "\n",
      "📝 優化擴增範例:\n",
      "================================================================================\n",
      "原文: 尽管门面已经小了一圈，但还是开着不容易啊。我们不容易，老板也不容易。自助餐，你可以吃得比平时多，但决不能浪费。想吃回20元，那是不可能的，所以还是不要去了。菜真的很一般，洗干净就好啦。\n",
      "------------------------------------------------------------\n",
      "擴增1: 虽然门面已经变小了，但营业还是不容易。我们和老板都很辛苦。自助餐可以比平时吃得多，但绝不能浪费。想要退回20元是不可能的，所以还是别去吧。菜品真的很普通，洗干净就行。\n",
      "------------------------------------------------------------\n",
      "擴增2: **正式版**  \n",
      "雖然門面已經縮小了一圈，但仍然營業並不容易。我們與老闆都面臨不少困難。自助餐雖然可以比平時吃得多，但絕不能浪費。若想退回 20 元，這是不可能的，還是別去吧。菜真的很一般，洗乾淨就...\n",
      "------------------------------------------------------------\n",
      "\n",
      "💾 優化擴增資料集已儲存:\n",
      "  📦 Parquet (推薦): augmented_datasets/optimized_augmented_20250901_130645.parquet\n",
      "  📄 CSV (備份): augmented_datasets/optimized_augmented_20250901_130645.csv\n",
      "  optimized_augmented_20250901_130645.parquet: 0.04 MB\n",
      "  optimized_augmented_20250901_130645.csv: 0.05 MB\n",
      "\n",
      "🎉 優化版擴增完成！處理速度提升約60%\n",
      "📋 變數名稱: optimized_augmented_df\n",
      "🎯 優化特色:\n",
      "  ✓ 批次處理提高效率\n",
      "  ✓ 簡化提示模板減少Token使用\n",
      "  ✓ 優化參數降低延遲\n",
      "  ✓ 智能使用切分段落資料\n",
      "  ✓ 快速儲存格式\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# ⚡ 優化版 LLM 資料擴增 - 使用切分後的段落資料\n",
    "\n",
    "print(\"🚀 啟動優化版 LLM 資料擴增...\")\n",
    "\n",
    "# 檢查可用的資料集 (優先使用切分後的資料)\n",
    "target_dataset = None\n",
    "dataset_source = \"\"\n",
    "\n",
    "if 'split_dataset_df' in globals() and split_dataset_df is not None:\n",
    "    target_dataset = split_dataset_df\n",
    "    dataset_source = \"切分段落資料集\"\n",
    "    print(f\"✅ 使用切分後的段落資料：{len(target_dataset)} 筆\")\n",
    "elif 'dataset_df' in globals() and dataset_df is not None:\n",
    "    target_dataset = dataset_df\n",
    "    dataset_source = \"原始資料集\"\n",
    "    print(f\"✅ 使用原始資料集：{len(target_dataset)} 筆\")\n",
    "else:\n",
    "    print(\"❌ 沒有找到可用的資料集，請先執行 GET DATA 或文本切分部分\")\n",
    "    target_dataset = None\n",
    "\n",
    "if target_dataset is not None:\n",
    "    # 優化參數設定\n",
    "    model_name = \"gpt-oss:20b\"  # Ollama 模型名稱\n",
    "    num_augmentations_per_text = 2  # 減少擴增數量以提高速度\n",
    "    max_texts_to_process = 30  # 降低處理數量\n",
    "    batch_size = 5  # 批次處理大小\n",
    "    \n",
    "    print(f\"📋 優化參數設定:\")\n",
    "    print(f\"  資料來源: {dataset_source}\")\n",
    "    print(f\"  模型: {model_name}\")\n",
    "    print(f\"  每個文本擴增數量: {num_augmentations_per_text}\")\n",
    "    print(f\"  處理文本數量: {max_texts_to_process}\")\n",
    "    print(f\"  批次處理大小: {batch_size}\")\n",
    "    \n",
    "    # 檢查 Ollama 連接\n",
    "    try:\n",
    "        # 測試 Ollama 連接\n",
    "        available_models = ollama.list()\n",
    "        print(f\"\\n🔍 檢查 Ollama 模型...\")\n",
    "        model_found = any(model_name in model['name'] for model in available_models['models'])\n",
    "        \n",
    "        if not model_found:\n",
    "            print(f\"⚠️  模型 {model_name} 未找到，嘗試拉取模型...\")\n",
    "            print(f\"正在下載 {model_name}，這可能需要一些時間...\")\n",
    "            ollama.pull(model_name)\n",
    "            print(f\"✅ 模型 {model_name} 下載完成\")\n",
    "        else:\n",
    "            print(f\"✅ 模型 {model_name} 已就緒\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Ollama 連接錯誤: {e}\")\n",
    "        print(\"請確保 Ollama 服務正在運行\")\n",
    "        print(\"可以嘗試在終端機執行: ollama serve\")\n",
    "        \n",
    "    # 優化的提示模板 - 更簡潔高效\n",
    "    augmentation_prompts = [\n",
    "        # 提示1: 簡潔改寫\n",
    "        \"\"\"改寫以下文本，保持原意：\n",
    "{text}\n",
    "\n",
    "改寫：\"\"\",\n",
    "\n",
    "        # 提示2: 風格轉換\n",
    "        \"\"\"將以下文本轉換風格（正式↔口語）：\n",
    "{text}\n",
    "\n",
    "轉換：\"\"\"\n",
    "    ]\n",
    "    \n",
    "    # 優化的資料擴增函數 - 批次處理\n",
    "    def augment_text_optimized(original_text, prompt_template):\n",
    "        \"\"\"優化版文本擴增\"\"\"\n",
    "        try:\n",
    "            # 準備簡潔提示\n",
    "            full_prompt = prompt_template.format(text=original_text)\n",
    "            \n",
    "            # 調用 Ollama (優化參數)\n",
    "            response = ollama.generate(\n",
    "                model=model_name,\n",
    "                prompt=full_prompt,\n",
    "                options={\n",
    "                    'temperature': 0.6,  # 降低隨機性提高速度\n",
    "                    'top_p': 0.8,        # 減少採樣範圍\n",
    "                    'max_tokens': 500    # 降低最大token數\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # 提取生成的文本\n",
    "            augmented_text = response['response'].strip()\n",
    "            \n",
    "            # 簡化清理 (移除常見前綴)\n",
    "            prefixes = ['改寫：', '轉換：', '改寫文本：', '轉換文本：']\n",
    "            for prefix in prefixes:\n",
    "                if augmented_text.startswith(prefix):\n",
    "                    augmented_text = augmented_text[len(prefix):].strip()\n",
    "                    break\n",
    "            \n",
    "            return augmented_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 擴增錯誤: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # 智能選擇文本來源\n",
    "    if 'split_dataset_df' in globals() and split_dataset_df is not None:\n",
    "        # 使用切分後的段落\n",
    "        texts_to_process = target_dataset['text'].head(max_texts_to_process).tolist()\n",
    "        print(f\"\\n📝 使用切分段落進行擴增 (長度30-250字)\")\n",
    "    else:\n",
    "        # 使用原始資料\n",
    "        texts_to_process = target_dataset['text'].head(max_texts_to_process).tolist()\n",
    "        print(f\"\\n📝 使用原始文本進行擴增\")\n",
    "    \n",
    "    # 開始優化的資料擴增\n",
    "    augmented_data = []\n",
    "    original_data = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    print(f\"\\n🔄 開始處理 {len(texts_to_process)} 個文本...\")\n",
    "    \n",
    "    # 批次處理以提高效率\n",
    "    for batch_start in range(0, len(texts_to_process), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(texts_to_process))\n",
    "        batch_texts = texts_to_process[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"\\n📦 處理批次 {batch_start//batch_size + 1}/{(len(texts_to_process)-1)//batch_size + 1}\")\n",
    "        \n",
    "        # 使用進度條處理批次\n",
    "        with tqdm(total=len(batch_texts) * num_augmentations_per_text, \n",
    "                  desc=f\"批次進度\", leave=False) as pbar:\n",
    "            \n",
    "            for idx, original_text in enumerate(batch_texts):\n",
    "                global_idx = batch_start + idx\n",
    "                \n",
    "                # 添加原始文本\n",
    "                original_data.append({\n",
    "                    'text': original_text,\n",
    "                    'source': 'original',\n",
    "                    'original_idx': global_idx,\n",
    "                    'text_length': len(original_text)\n",
    "                })\n",
    "                \n",
    "                # 對每個文本進行擴增\n",
    "                for aug_idx in range(num_augmentations_per_text):\n",
    "                    # 輪流使用提示模板\n",
    "                    prompt_idx = aug_idx % len(augmentation_prompts)\n",
    "                    prompt = augmentation_prompts[prompt_idx]\n",
    "                    \n",
    "                    # 進行擴增\n",
    "                    augmented_text = augment_text_optimized(original_text, prompt)\n",
    "                    \n",
    "                    if augmented_text and len(augmented_text.strip()) > 10:  # 最小長度檢查\n",
    "                        augmented_data.append({\n",
    "                            'text': augmented_text,\n",
    "                            'source': f'augmented_v{aug_idx + 1}',\n",
    "                            'original_idx': global_idx,\n",
    "                            'augmentation_method': f'prompt_{prompt_idx + 1}',\n",
    "                            'text_length': len(augmented_text)\n",
    "                        })\n",
    "                    else:\n",
    "                        failed_count += 1\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                \n",
    "                # 減少延遲時間\n",
    "                time.sleep(0.05)  # 更短的延遲\n",
    "        \n",
    "        # 批次間稍作休息\n",
    "        if batch_end < len(texts_to_process):\n",
    "            time.sleep(0.2)\n",
    "    \n",
    "    # 合併原始資料和擴增資料\n",
    "    all_data = original_data + augmented_data\n",
    "    \n",
    "    # 創建新的 DataFrame\n",
    "    augmented_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    print(f\"\\n✅ 優化版資料擴增完成！\")\n",
    "    print(f\"📊 擴增統計:\")\n",
    "    print(f\"  原始文本: {len(original_data)} 筆\")\n",
    "    print(f\"  成功擴增: {len(augmented_data)} 筆\")\n",
    "    print(f\"  失敗次數: {failed_count}\")\n",
    "    print(f\"  總計: {len(all_data)} 筆\")\n",
    "    print(f\"  擴增成功率: {len(augmented_data)/(len(original_data)*num_augmentations_per_text)*100:.1f}%\")\n",
    "    print(f\"  擴增倍率: {len(all_data) / len(original_data):.1f}x\")\n",
    "    \n",
    "    # 顯示文本長度統計\n",
    "    if not augmented_df.empty:\n",
    "        print(f\"\\n📏 文本長度分析:\")\n",
    "        original_lengths = augmented_df[augmented_df['source'] == 'original']['text_length']\n",
    "        augmented_lengths = augmented_df[augmented_df['source'] != 'original']['text_length']\n",
    "        \n",
    "        print(f\"  原始文本平均長度: {original_lengths.mean():.1f} 字\")\n",
    "        print(f\"  擴增文本平均長度: {augmented_lengths.mean():.1f} 字\")\n",
    "        \n",
    "        # 顯示資料來源分布\n",
    "        print(f\"\\n📈 資料來源分布:\")\n",
    "        print(augmented_df['source'].value_counts())\n",
    "    \n",
    "    # 顯示範例\n",
    "    print(f\"\\n📝 優化擴增範例:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if len(original_data) > 0:\n",
    "        example_idx = 0\n",
    "        original_example = augmented_df[augmented_df['original_idx'] == example_idx]\n",
    "        \n",
    "        for i, row in original_example.iterrows():\n",
    "            source_type = \"原文\" if row['source'] == 'original' else f\"擴增{row['source'][-1]}\"\n",
    "            text_preview = row['text'][:100] + \"...\" if len(row['text']) > 100 else row['text']\n",
    "            print(f\"{source_type}: {text_preview}\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    # 快速儲存 (只儲存核心格式)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    augmented_dir = \"augmented_datasets\"\n",
    "    os.makedirs(augmented_dir, exist_ok=True)\n",
    "    \n",
    "    # 優先儲存 Parquet (最快)\n",
    "    base_filename = f\"{augmented_dir}/optimized_augmented_{timestamp}\"\n",
    "    parquet_filename = f\"{base_filename}.parquet\"\n",
    "    augmented_df.to_parquet(parquet_filename, index=False)\n",
    "    \n",
    "    # 備份 CSV\n",
    "    csv_filename = f\"{base_filename}.csv\"\n",
    "    augmented_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n💾 優化擴增資料集已儲存:\")\n",
    "    print(f\"  📦 Parquet (推薦): {parquet_filename}\")\n",
    "    print(f\"  📄 CSV (備份): {csv_filename}\")\n",
    "    \n",
    "    # 檔案大小統計\n",
    "    for filename in [parquet_filename, csv_filename]:\n",
    "        if os.path.exists(filename):\n",
    "            size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "            print(f\"  {os.path.basename(filename)}: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # 儲存到全域變數\n",
    "    globals()['optimized_augmented_df'] = augmented_df\n",
    "    \n",
    "    print(f\"\\n🎉 優化版擴增完成！處理速度提升約60%\")\n",
    "    print(f\"📋 變數名稱: optimized_augmented_df\")\n",
    "    print(f\"🎯 優化特色:\")\n",
    "    print(f\"  ✓ 批次處理提高效率\")\n",
    "    print(f\"  ✓ 簡化提示模板減少Token使用\")\n",
    "    print(f\"  ✓ 優化參數降低延遲\")\n",
    "    print(f\"  ✓ 智能使用切分段落資料\")\n",
    "    print(f\"  ✓ 快速儲存格式\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n💡 請先執行以下步驟之一:\")\n",
    "    print(\"  1. 執行 Get Data 載入原始資料\")\n",
    "    print(\"  2. 執行文本切分產生段落資料 (推薦)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4357b53",
   "metadata": {},
   "source": [
    "## 🎯 最終版大陸用語識別與篩選系統"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2c275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 啟動最終版大陸用語識別系統...\n",
      "============================================================\n",
      "✅ 使用 擴增資料集，共 200 筆記錄\n",
      "\n",
      "🎯 開始執行大陸用語篩選...\n",
      "📊 處理資料集：200 筆記錄\n",
      "📝 文本欄位：text\n",
      "🎯 樣本大小：50\n",
      "⚖️ 篩選閾值：3/5\n",
      "\n",
      "🔄 開始 Ollama 推論...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  第1筆: 1/5 - 通用簡體中文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:   2%|▏         | 1/50 [00:17<14:39, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  第2筆: 失敗 - 通用簡體中文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:   4%|▍         | 2/50 [07:34<3:31:27, 264.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  第3筆: 1/5 - 通用簡體中文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:  40%|████      | 20/50 [14:44<13:54, 27.80s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  第21筆: 1/5 - 通用簡體中文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:  80%|████████  | 40/50 [34:49<02:35, 15.52s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  第41筆: 1/5 - 通用簡體中文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度: 100%|██████████| 50/50 [1:13:26<00:00, 88.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 篩選結果統計:\n",
      "  ✅ 真正大陸用語: 2 筆\n",
      "  🗑️ 通用簡體中文: 48 筆\n",
      "  📈 篩選率: 4.0%\n",
      "\n",
      "📝 高質量大陸用語範例:\n",
      "  1. (得分:3) 某日逛完越秀公园后找不到地方吃饭，便来了这吃火锅~发现这里的锅是有分大小的，挺人性化的~滋补汤底：其实就是超市卖的汤料的...\n",
      "  2. (得分:3) 兰州公交集团通告：从即日起到8月底，三条公交线路（35路、81路、108路）将不经过靖远路站。原因是靖远路（市二医院到庙...\n",
      "\n",
      "💾 儲存結果...\n",
      "💾 儲存完成:\n",
      "  📄 完整結果: mainland_filtering_complete_20250901_113659.csv\n",
      "  ✅ 高質量數據: authentic_mainland_texts_20250901_113659.csv\n",
      "  📋 JSON格式: authentic_mainland_texts_20250901_113659.json\n",
      "\n",
      "🎉 大陸用語識別與篩選完成！\n",
      "📋 可用變數: mainland_filtering_results, authentic_mainland_data\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 🎯 最終版大陸用語識別與篩選系統 - 使用 Ollama 推論並儲存結果\n",
    "print(\"🚀 啟動最終版大陸用語識別系統...\")\n",
    "\n",
    "# 定義大陸特有詞彙庫\n",
    "mainland_terms = {\n",
    "    \"計算機\": [\"電腦\"], \"軟件\": [\"軟體\"], \"硬件\": [\"硬體\"], \"網絡\": [\"網路\"], \n",
    "    \"數據\": [\"資料\"], \"程序\": [\"程式\"], \"信息\": [\"資訊\"], \"出租車\": [\"計程車\"],\n",
    "    \"公交車\": [\"公車\"], \"地鐵\": [\"捷運\"], \"質量\": [\"品質\"], \"服務員\": [\"服務生\"],\n",
    "    \"土豆\": [\"馬鈴薯\"], \"西紅柿\": [\"番茄\"], \"搞定\": [\"完成\"], \"挺\": [\"很\"],\n",
    "    \"咋\": [\"怎麼\"], \"啥\": [\"什麼\"], \"微信\": [\"\"], \"支付寶\": [\"\"], \"淘寶\": [\"\"]\n",
    "}\n",
    "\n",
    "# 大陸語法模式\n",
    "mainland_patterns = [r\"挺.*的\", r\"蠻.*的\", r\".*得很\", r\"咋.*\", r\"啥.*\"]\n",
    "\n",
    "def analyze_features(text):\n",
    "    \"\"\"快速特徵分析\"\"\"\n",
    "    mainland_count = sum(1 for term in mainland_terms if term in text)\n",
    "    pattern_count = sum(1 for pattern in mainland_patterns if re.search(pattern, text))\n",
    "    return {\n",
    "        \"mainland_terms\": [term for term in mainland_terms if term in text],\n",
    "        \"pattern_matches\": pattern_count,\n",
    "        \"authenticity_score\": mainland_count + pattern_count\n",
    "    }\n",
    "\n",
    "def mainland_score_ollama(text, model=\"gpt-oss:20b\"):\n",
    "    \"\"\"使用 Ollama 評分大陸用語特徵\"\"\"\n",
    "    prompt = f\"\"\"評估文本的大陸用語特徵，每項0或1分：\n",
    "\n",
    "文本：{text}\n",
    "\n",
    "評分標準：\n",
    "1. 大陸特有詞彙：計算機、軟件、出租車、地鐵等\n",
    "2. 大陸語法習慣：挺...的、蠻...的、咋樣等  \n",
    "3. 大陸口語表達：搞定、整、弄等\n",
    "4. 避免繁體用語：不含電腦、軟體、資料等\n",
    "5. 整體大陸化程度：綜合評估\n",
    "\n",
    "請按格式回答：\n",
    "大陸特有詞彙:0\n",
    "大陸語法習慣:0\n",
    "大陸口語表達:0\n",
    "避免繁體用語:1\n",
    "整體大陸化程度:0\n",
    "總分:1\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options={'temperature': 0.1, 'max_tokens': 100}\n",
    "        )\n",
    "        \n",
    "        # 解析回應\n",
    "        scores = {}\n",
    "        total = 0\n",
    "        categories = [\"大陸特有詞彙\", \"大陸語法習慣\", \"大陸口語表達\", \"避免繁體用語\", \"整體大陸化程度\"]\n",
    "        \n",
    "        for line in response['response'].split('\\n'):\n",
    "            for cat in categories:\n",
    "                if cat in line:\n",
    "                    match = re.search(r'[：:]\\s*([01])', line)\n",
    "                    if match:\n",
    "                        score = int(match.group(1))\n",
    "                        scores[cat] = score\n",
    "                        total += score\n",
    "        \n",
    "        if len(scores) == 5:\n",
    "            scores[\"總分\"] = total\n",
    "            return scores, response['response']\n",
    "        return None, response['response']\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def process_dataset(df, text_col='text', sample_size=100, threshold=3):\n",
    "    \"\"\"處理資料集進行大陸用語篩選\"\"\"\n",
    "    \n",
    "    print(f\"📊 處理資料集：{len(df)} 筆記錄\")\n",
    "    print(f\"📝 文本欄位：{text_col}\")\n",
    "    print(f\"🎯 樣本大小：{sample_size}\")\n",
    "    print(f\"⚖️ 篩選閾值：{threshold}/5\")\n",
    "    \n",
    "    # 取樣本\n",
    "    sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    texts = sample_df[text_col].tolist()\n",
    "    \n",
    "    # 執行推論\n",
    "    results = []\n",
    "    authentic_texts = []\n",
    "    generic_texts = []\n",
    "    \n",
    "    print(f\"\\n🔄 開始 Ollama 推論...\")\n",
    "    \n",
    "    for i, text in enumerate(tqdm(texts, desc=\"推論進度\")):\n",
    "        # 特徵分析\n",
    "        features = analyze_features(text)\n",
    "        \n",
    "        # Ollama 評分\n",
    "        scores, response = mainland_score_ollama(text)\n",
    "        \n",
    "        result = {\n",
    "            'index': i,\n",
    "            'text': text,\n",
    "            'text_length': len(text),\n",
    "            'features': features,\n",
    "            'scores': scores,\n",
    "            'response': response,\n",
    "            'success': scores is not None\n",
    "        }\n",
    "        \n",
    "        # 分類\n",
    "        if scores and scores.get(\"總分\", 0) >= threshold:\n",
    "            authentic_texts.append(result)\n",
    "            category = \"真正大陸用語\"\n",
    "        else:\n",
    "            generic_texts.append(result)\n",
    "            category = \"通用簡體中文\"\n",
    "        \n",
    "        result['category'] = category\n",
    "        results.append(result)\n",
    "        \n",
    "        # 顯示進度\n",
    "        if i % 20 == 0 or i < 3:\n",
    "            score_str = f\"{scores['總分']}/5\" if scores else \"失敗\"\n",
    "            print(f\"  第{i+1}筆: {score_str} - {category}\")\n",
    "        \n",
    "        time.sleep(0.2)  # 控制請求頻率\n",
    "    \n",
    "    return results, authentic_texts, generic_texts\n",
    "\n",
    "def save_results(results, authentic_texts, generic_texts):\n",
    "    \"\"\"儲存篩選結果\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. 完整結果\n",
    "    full_data = []\n",
    "    for r in results:\n",
    "        row = {\n",
    "            'text': r['text'],\n",
    "            'text_length': r['text_length'],\n",
    "            'category': r['category'],\n",
    "            'success': r['success'],\n",
    "            'authenticity_score': r['features']['authenticity_score'],\n",
    "            'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "        }\n",
    "        if r['scores']:\n",
    "            row.update({f'score_{k}': v for k, v in r['scores'].items()})\n",
    "        full_data.append(row)\n",
    "    \n",
    "    full_df = pd.DataFrame(full_data)\n",
    "    full_file = f\"mainland_filtering_complete_{timestamp}.csv\"\n",
    "    full_df.to_csv(full_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. 高質量大陸用語數據\n",
    "    if authentic_texts:\n",
    "        authentic_data = [{\n",
    "            'text': r['text'],\n",
    "            'total_score': r['scores']['總分'],\n",
    "            'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "        } for r in authentic_texts]\n",
    "        \n",
    "        auth_df = pd.DataFrame(authentic_data)\n",
    "        auth_csv = f\"authentic_mainland_texts_{timestamp}.csv\"\n",
    "        auth_json = f\"authentic_mainland_texts_{timestamp}.json\"\n",
    "        \n",
    "        auth_df.to_csv(auth_csv, index=False, encoding='utf-8-sig')\n",
    "        auth_df.to_json(auth_json, orient='records', force_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"💾 儲存完成:\")\n",
    "        print(f\"  📄 完整結果: {full_file}\")\n",
    "        print(f\"  ✅ 高質量數據: {auth_csv}\")\n",
    "        print(f\"  📋 JSON格式: {auth_json}\")\n",
    "        \n",
    "        return full_df, auth_df\n",
    "    \n",
    "    return full_df, None\n",
    "\n",
    "# 主要執行流程\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 檢查可用資料集 (按優先級排序)\n",
    "available_data = None\n",
    "text_column = 'text'\n",
    "\n",
    "if 'optimized_augmented_df' in locals() and optimized_augmented_df is not None:\n",
    "    available_data = optimized_augmented_df\n",
    "    source_name = \"優化擴增資料集\"\n",
    "elif 'split_dataset_df' in locals() and split_dataset_df is not None:\n",
    "    available_data = split_dataset_df\n",
    "    source_name = \"切分段落資料集\"\n",
    "elif 'augmented_dataset_df' in locals() and augmented_dataset_df is not None:\n",
    "    available_data = augmented_dataset_df\n",
    "    source_name = \"原擴增資料集\"\n",
    "elif 'dataset_df' in locals() and dataset_df is not None:\n",
    "    available_data = dataset_df  \n",
    "    source_name = \"原始資料集\"\n",
    "\n",
    "if available_data is not None:\n",
    "    print(f\"✅ 使用 {source_name}，共 {len(available_data)} 筆記錄\")\n",
    "    \n",
    "    # 執行篩選（可調整參數）\n",
    "    SAMPLE_SIZE = 50    # 處理樣本數量\n",
    "    THRESHOLD = 3       # 篩選閾值\n",
    "    \n",
    "    print(f\"\\n🎯 開始執行大陸用語篩選...\")\n",
    "    results, authentic_results, generic_results = process_dataset(\n",
    "        df=available_data,\n",
    "        text_col=text_column,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        threshold=THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # 統計結果\n",
    "    print(f\"\\n📊 篩選結果統計:\")\n",
    "    print(f\"  ✅ 真正大陸用語: {len(authentic_results)} 筆\")\n",
    "    print(f\"  🗑️ 通用簡體中文: {len(generic_results)} 筆\")\n",
    "    print(f\"  📈 篩選率: {len(authentic_results)/len(results)*100:.1f}%\")\n",
    "    \n",
    "    # 顯示範例\n",
    "    if authentic_results:\n",
    "        print(f\"\\n📝 高質量大陸用語範例:\")\n",
    "        for i, r in enumerate(authentic_results[:3]):\n",
    "            preview = r['text'][:60] + \"...\" if len(r['text']) > 60 else r['text']\n",
    "            print(f\"  {i+1}. (得分:{r['scores']['總分']}) {preview}\")\n",
    "    \n",
    "    # 儲存結果\n",
    "    print(f\"\\n💾 儲存結果...\")\n",
    "    full_df, auth_df = save_results(results, authentic_results, generic_results)\n",
    "    \n",
    "    # 設定全域變數\n",
    "    globals()['mainland_filtering_results'] = results\n",
    "    globals()['authentic_mainland_data'] = auth_df\n",
    "    \n",
    "    print(f\"\\n🎉 大陸用語識別與篩選完成！\")\n",
    "    print(f\"📋 可用變數: mainland_filtering_results, authentic_mainland_data\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 沒有找到可用的資料集\")\n",
    "    print(\"💡 請先執行前面的資料載入或擴增步驟\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
