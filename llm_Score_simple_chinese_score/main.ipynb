{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008ebe94",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9d4a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "import os\n",
    "import ollama\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712a89a",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c0fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== è³‡æ–™è¼‰å…¥é¸é … ===\n",
      "ä½¿ç”¨æœ¬åœ°æª”æ¡ˆ: True\n",
      "ä½¿ç”¨ä¸²æµæ¨¡å¼: False\n",
      "ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†: False\n",
      "\n",
      "ğŸ“ å¾æœ¬åœ°æª”æ¡ˆè®€å–è³‡æ–™...\n",
      "æ‰¾åˆ°çš„æª”æ¡ˆ:\n",
      "  CSV æª”æ¡ˆ: 1 å€‹\n",
      "  JSON æª”æ¡ˆ: 1 å€‹\n",
      "  Parquet æª”æ¡ˆ: 1 å€‹\n",
      "\n",
      "ğŸ“Š è®€å–æœ€æ–°çš„ Parquet æª”æ¡ˆ: saved_datasets/clue_corpus_small_20250901_085816.parquet\n",
      "\n",
      "âœ… è³‡æ–™è¼‰å…¥æˆåŠŸï¼\n",
      "ğŸ“Š è³‡æ–™å½¢ç‹€: (1000, 1)\n",
      "ğŸ“‹ æ¬„ä½åç¨±: ['text']\n",
      "\n",
      "ğŸ“ˆ æ–‡æœ¬é•·åº¦çµ±è¨ˆ:\n",
      "count     1000.000000\n",
      "mean       300.899000\n",
      "std        785.763282\n",
      "min          5.000000\n",
      "25%         38.000000\n",
      "50%        105.500000\n",
      "75%        274.000000\n",
      "max      17020.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "ğŸ“ å‰ 3 ç­†è³‡æ–™ç¯„ä¾‹:\n",
      "ç¯„ä¾‹ 1 (132 å­—ç¬¦): 130çœŸæ˜¯ä½©æœè¿™å®¶åº—å¼€è¿™ä¹ˆä¹…ã€‚å°½ç®¡é—¨é¢å·²ç»å°äº†ä¸€åœˆï¼Œä½†è¿˜æ˜¯å¼€ç€ä¸å®¹æ˜“å•Šã€‚æˆ‘ä»¬ä¸å®¹æ˜“ï¼Œè€æ¿ä¹Ÿä¸å®¹æ˜“ã€‚è‡ªåŠ©é¤ï¼Œä½ å¯ä»¥åƒå¾—æ¯”å¹³æ—¶å¤šï¼Œä½†å†³ä¸èƒ½æµªè´¹ã€‚æƒ³åƒå›20å…ƒï¼Œé‚£æ˜¯ä¸å¯èƒ½çš„ï¼Œæ‰€ä»¥è¿˜æ˜¯ä¸è¦å»äº†ã€‚èœçœŸçš„å¾ˆä¸€èˆ¬ï¼Œ...\n",
      "--------------------------------------------------------------------------------\n",
      "ç¯„ä¾‹ 2 (8 å­—ç¬¦): é€è´§é€Ÿåº¦å¥‡æ…¢æ— æ¯”\n",
      "--------------------------------------------------------------------------------\n",
      "ç¯„ä¾‹ 3 (12 å­—ç¬¦): è¿™æ˜¯è‡ªå·±ç”¨è¿‡æœ€å¥½çš„ç”¨çš„äº†\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ¯ è³‡æ–™å·²æº–å‚™å°±ç·’ï¼Œå¯ç”¨æ–¼å¾ŒçºŒçš„ LLM è©•åˆ†è™•ç†ï¼\n"
     ]
    }
   ],
   "source": [
    "# è³‡æ–™è®€å–é¸é …\n",
    "# æ‚¨å¯ä»¥é¸æ“‡ä»¥ä¸‹ä»»ä¸€ç¨®æ–¹å¼ä¾†è¼‰å…¥è³‡æ–™ï¼š\n",
    "\n",
    "# é¸é … 1: å¾å·²å„²å­˜çš„æœ¬åœ°æª”æ¡ˆè®€å– (æ¨è–¦ï¼Œé€Ÿåº¦å¿«)\n",
    "use_local_files = False\n",
    "\n",
    "# é¸é … 2: å¾ Hugging Face ç›´æ¥ä¸²æµè¼‰å…¥ (éœ€è¦ç¶²è·¯é€£ç·š)\n",
    "use_streaming = True\n",
    "\n",
    "# é¸é … 3: ä¸‹è¼‰å®Œæ•´è³‡æ–™é›† (æª”æ¡ˆå¾ˆå¤§ï¼Œä¸æ¨è–¦)\n",
    "use_full_download = False\n",
    "\n",
    "print(\"=== è³‡æ–™è¼‰å…¥é¸é … ===\")\n",
    "print(f\"ä½¿ç”¨æœ¬åœ°æª”æ¡ˆ: {use_local_files}\")\n",
    "print(f\"ä½¿ç”¨ä¸²æµæ¨¡å¼: {use_streaming}\")\n",
    "print(f\"ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†: {use_full_download}\")\n",
    "\n",
    "# è³‡æ–™è¼‰å…¥\n",
    "if use_local_files:\n",
    "    print(\"\\nğŸ“ å¾æœ¬åœ°æª”æ¡ˆè®€å–è³‡æ–™...\")\n",
    "    \n",
    "    # æª¢æŸ¥å·²å„²å­˜çš„æª”æ¡ˆ\n",
    "    save_dir = \"saved_datasets\"\n",
    "    \n",
    "    if os.path.exists(save_dir):\n",
    "        import glob\n",
    "        \n",
    "        # å°‹æ‰¾å¯ç”¨çš„æª”æ¡ˆ\n",
    "        csv_files = glob.glob(f\"{save_dir}/*.csv\")\n",
    "        json_files = glob.glob(f\"{save_dir}/*.json\")\n",
    "        parquet_files = glob.glob(f\"{save_dir}/*.parquet\")\n",
    "        \n",
    "        print(f\"æ‰¾åˆ°çš„æª”æ¡ˆ:\")\n",
    "        print(f\"  CSV æª”æ¡ˆ: {len(csv_files)} å€‹\")\n",
    "        print(f\"  JSON æª”æ¡ˆ: {len(json_files)} å€‹\")\n",
    "        print(f\"  Parquet æª”æ¡ˆ: {len(parquet_files)} å€‹\")\n",
    "        \n",
    "        # å„ªå…ˆä½¿ç”¨ Parquet æª”æ¡ˆ (æœ€é«˜æ•ˆ)\n",
    "        if parquet_files:\n",
    "            latest_file = max(parquet_files, key=os.path.getctime)\n",
    "            print(f\"\\nğŸ“Š è®€å–æœ€æ–°çš„ Parquet æª”æ¡ˆ: {latest_file}\")\n",
    "            df = pd.read_parquet(latest_file)\n",
    "            \n",
    "        # å…¶æ¬¡ä½¿ç”¨ CSV æª”æ¡ˆ\n",
    "        elif csv_files:\n",
    "            latest_file = max(csv_files, key=os.path.getctime)\n",
    "            print(f\"\\nğŸ“Š è®€å–æœ€æ–°çš„ CSV æª”æ¡ˆ: {latest_file}\")\n",
    "            df = pd.read_csv(latest_file)\n",
    "            \n",
    "        # æœ€å¾Œä½¿ç”¨ JSON æª”æ¡ˆ\n",
    "        elif json_files:\n",
    "            latest_file = max(json_files, key=os.path.getctime)\n",
    "            print(f\"\\nğŸ“Š è®€å–æœ€æ–°çš„ JSON æª”æ¡ˆ: {latest_file}\")\n",
    "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ æ²’æœ‰æ‰¾åˆ°å·²å„²å­˜çš„è³‡æ–™æª”æ¡ˆ\")\n",
    "            print(\"è«‹å…ˆåŸ·è¡Œè³‡æ–™ä¸‹è¼‰å’Œå„²å­˜çš„ç¨‹å¼ç¢¼\")\n",
    "            df = None\n",
    "    else:\n",
    "        print(\"âŒ æ‰¾ä¸åˆ° saved_datasets ç›®éŒ„\")\n",
    "        print(\"è«‹å…ˆåŸ·è¡Œè³‡æ–™ä¸‹è¼‰å’Œå„²å­˜çš„ç¨‹å¼ç¢¼\")\n",
    "        df = None\n",
    "\n",
    "elif use_streaming:\n",
    "    print(\"\\nğŸŒ å¾ Hugging Face ä¸²æµè¼‰å…¥è³‡æ–™...\")\n",
    "    \n",
    "    # ä½¿ç”¨ä¸²æµæ¨¡å¼è¼‰å…¥è³‡æ–™é›†\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\", streaming=True)\n",
    "    \n",
    "    # è¨­å®šè¦è¼‰å…¥çš„æ¨£æœ¬æ•¸é‡ - æ¸›å°‘åˆ°100ç­†ç”¨æ–¼æ¼”ç¤º\n",
    "    num_samples = 100\n",
    "    print(f\"è¼‰å…¥å‰ {num_samples} ç­†è³‡æ–™...\")\n",
    "    \n",
    "    # æ”¶é›†è³‡æ–™\n",
    "    sample_data = []\n",
    "    for i, example in enumerate(dataset['train']):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        sample_data.append(example)\n",
    "        if (i + 1) % 25 == 0:\n",
    "            print(f\"  å·²è¼‰å…¥ {i + 1} ç­†è³‡æ–™...\")\n",
    "    \n",
    "    # è½‰æ›ç‚º DataFrame\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    \n",
    "elif use_full_download:\n",
    "    print(\"\\nâ¬‡ï¸ ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†...\")\n",
    "    print(\"è­¦å‘Šï¼šé€™å°‡ä¸‹è¼‰ 13.7GB çš„è³‡æ–™ï¼Œå¯èƒ½éœ€è¦å¾ˆé•·æ™‚é–“\")\n",
    "    \n",
    "    # ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\")\n",
    "    df = dataset['train'].to_pandas()\n",
    "\n",
    "else:\n",
    "    print(\"âŒ æ²’æœ‰é¸æ“‡ä»»ä½•è³‡æ–™è¼‰å…¥é¸é …\")\n",
    "    df = None\n",
    "\n",
    "# é¡¯ç¤ºè³‡æ–™è³‡è¨Š\n",
    "if df is not None:\n",
    "    print(f\"\\nâœ… è³‡æ–™è¼‰å…¥æˆåŠŸï¼\")\n",
    "    print(f\"ğŸ“Š è³‡æ–™å½¢ç‹€: {df.shape}\")\n",
    "    print(f\"ğŸ“‹ æ¬„ä½åç¨±: {list(df.columns)}\")\n",
    "    \n",
    "    # é¡¯ç¤ºåŸºæœ¬çµ±è¨ˆ\n",
    "    if 'text' in df.columns: # type: ignore\n",
    "        df['text_length'] = df['text'].str.len() # type: ignore\n",
    "        print(f\"\\nğŸ“ˆ æ–‡æœ¬é•·åº¦çµ±è¨ˆ:\")\n",
    "        print(df['text_length'].describe()) # type: ignore\n",
    "        \n",
    "        # é¡¯ç¤ºå‰å¹¾ç­†è³‡æ–™ç¯„ä¾‹\n",
    "        print(f\"\\nğŸ“ å‰ 3 ç­†è³‡æ–™ç¯„ä¾‹:\")\n",
    "        for i in range(min(3, len(df))): # type: ignore\n",
    "            text = df.iloc[i]['text']\n",
    "            # é¡¯ç¤ºå‰100å€‹å­—ç¬¦\n",
    "            preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            print(f\"ç¯„ä¾‹ {i+1} ({len(text)} å­—ç¬¦): {preview}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ è³‡æ–™å·²æº–å‚™å°±ç·’ï¼Œå¯ç”¨æ–¼å¾ŒçºŒçš„ LLM è©•åˆ†è™•ç†ï¼\")\n",
    "else:\n",
    "    print(\"\\nâŒ è³‡æ–™è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥è¨­å®šä¸¦é‡æ–°åŸ·è¡Œ\")\n",
    "\n",
    "# å„²å­˜åˆ°å…¨åŸŸè®Šæ•¸ä¾›å¾ŒçºŒä½¿ç”¨\n",
    "globals()['dataset_df'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af633432",
   "metadata": {},
   "source": [
    "## ğŸ“ æ–‡æœ¬åˆ‡åˆ†è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ebb3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”ª å•Ÿå‹•æ–‡æœ¬åˆ‡åˆ†è™•ç†...\n",
      "âŒ æ²’æœ‰æ‰¾åˆ°è³‡æ–™é›†ï¼Œè«‹å…ˆåŸ·è¡Œ Get Data éƒ¨åˆ†\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“ æ–‡æœ¬åˆ‡åˆ†è™•ç† - åˆ‡åˆ†ç‚ºå¥å­ç´šåˆ¥çš„ç‰‡æ®µ\n",
    "print(\"ğŸ”ª å•Ÿå‹•æ–‡æœ¬åˆ‡åˆ†è™•ç†...\")\n",
    "\n",
    "def split_text_to_sentences(text, min_length=10, max_length=50):\n",
    "    \"\"\"\n",
    "    å°‡æ–‡æœ¬åˆ‡åˆ†ç‚ºå¥å­ç´šåˆ¥çš„ç‰‡æ®µ\n",
    "    \n",
    "    Args:\n",
    "        text (str): åŸå§‹æ–‡æœ¬\n",
    "        min_length (int): æœ€å°ç‰‡æ®µé•·åº¦\n",
    "        max_length (int): æœ€å¤§ç‰‡æ®µé•·åº¦\n",
    "    \n",
    "    Returns:\n",
    "        list: åˆ‡åˆ†å¾Œçš„å¥å­ç‰‡æ®µåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # å®šç¾©æ¨™é»ç¬¦è™Ÿåˆ†éš”ç¬¦\n",
    "    sentence_separators = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'â€¦']  # å¼·å¥è™Ÿåˆ†éš”ç¬¦\n",
    "    phrase_separators = ['ï¼Œ', 'ã€', 'ï¼š', 'ï¼›']  # å¼±åˆ†éš”ç¬¦\n",
    "    \n",
    "    # 1. é¦–å…ˆæŒ‰å¼·æ¨™é»ç¬¦è™Ÿåˆ‡åˆ†æˆå¥å­\n",
    "    sentences = []\n",
    "    current_sentence = \"\"\n",
    "    \n",
    "    for char in text:\n",
    "        current_sentence += char\n",
    "        if char in sentence_separators:\n",
    "            if current_sentence.strip():\n",
    "                sentences.append(current_sentence.strip())\n",
    "                current_sentence = \"\"\n",
    "    \n",
    "    # è™•ç†æœ€å¾Œä¸€å€‹å¥å­ï¼ˆå¦‚æœæ²’æœ‰ä»¥å¼·æ¨™é»çµå°¾ï¼‰\n",
    "    if current_sentence.strip():\n",
    "        sentences.append(current_sentence.strip())\n",
    "    \n",
    "    # 2. å°æ¯å€‹å¥å­é€²ä¸€æ­¥æŒ‰é€—è™Ÿç­‰åˆ†éš”ç¬¦åˆ‡åˆ†\n",
    "    fragments = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # è·³éå¤ªçŸ­çš„å¥å­\n",
    "        if len(sentence) < min_length:\n",
    "            continue\n",
    "            \n",
    "        # å¦‚æœå¥å­é•·åº¦åœ¨åˆç†ç¯„åœå…§ï¼Œç›´æ¥ä½¿ç”¨\n",
    "        if len(sentence) <= max_length:\n",
    "            fragments.append(sentence)\n",
    "        else:\n",
    "            # å°é•·å¥å­æŒ‰é€—è™Ÿç­‰é€²ä¸€æ­¥åˆ‡åˆ†\n",
    "            parts = []\n",
    "            current_part = \"\"\n",
    "            \n",
    "            for char in sentence:\n",
    "                current_part += char\n",
    "                if char in phrase_separators:\n",
    "                    if current_part.strip() and len(current_part.strip()) >= min_length:\n",
    "                        parts.append(current_part.strip())\n",
    "                        current_part = \"\"\n",
    "            \n",
    "            # è™•ç†æœ€å¾Œä¸€éƒ¨åˆ†\n",
    "            if current_part.strip() and len(current_part.strip()) >= min_length:\n",
    "                parts.append(current_part.strip())\n",
    "            \n",
    "            # å¦‚æœåˆ‡åˆ†å¾Œçš„éƒ¨åˆ†å¤ªçŸ­ï¼Œå˜—è©¦åˆä½µ\n",
    "            merged_parts = []\n",
    "            temp_part = \"\"\n",
    "            \n",
    "            for part in parts:\n",
    "                if len(temp_part + part) <= max_length:\n",
    "                    temp_part = temp_part + part if temp_part else part\n",
    "                else:\n",
    "                    if temp_part and len(temp_part) >= min_length:\n",
    "                        merged_parts.append(temp_part)\n",
    "                    temp_part = part\n",
    "            \n",
    "            # æ·»åŠ æœ€å¾Œä¸€éƒ¨åˆ†\n",
    "            if temp_part and len(temp_part) >= min_length:\n",
    "                merged_parts.append(temp_part)\n",
    "            \n",
    "            # å¦‚æœåˆ‡åˆ†å¤±æ•—ï¼Œç›´æ¥æˆªæ–·\n",
    "            if not merged_parts and len(sentence) >= min_length:\n",
    "                # ç°¡å–®æˆªæ–·æˆåˆé©é•·åº¦çš„ç‰‡æ®µ\n",
    "                for i in range(0, len(sentence), max_length):\n",
    "                    fragment = sentence[i:i+max_length]\n",
    "                    if len(fragment) >= min_length:\n",
    "                        merged_parts.append(fragment)\n",
    "            \n",
    "            fragments.extend(merged_parts)\n",
    "    \n",
    "    return fragments\n",
    "\n",
    "def split_text_by_punctuation(text, min_length=10, max_length=50):\n",
    "    \"\"\"\n",
    "    å…¼å®¹æ€§å‡½æ•¸ - èª¿ç”¨æ–°çš„å¥å­åˆ‡åˆ†å‡½æ•¸\n",
    "    \"\"\"\n",
    "    return split_text_to_sentences(text, min_length, max_length)\n",
    "\n",
    "def process_text_splitting(df, text_column='text', min_length=10, max_length=50):\n",
    "    \"\"\"\n",
    "    è™•ç†æ•´å€‹è³‡æ–™é›†çš„æ–‡æœ¬åˆ‡åˆ† - å¥å­ç´šåˆ¥åˆ‡åˆ†\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): åŸå§‹è³‡æ–™é›†\n",
    "        text_column (str): æ–‡æœ¬æ¬„ä½åç¨±\n",
    "        min_length (int): æœ€å°å¥å­ç‰‡æ®µé•·åº¦\n",
    "        max_length (int): æœ€å¤§å¥å­ç‰‡æ®µé•·åº¦\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: åˆ‡åˆ†å¾Œçš„è³‡æ–™é›†\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“Š é–‹å§‹è™•ç†æ–‡æœ¬åˆ‡åˆ†...\")\n",
    "    print(f\"  åŸå§‹è³‡æ–™ç­†æ•¸: {len(df)}\")\n",
    "    print(f\"  æ–‡æœ¬æ¬„ä½: {text_column}\")\n",
    "    print(f\"  å¥å­ç‰‡æ®µé•·åº¦ç¯„åœ: {min_length}-{max_length} å­—\")\n",
    "    \n",
    "    split_data = []\n",
    "    total_fragments = 0\n",
    "    processed_texts = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"åˆ‡åˆ†é€²åº¦\"):\n",
    "        original_text = row[text_column]\n",
    "        \n",
    "        # è·³éå¤ªçŸ­çš„æ–‡æœ¬\n",
    "        if len(original_text) < min_length:\n",
    "            continue\n",
    "        \n",
    "        # åˆ‡åˆ†æ–‡æœ¬ç‚ºå¥å­ç‰‡æ®µ\n",
    "        fragments = split_text_to_sentences(original_text, min_length, max_length)\n",
    "        \n",
    "        # ç‚ºæ¯å€‹ç‰‡æ®µå‰µå»ºæ–°è¨˜éŒ„\n",
    "        for frag_idx, fragment in enumerate(fragments):\n",
    "            new_row = row.copy()\n",
    "            new_row[text_column] = fragment\n",
    "            new_row['original_index'] = idx\n",
    "            new_row['fragment_index'] = frag_idx\n",
    "            new_row['original_text_length'] = len(original_text)\n",
    "            new_row['fragment_length'] = len(fragment)\n",
    "            new_row['source_type'] = 'sentence_fragment'\n",
    "            \n",
    "            split_data.append(new_row)\n",
    "            total_fragments += 1\n",
    "        \n",
    "        processed_texts += 1\n",
    "    \n",
    "    # å‰µå»ºæ–°çš„DataFrame\n",
    "    split_df = pd.DataFrame(split_data)\n",
    "    \n",
    "    print(f\"\\nâœ… æ–‡æœ¬åˆ‡åˆ†å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“ˆ åˆ‡åˆ†çµ±è¨ˆ:\")\n",
    "    print(f\"  è™•ç†æ–‡æœ¬æ•¸: {processed_texts}\")\n",
    "    print(f\"  ç”Ÿæˆå¥å­ç‰‡æ®µæ•¸: {total_fragments}\")\n",
    "    print(f\"  å¹³å‡æ¯æ–‡æœ¬ç‰‡æ®µæ•¸: {total_fragments/processed_texts:.1f}\")\n",
    "    \n",
    "    if not split_df.empty:\n",
    "        print(f\"\\nğŸ“ ç‰‡æ®µé•·åº¦çµ±è¨ˆ:\")\n",
    "        length_stats = split_df['fragment_length'].describe()\n",
    "        print(f\"  å¹³å‡é•·åº¦: {length_stats['mean']:.1f} å­—\")\n",
    "        print(f\"  æœ€çŸ­ç‰‡æ®µ: {length_stats['min']:.0f} å­—\")\n",
    "        print(f\"  æœ€é•·ç‰‡æ®µ: {length_stats['max']:.0f} å­—\")\n",
    "        print(f\"  ä¸­ä½æ•¸é•·åº¦: {length_stats['50%']:.1f} å­—\")\n",
    "        \n",
    "        # é•·åº¦åˆ†å¸ƒ\n",
    "        length_ranges = [\n",
    "            (10, 20, \"çŸ­ç‰‡æ®µ\"),\n",
    "            (20, 30, \"ä¸­çŸ­ç‰‡æ®µ\"),\n",
    "            (30, 40, \"ä¸­ç­‰ç‰‡æ®µ\"),\n",
    "            (40, 50, \"é•·ç‰‡æ®µ\"),\n",
    "            (50, 100, \"è¶…é•·ç‰‡æ®µ\")\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ç‰‡æ®µé•·åº¦åˆ†å¸ƒ:\")\n",
    "        for min_len, max_len, desc in length_ranges:\n",
    "            count = len(split_df[(split_df['fragment_length'] >= min_len) & \n",
    "                                (split_df['fragment_length'] < max_len)])\n",
    "            percentage = count / len(split_df) * 100\n",
    "            print(f\"  {desc} ({min_len}-{max_len}å­—): {count} å€‹ ({percentage:.1f}%)\")\n",
    "    \n",
    "    return split_df\n",
    "\n",
    "# åŸ·è¡Œæ–‡æœ¬åˆ‡åˆ†\n",
    "if 'dataset_df' in globals() and dataset_df is not None:\n",
    "    print(f\"ğŸ¯ å°è¼‰å…¥çš„è³‡æ–™é›†é€²è¡Œå¥å­ç´šåˆ¥æ–‡æœ¬åˆ‡åˆ†...\")\n",
    "    \n",
    "    # è¨­å®šåˆ‡åˆ†åƒæ•¸ - æ”¹ç‚ºå¥å­ç´šåˆ¥\n",
    "    MIN_FRAGMENT_LENGTH = 10   # æœ€å°ç‰‡æ®µé•·åº¦\n",
    "    MAX_FRAGMENT_LENGTH = 50   # æœ€å¤§ç‰‡æ®µé•·åº¦\n",
    "    \n",
    "    print(f\"\\nâš™ï¸ åˆ‡åˆ†åƒæ•¸:\")\n",
    "    print(f\"  æœ€å°ç‰‡æ®µé•·åº¦: {MIN_FRAGMENT_LENGTH} å­—\")\n",
    "    print(f\"  æœ€å¤§ç‰‡æ®µé•·åº¦: {MAX_FRAGMENT_LENGTH} å­—\")\n",
    "    print(f\"  åˆ‡åˆ†æ¨¡å¼: å¥å­ç´šåˆ¥\")\n",
    "    \n",
    "    # åŸ·è¡Œåˆ‡åˆ†\n",
    "    split_dataset_df = process_text_splitting(\n",
    "        df=dataset_df, \n",
    "        text_column='text',\n",
    "        min_length=MIN_FRAGMENT_LENGTH,\n",
    "        max_length=MAX_FRAGMENT_LENGTH\n",
    "    )\n",
    "    \n",
    "    if not split_dataset_df.empty:\n",
    "        # é¡¯ç¤ºåˆ‡åˆ†ç¯„ä¾‹\n",
    "        print(f\"\\nğŸ“ åˆ‡åˆ†ç¯„ä¾‹:\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # æ‰¾ä¸€å€‹æœ‰å¤šå€‹ç‰‡æ®µçš„åŸå§‹æ–‡æœ¬\n",
    "        sample_original_idx = split_dataset_df['original_index'].value_counts().index[0]\n",
    "        sample_fragments = split_dataset_df[split_dataset_df['original_index'] == sample_original_idx]\n",
    "        \n",
    "        print(f\"åŸå§‹æ–‡æœ¬ #{sample_original_idx} è¢«åˆ‡åˆ†ç‚º {len(sample_fragments)} å€‹å¥å­ç‰‡æ®µ:\")\n",
    "        print()\n",
    "        \n",
    "        # é¡¯ç¤ºåŸå§‹æ–‡æœ¬\n",
    "        original_text = dataset_df.iloc[sample_original_idx]['text']\n",
    "        print(f\"åŸå§‹æ–‡æœ¬: {original_text[:200]}{'...' if len(original_text) > 200 else ''}\")\n",
    "        print()\n",
    "        print(\"åˆ‡åˆ†çµæœ:\")\n",
    "        \n",
    "        for i, (_, row) in enumerate(sample_fragments.iterrows()):\n",
    "            fragment = row['text']\n",
    "            length = row['fragment_length']\n",
    "            print(f\"ç‰‡æ®µ {i+1} ({length}å­—): {fragment}\")\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        # å„²å­˜åˆ‡åˆ†å¾Œçš„è³‡æ–™é›†\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        split_dir = \"split_datasets\"\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        # å„²å­˜ç‚ºå¤šç¨®æ ¼å¼\n",
    "        base_filename = f\"{split_dir}/sentence_fragments_{timestamp}\"\n",
    "        \n",
    "        # CSV æ ¼å¼\n",
    "        csv_filename = f\"{base_filename}.csv\"\n",
    "        split_dataset_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        # JSON æ ¼å¼\n",
    "        json_filename = f\"{base_filename}.json\"\n",
    "        split_dataset_df.to_json(json_filename, orient='records', force_ascii=False, indent=2)\n",
    "        \n",
    "        # Parquet æ ¼å¼\n",
    "        parquet_filename = f\"{base_filename}.parquet\"\n",
    "        split_dataset_df.to_parquet(parquet_filename, index=False)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ å¥å­ç‰‡æ®µè³‡æ–™é›†å·²å„²å­˜:\")\n",
    "        print(f\"  ğŸ“„ CSV: {csv_filename}\")\n",
    "        print(f\"  ğŸ“‹ JSON: {json_filename}\")\n",
    "        print(f\"  ğŸ“¦ Parquet: {parquet_filename}\")\n",
    "        \n",
    "        # æª”æ¡ˆå¤§å°çµ±è¨ˆ\n",
    "        print(f\"\\nğŸ“ æª”æ¡ˆå¤§å°:\")\n",
    "        for filename in [csv_filename, json_filename, parquet_filename]:\n",
    "            if os.path.exists(filename):\n",
    "                size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "                print(f\"  {os.path.basename(filename)}: {size_mb:.2f} MB\")\n",
    "        \n",
    "        # å„²å­˜åˆ°å…¨åŸŸè®Šæ•¸\n",
    "        globals()['split_dataset_df'] = split_dataset_df\n",
    "        \n",
    "        print(f\"\\nğŸ‰ å¥å­ç´šåˆ¥æ–‡æœ¬åˆ‡åˆ†è™•ç†å®Œæˆï¼\")\n",
    "        print(f\"ğŸ“‹ è®Šæ•¸åç¨±: split_dataset_df\")\n",
    "        print(f\"ğŸ¯ å¥å­ç‰‡æ®µè³‡æ–™é›†å¯ç”¨æ–¼å¾ŒçºŒçš„ LLM è™•ç†ï¼\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âŒ åˆ‡åˆ†å¾Œæ²’æœ‰ç”¢ç”Ÿæœ‰æ•ˆç‰‡æ®µï¼Œè«‹æª¢æŸ¥åŸå§‹è³‡æ–™\")\n",
    "        split_dataset_df = None\n",
    "\n",
    "else:\n",
    "    print(\"âŒ æ²’æœ‰æ‰¾åˆ°è³‡æ–™é›†ï¼Œè«‹å…ˆåŸ·è¡Œ Get Data éƒ¨åˆ†\")\n",
    "    split_dataset_df = None\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525fac3d",
   "metadata": {},
   "source": [
    "## LLM AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc2c275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å•Ÿå‹•æœ€çµ‚ç‰ˆå¤§é™¸ç”¨èªè­˜åˆ¥ç³»çµ±...\n",
      "============================================================\n",
      "âŒ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„è³‡æ–™é›†\n",
      "ğŸ’¡ è«‹å…ˆåŸ·è¡Œå‰é¢çš„è³‡æ–™è¼‰å…¥ã€æ–‡æœ¬åˆ‡åˆ†æˆ–æ“´å¢æ­¥é©Ÿ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ æœ€çµ‚ç‰ˆå¤§é™¸ç”¨èªè­˜åˆ¥èˆ‡ç¯©é¸ç³»çµ± - ä½¿ç”¨ Ollama æ¨è«–ä¸¦å„²å­˜çµæœ\n",
    "print(\"ğŸš€ å•Ÿå‹•æœ€çµ‚ç‰ˆå¤§é™¸ç”¨èªè­˜åˆ¥ç³»çµ±...\")\n",
    "\n",
    "# å®šç¾©å¤§é™¸ç‰¹æœ‰è©å½™åº«\n",
    "mainland_terms = {\n",
    "    \"è¨ˆç®—æ©Ÿ\": [\"é›»è…¦\"], \"è»Ÿä»¶\": [\"è»Ÿé«”\"], \"ç¡¬ä»¶\": [\"ç¡¬é«”\"], \"ç¶²çµ¡\": [\"ç¶²è·¯\"], \n",
    "    \"æ•¸æ“š\": [\"è³‡æ–™\"], \"ç¨‹åº\": [\"ç¨‹å¼\"], \"ä¿¡æ¯\": [\"è³‡è¨Š\"], \"å‡ºç§Ÿè»Š\": [\"è¨ˆç¨‹è»Š\"],\n",
    "    \"å…¬äº¤è»Š\": [\"å…¬è»Š\"], \"åœ°éµ\": [\"æ·é‹\"], \"è³ªé‡\": [\"å“è³ª\"], \"æœå‹™å“¡\": [\"æœå‹™ç”Ÿ\"],\n",
    "    \"åœŸè±†\": [\"é¦¬éˆ´è–¯\"], \"è¥¿ç´…æŸ¿\": [\"ç•ªèŒ„\"], \"æå®š\": [\"å®Œæˆ\"], \"æŒº\": [\"å¾ˆ\"],\n",
    "    \"å’‹\": [\"æ€éº¼\"], \"å•¥\": [\"ä»€éº¼\"], \"å¾®ä¿¡\": [\"\"], \"æ”¯ä»˜å¯¶\": [\"\"], \"æ·˜å¯¶\": [\"\"]\n",
    "}\n",
    "\n",
    "# å¤§é™¸èªæ³•æ¨¡å¼\n",
    "mainland_patterns = [r\"æŒº.*çš„\", r\"è ».*çš„\", r\".*å¾—å¾ˆ\", r\"å’‹.*\", r\"å•¥.*\"]\n",
    "\n",
    "def analyze_features(text):\n",
    "    \"\"\"å¿«é€Ÿç‰¹å¾µåˆ†æ\"\"\"\n",
    "    mainland_count = sum(1 for term in mainland_terms if term in text)\n",
    "    pattern_count = sum(1 for pattern in mainland_patterns if re.search(pattern, text))\n",
    "    return {\n",
    "        \"mainland_terms\": [term for term in mainland_terms if term in text],\n",
    "        \"pattern_matches\": pattern_count,\n",
    "        \"authenticity_score\": mainland_count + pattern_count\n",
    "    }\n",
    "\n",
    "def mainland_score_ollama(text, model=\"gpt-oss:20b\"):\n",
    "    \"\"\"ä½¿ç”¨ Ollama è©•åˆ†å¤§é™¸ç”¨èªç‰¹å¾µ\"\"\"\n",
    "    prompt = f\"\"\"è©•ä¼°æ–‡æœ¬çš„å¤§é™¸ç”¨èªç‰¹å¾µï¼Œæ¯é …0æˆ–1åˆ†ï¼š\n",
    "\n",
    "æ–‡æœ¬ï¼š{text}\n",
    "\n",
    "è©•åˆ†æ¨™æº–ï¼š\n",
    "1. å¤§é™¸ç‰¹æœ‰è©å½™ï¼šè¨ˆç®—æ©Ÿã€è»Ÿä»¶ã€å‡ºç§Ÿè»Šã€åœ°éµç­‰\n",
    "2. å¤§é™¸èªæ³•ç¿’æ…£ï¼šæŒº...çš„ã€è »...çš„ã€å’‹æ¨£ç­‰  \n",
    "3. å¤§é™¸å£èªè¡¨é”ï¼šæå®šã€æ•´ã€å¼„ç­‰\n",
    "4. é¿å…ç¹é«”ç”¨èªï¼šä¸å«é›»è…¦ã€è»Ÿé«”ã€è³‡æ–™ç­‰\n",
    "5. æ•´é«”å¤§é™¸åŒ–ç¨‹åº¦ï¼šç¶œåˆè©•ä¼°\n",
    "\n",
    "è«‹æŒ‰æ ¼å¼å›ç­”ï¼š\n",
    "å¤§é™¸ç‰¹æœ‰è©å½™:0\n",
    "å¤§é™¸èªæ³•ç¿’æ…£:0\n",
    "å¤§é™¸å£èªè¡¨é”:0\n",
    "é¿å…ç¹é«”ç”¨èª:1\n",
    "æ•´é«”å¤§é™¸åŒ–ç¨‹åº¦:0\n",
    "ç¸½åˆ†:1\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options={'temperature': 0.1, 'max_tokens': 100}\n",
    "        )\n",
    "        \n",
    "        # è§£æå›æ‡‰\n",
    "        scores = {}\n",
    "        total = 0\n",
    "        categories = [\"å¤§é™¸ç‰¹æœ‰è©å½™\", \"å¤§é™¸èªæ³•ç¿’æ…£\", \"å¤§é™¸å£èªè¡¨é”\", \"é¿å…ç¹é«”ç”¨èª\", \"æ•´é«”å¤§é™¸åŒ–ç¨‹åº¦\"]\n",
    "        \n",
    "        for line in response['response'].split('\\n'):\n",
    "            for cat in categories:\n",
    "                if cat in line:\n",
    "                    match = re.search(r'[ï¼š:]\\s*([01])', line)\n",
    "                    if match:\n",
    "                        score = int(match.group(1))\n",
    "                        scores[cat] = score\n",
    "                        total += score\n",
    "        \n",
    "        if len(scores) == 5:\n",
    "            scores[\"ç¸½åˆ†\"] = total\n",
    "            return scores, response['response']\n",
    "        return None, response['response']\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def process_dataset(df, text_col='text', sample_size=100, threshold=3):\n",
    "    \"\"\"è™•ç†è³‡æ–™é›†é€²è¡Œå¤§é™¸ç”¨èªç¯©é¸\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“Š è™•ç†è³‡æ–™é›†ï¼š{len(df)} ç­†è¨˜éŒ„\")\n",
    "    print(f\"ğŸ“ æ–‡æœ¬æ¬„ä½ï¼š{text_col}\")\n",
    "    print(f\"ğŸ¯ æ¨£æœ¬å¤§å°ï¼š{sample_size}\")\n",
    "    print(f\"âš–ï¸ ç¯©é¸é–¾å€¼ï¼š{threshold}/5\")\n",
    "    \n",
    "    # å–æ¨£æœ¬\n",
    "    sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    texts = sample_df[text_col].tolist()\n",
    "    \n",
    "    # åŸ·è¡Œæ¨è«–\n",
    "    results = []\n",
    "    authentic_texts = []\n",
    "    generic_texts = []\n",
    "    \n",
    "    print(f\"\\nğŸ”„ é–‹å§‹ Ollama æ¨è«–...\")\n",
    "    \n",
    "    for i, text in enumerate(tqdm(texts, desc=\"æ¨è«–é€²åº¦\")):\n",
    "        # ç‰¹å¾µåˆ†æ\n",
    "        features = analyze_features(text)\n",
    "        \n",
    "        # Ollama è©•åˆ†\n",
    "        scores, response = mainland_score_ollama(text)\n",
    "        \n",
    "        result = {\n",
    "            'index': i,\n",
    "            'text': text,\n",
    "            'text_length': len(text),\n",
    "            'features': features,\n",
    "            'scores': scores,\n",
    "            'response': response,\n",
    "            'success': scores is not None\n",
    "        }\n",
    "        \n",
    "        # åˆ†é¡\n",
    "        if scores and scores.get(\"ç¸½åˆ†\", 0) >= threshold:\n",
    "            authentic_texts.append(result)\n",
    "            category = \"çœŸæ­£å¤§é™¸ç”¨èª\"\n",
    "        else:\n",
    "            generic_texts.append(result)\n",
    "            category = \"é€šç”¨ç°¡é«”ä¸­æ–‡\"\n",
    "        \n",
    "        result['category'] = category\n",
    "        results.append(result)\n",
    "        \n",
    "        # é¡¯ç¤ºé€²åº¦\n",
    "        if i % 20 == 0 or i < 3:\n",
    "            score_str = f\"{scores['ç¸½åˆ†']}/5\" if scores else \"å¤±æ•—\"\n",
    "            print(f\"  ç¬¬{i+1}ç­†: {score_str} - {category}\")\n",
    "        \n",
    "        time.sleep(0.2)  # æ§åˆ¶è«‹æ±‚é »ç‡\n",
    "    \n",
    "    return results, authentic_texts, generic_texts\n",
    "\n",
    "def save_results(results, authentic_texts, generic_texts):\n",
    "    \"\"\"å„²å­˜ç¯©é¸çµæœ - æ”¯æ´åˆ‡åˆ†è³‡æ–™æ ¼å¼\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. å®Œæ•´çµæœ\n",
    "    full_data = []\n",
    "    for r in results:\n",
    "        row = {\n",
    "            'text': r['text'],\n",
    "            'text_length': r['text_length'],\n",
    "            'category': r['category'],\n",
    "            'success': r['success'],\n",
    "            'authenticity_score': r['features']['authenticity_score'],\n",
    "            'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "        }\n",
    "        \n",
    "        # æ·»åŠ åˆ‡åˆ†ç›¸é—œæ¬„ä½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n",
    "        original_row = available_data.iloc[r['index']]\n",
    "        if 'source_type' in original_row:\n",
    "            row['source_type'] = original_row['source_type']\n",
    "        if 'source' in original_row:\n",
    "            row['source'] = original_row['source']\n",
    "        if 'fragment_length' in original_row:\n",
    "            row['fragment_length'] = original_row['fragment_length']\n",
    "        if 'augmentation_method' in original_row:\n",
    "            row['augmentation_method'] = original_row['augmentation_method']\n",
    "        \n",
    "        if r['scores']:\n",
    "            row.update({f'score_{k}': v for k, v in r['scores'].items()})\n",
    "        full_data.append(row)\n",
    "    \n",
    "    full_df = pd.DataFrame(full_data)\n",
    "    full_file = f\"mainland_filtering_complete_{timestamp}.csv\"\n",
    "    full_df.to_csv(full_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. é«˜è³ªé‡å¤§é™¸ç”¨èªæ•¸æ“šï¼ˆåˆ‡åˆ†æ ¼å¼ï¼‰\n",
    "    if authentic_texts:\n",
    "        authentic_data = []\n",
    "        for r in authentic_texts:\n",
    "            original_row = available_data.iloc[r['index']]\n",
    "            auth_row = {\n",
    "                'text': r['text'],\n",
    "                'total_score': r['scores']['ç¸½åˆ†'],\n",
    "                'mainland_terms': ','.join(r['features']['mainland_terms']),\n",
    "                'text_length': r['text_length']\n",
    "            }\n",
    "            \n",
    "            # ä¿ç•™åˆ‡åˆ†ç›¸é—œæ¬„ä½\n",
    "            if 'source_type' in original_row:\n",
    "                auth_row['source_type'] = original_row['source_type']\n",
    "            if 'source' in original_row:\n",
    "                auth_row['source'] = original_row['source']\n",
    "            if 'fragment_length' in original_row:\n",
    "                auth_row['fragment_length'] = original_row['fragment_length']\n",
    "            if 'augmentation_method' in original_row:\n",
    "                auth_row['augmentation_method'] = original_row['augmentation_method']\n",
    "            if 'original_idx' in original_row:\n",
    "                auth_row['original_idx'] = original_row['original_idx']\n",
    "            if 'fragment_index' in original_row:\n",
    "                auth_row['fragment_index'] = original_row['fragment_index']\n",
    "            \n",
    "            authentic_data.append(auth_row)\n",
    "        \n",
    "        auth_df = pd.DataFrame(authentic_data)\n",
    "        auth_csv = f\"authentic_mainland_texts_{timestamp}.csv\"\n",
    "        auth_json = f\"authentic_mainland_texts_{timestamp}.json\"\n",
    "        \n",
    "        auth_df.to_csv(auth_csv, index=False, encoding='utf-8-sig')\n",
    "        auth_df.to_json(auth_json, orient='records', force_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"ğŸ’¾ å„²å­˜å®Œæˆ:\")\n",
    "        print(f\"  ğŸ“„ å®Œæ•´çµæœ: {full_file}\")\n",
    "        print(f\"  âœ… é«˜è³ªé‡å¥å­ç‰‡æ®µæ•¸æ“š: {auth_csv}\")\n",
    "        print(f\"  ğŸ“‹ JSONæ ¼å¼: {auth_json}\")\n",
    "        \n",
    "        # é¡¯ç¤ºåˆ‡åˆ†è³‡æ–™çµ±è¨ˆ\n",
    "        if 'source' in auth_df.columns:\n",
    "            print(f\"\\nğŸ“Š é«˜è³ªé‡æ•¸æ“šä¾†æºåˆ†å¸ƒ:\")\n",
    "            print(auth_df['source'].value_counts())\n",
    "        \n",
    "        return full_df, auth_df\n",
    "    \n",
    "    return full_df, None\n",
    "\n",
    "# ä¸»è¦åŸ·è¡Œæµç¨‹\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æª¢æŸ¥å¯ç”¨è³‡æ–™é›† (å„ªå…ˆä½¿ç”¨æœ€çµ‚åˆ‡åˆ†å¥å­ç‰‡æ®µè³‡æ–™é›†)\n",
    "available_data = None\n",
    "text_column = 'text'\n",
    "\n",
    "if 'final_split_augmented_df' in locals() and final_split_augmented_df is not None:\n",
    "    available_data = final_split_augmented_df\n",
    "    source_name = \"æœ€çµ‚å¥å­ç‰‡æ®µæ“´å¢è³‡æ–™é›†\"\n",
    "elif 'split_dataset_df' in locals() and split_dataset_df is not None:\n",
    "    available_data = split_dataset_df\n",
    "    source_name = \"å¥å­ç‰‡æ®µè³‡æ–™é›†\"\n",
    "elif 'optimized_augmented_df' in locals() and optimized_augmented_df is not None:\n",
    "    available_data = optimized_augmented_df\n",
    "    source_name = \"å„ªåŒ–æ“´å¢è³‡æ–™é›†\"\n",
    "elif 'dataset_df' in locals() and dataset_df is not None:\n",
    "    available_data = dataset_df  \n",
    "    source_name = \"åŸå§‹è³‡æ–™é›†\"\n",
    "\n",
    "if available_data is not None:\n",
    "    print(f\"âœ… ä½¿ç”¨ {source_name}ï¼Œå…± {len(available_data)} ç­†è¨˜éŒ„\")\n",
    "    \n",
    "    # åŸ·è¡Œç¯©é¸ï¼ˆå¯èª¿æ•´åƒæ•¸ï¼‰\n",
    "    SAMPLE_SIZE = 50    # è™•ç†æ¨£æœ¬æ•¸é‡\n",
    "    THRESHOLD = 3       # ç¯©é¸é–¾å€¼\n",
    "    \n",
    "    print(f\"\\nğŸ¯ é–‹å§‹åŸ·è¡Œå¤§é™¸ç”¨èªç¯©é¸...\")\n",
    "    results, authentic_results, generic_results = process_dataset(\n",
    "        df=available_data,\n",
    "        text_col=text_column,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        threshold=THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # çµ±è¨ˆçµæœ\n",
    "    print(f\"\\nğŸ“Š ç¯©é¸çµæœçµ±è¨ˆ:\")\n",
    "    print(f\"  âœ… çœŸæ­£å¤§é™¸ç”¨èª: {len(authentic_results)} ç­†\")\n",
    "    print(f\"  ğŸ—‘ï¸ é€šç”¨ç°¡é«”ä¸­æ–‡: {len(generic_results)} ç­†\")\n",
    "    print(f\"  ğŸ“ˆ ç¯©é¸ç‡: {len(authentic_results)/len(results)*100:.1f}%\")\n",
    "    \n",
    "    # é¡¯ç¤ºç¯„ä¾‹\n",
    "    if authentic_results:\n",
    "        print(f\"\\nğŸ“ é«˜è³ªé‡å¤§é™¸ç”¨èªç¯„ä¾‹:\")\n",
    "        for i, r in enumerate(authentic_results[:3]):\n",
    "            preview = r['text'][:60] + \"...\" if len(r['text']) > 60 else r['text']\n",
    "            print(f\"  {i+1}. (å¾—åˆ†:{r['scores']['ç¸½åˆ†']}) {preview}\")\n",
    "    \n",
    "    # å„²å­˜çµæœ\n",
    "    print(f\"\\nğŸ’¾ å„²å­˜çµæœ...\")\n",
    "    full_df, auth_df = save_results(results, authentic_results, generic_results)\n",
    "    \n",
    "    # è¨­å®šå…¨åŸŸè®Šæ•¸\n",
    "    globals()['mainland_filtering_results'] = results\n",
    "    globals()['authentic_mainland_data'] = auth_df\n",
    "    \n",
    "    print(f\"\\nğŸ‰ å¤§é™¸ç”¨èªè­˜åˆ¥èˆ‡ç¯©é¸å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“‹ å¯ç”¨è®Šæ•¸: mainland_filtering_results, authentic_mainland_data\")\n",
    "    print(f\"ğŸ¯ æœ€çµ‚è¼¸å‡ºç‚ºå¥å­ç´šåˆ¥çš„ç‰‡æ®µè³‡æ–™ (10-50å­—)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„è³‡æ–™é›†\")\n",
    "    print(\"ğŸ’¡ è«‹å…ˆåŸ·è¡Œå‰é¢çš„è³‡æ–™è¼‰å…¥ã€æ–‡æœ¬åˆ‡åˆ†æˆ–æ“´å¢æ­¥é©Ÿ\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048a5fb",
   "metadata": {},
   "source": [
    "## ğŸ“‹ å¥å­ç´šåˆ¥åˆ‡åˆ†ç¤ºä¾‹\n",
    "\n",
    "ä»¥ä¸‹æ˜¯æ–°çš„å¥å­ç´šåˆ¥åˆ‡åˆ†åŠŸèƒ½çš„æ¼”ç¤ºã€‚åŸæ–‡æœ¬æœƒè¢«åˆ‡åˆ†æˆ10-50å­—çš„å¥å­ç‰‡æ®µï¼Œé©åˆé€²è¡Œæ›´ç²¾ç´°çš„å¤§é™¸ç”¨èªåˆ†æã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674bad99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” å¥å­ç´šåˆ¥åˆ‡åˆ†åŠŸèƒ½æ¼”ç¤º\n",
      "ğŸ“ åŸå§‹æ–‡æœ¬ (197å­—):\n",
      "å¦‚ä»Šï¼Œä¸ç®¡æ˜¯åœ¨å¤§å‹å•†è¶…è¿˜æ˜¯å··å­é‡Œçš„å°é“ºï¼Œéšå¤„å¯è§å„ç§æ‰«ç æ”¯ä»˜çš„èº«å½±ã€‚è€Œéšç€æ‰«ç æ”¯ä»˜çº¿ä¸‹å¸ƒå±€çš„é€æ¸å¹¿æ³›å’Œå·²åŸ¹è‚²æˆç†Ÿçš„ç”¨æˆ·æ¶ˆè´¹ä¹ æƒ¯ï¼Œä½¿å¾—è¿‘æœŸæ‰«ç æ”¯ä»˜é£æ½®é¢‡æœ‰æ¨ªæ‰«æ”¯ä»˜å¸‚åœºçš„åŠ¿å¤´ã€‚ä¸ç®¡ä½ æ˜¯å•†å®¶å’Œä¸ªäººéƒ½å¯ä»¥å¼€é€šæ— å¡æ”¯ä»˜ï¼å¯¹æ­¤ï¼Œä¼˜å£¹ä»˜æ–¹é¢è´Ÿè´£äººè¡¨ç¤ºï¼š\"éšç€è´¹ç‡æ”¹é©çš„æ­£å¼æ¨è¡Œï¼Œä½¿å¾—æ— å¡æ”¯ä»˜ã€æ‰«ç æ”¯ä»˜è¿›å…¥äº†å…¨å›½çˆ†å‘æœŸã€‚ç›®å‰ï¼Œæ‰«ç æ”¯ä»˜å¸‚åœºä¹Ÿæ¿€æˆ˜æ­£é…£ï¼Œè¶Šæ¥è¶Šå¤šçš„ä¼ä¸šã€ä»£ç†å•†ä»¥åŠä¸ªä½“éƒ½åœ¨è¿›å†›ç§»åŠ¨æ”¯ä»˜æˆ˜åœºæŠ¢æ»©åˆ†é£Ÿã€‚\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'split_text_to_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# åŸ·è¡Œå¥å­ç´šåˆ¥åˆ‡åˆ†\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m fragments = \u001b[43msplit_text_to_sentences\u001b[49m(example_text, min_length=\u001b[32m10\u001b[39m, max_length=\u001b[32m50\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ”ª åˆ‡åˆ†çµæœ (å…±\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(fragments)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33må€‹ç‰‡æ®µ):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'split_text_to_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# ğŸ“‹ å¥å­ç´šåˆ¥åˆ‡åˆ†ç¤ºä¾‹æ¼”ç¤º\n",
    "print(\"ğŸ” å¥å­ç´šåˆ¥åˆ‡åˆ†åŠŸèƒ½æ¼”ç¤º\")\n",
    "\n",
    "# ç¤ºä¾‹æ–‡æœ¬ï¼ˆæ‚¨æåˆ°çš„ä¾‹å­ï¼‰\n",
    "example_text = \"\"\"å¦‚ä»Šï¼Œä¸ç®¡æ˜¯åœ¨å¤§å‹å•†è¶…è¿˜æ˜¯å··å­é‡Œçš„å°é“ºï¼Œéšå¤„å¯è§å„ç§æ‰«ç æ”¯ä»˜çš„èº«å½±ã€‚è€Œéšç€æ‰«ç æ”¯ä»˜çº¿ä¸‹å¸ƒå±€çš„é€æ¸å¹¿æ³›å’Œå·²åŸ¹è‚²æˆç†Ÿçš„ç”¨æˆ·æ¶ˆè´¹ä¹ æƒ¯ï¼Œä½¿å¾—è¿‘æœŸæ‰«ç æ”¯ä»˜é£æ½®é¢‡æœ‰æ¨ªæ‰«æ”¯ä»˜å¸‚åœºçš„åŠ¿å¤´ã€‚ä¸ç®¡ä½ æ˜¯å•†å®¶å’Œä¸ªäººéƒ½å¯ä»¥å¼€é€šæ— å¡æ”¯ä»˜ï¼å¯¹æ­¤ï¼Œä¼˜å£¹ä»˜æ–¹é¢è´Ÿè´£äººè¡¨ç¤ºï¼š\"éšç€è´¹ç‡æ”¹é©çš„æ­£å¼æ¨è¡Œï¼Œä½¿å¾—æ— å¡æ”¯ä»˜ã€æ‰«ç æ”¯ä»˜è¿›å…¥äº†å…¨å›½çˆ†å‘æœŸã€‚ç›®å‰ï¼Œæ‰«ç æ”¯ä»˜å¸‚åœºä¹Ÿæ¿€æˆ˜æ­£é…£ï¼Œè¶Šæ¥è¶Šå¤šçš„ä¼ä¸šã€ä»£ç†å•†ä»¥åŠä¸ªä½“éƒ½åœ¨è¿›å†›ç§»åŠ¨æ”¯ä»˜æˆ˜åœºæŠ¢æ»©åˆ†é£Ÿã€‚\"\"\"\n",
    "\n",
    "print(f\"ğŸ“ åŸå§‹æ–‡æœ¬ ({len(example_text)}å­—):\")\n",
    "print(f\"{example_text}\")\n",
    "print()\n",
    "\n",
    "# åŸ·è¡Œå¥å­ç´šåˆ¥åˆ‡åˆ†\n",
    "fragments = split_text_to_sentences(example_text, min_length=10, max_length=50)\n",
    "\n",
    "print(f\"ğŸ”ª åˆ‡åˆ†çµæœ (å…±{len(fragments)}å€‹ç‰‡æ®µ):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, fragment in enumerate(fragments, 1):\n",
    "    print(f\"ç‰‡æ®µ {i} ({len(fragment)}å­—): {fragment}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nğŸ“Š åˆ‡åˆ†çµ±è¨ˆ:\")\n",
    "print(f\"  åŸå§‹æ–‡æœ¬é•·åº¦: {len(example_text)} å­—\")\n",
    "print(f\"  ç”Ÿæˆç‰‡æ®µæ•¸: {len(fragments)}\")\n",
    "print(f\"  å¹³å‡ç‰‡æ®µé•·åº¦: {sum(len(f) for f in fragments) / len(fragments):.1f} å­—\")\n",
    "print(f\"  æœ€çŸ­ç‰‡æ®µ: {min(len(f) for f in fragments)} å­—\")\n",
    "print(f\"  æœ€é•·ç‰‡æ®µ: {max(len(f) for f in fragments)} å­—\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ç¾åœ¨æ¯å€‹ç‰‡æ®µéƒ½å¯ä»¥ç¨ç«‹é€²è¡Œå¤§é™¸ç”¨èªåˆ†æï¼\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94844d",
   "metadata": {},
   "source": [
    "## ğŸ“ å¾å¤§æ–‡ä»¶æŒ‘é¸è³‡æ–™\n",
    "\n",
    "å¾æŒ‡å®šçš„å¤§æ–‡ä»¶ä¸­éš¨æ©ŸæŒ‘é¸250ç­†è³‡æ–™é€²è¡Œè©•åˆ†è™•ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef2f5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” å¾å¤§æ–‡ä»¶ä¸­æŒ‘é¸è³‡æ–™...\n",
      "ğŸ“‚ ç›®æ¨™æ–‡ä»¶: /Users/edwardhuang/Documents/GitHub/LLM_Aug/data/CLUECorpusSmall.txt\n",
      "ğŸ¯ ç›®æ¨™æ¨£æœ¬æ•¸: 250\n",
      "ğŸ“Š æ­£åœ¨è¨ˆç®—æ–‡ä»¶ç¸½è¡Œæ•¸...\n",
      "  å·²è¨ˆç®—: 100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 1,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 1,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 1,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 1,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 1,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 1,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 1,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 1,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 1,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 1,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 2,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 2,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 2,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 2,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 2,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 2,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 2,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 2,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 2,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 2,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 3,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 3,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 3,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 3,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 3,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 3,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 3,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 3,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 3,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 3,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 4,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 4,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 4,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 4,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 4,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 4,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 4,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 4,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 4,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 4,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 5,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 5,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 5,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 5,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 5,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 5,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 5,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 5,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 5,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 5,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 6,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 6,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 6,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 6,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 6,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 6,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 6,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 6,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 6,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 6,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 7,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 7,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 7,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 7,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 7,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 7,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 7,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 7,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 7,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 7,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 8,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 8,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 8,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 8,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 8,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 8,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 8,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 8,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 8,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 8,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 9,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 9,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 9,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 9,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 9,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 9,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 9,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 9,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 9,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 9,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 10,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 10,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 10,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 10,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 10,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 10,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 10,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 10,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 10,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 10,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 11,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 11,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 11,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 11,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 11,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 11,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 11,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 11,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 11,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 11,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 12,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 12,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 12,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 12,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 12,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 12,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 12,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 12,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 12,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 12,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 13,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 13,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 13,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 13,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 13,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 13,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 13,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 13,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 13,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 13,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 14,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 14,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 14,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 14,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 14,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 14,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 14,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 14,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 14,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 14,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 15,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 15,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 15,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 15,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 15,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 15,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 15,600,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 15,700,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 15,800,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 15,900,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 16,000,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 16,100,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 16,200,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 16,300,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 16,400,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 16,500,000 è¡Œ\n",
      "  å·²è¨ˆç®—: 16,600,000 è¡Œ\n",
      "ğŸ“ˆ æ–‡ä»¶ç¸½è¡Œæ•¸: 16,608,422\n",
      "ğŸ² éš¨æ©ŸæŒ‘é¸ 250 è¡Œ...\n",
      "âœ… å·²ç”Ÿæˆ 250 å€‹éš¨æ©Ÿè¡Œè™Ÿ\n",
      "ğŸ“– æ­£åœ¨è®€å–é¸ä¸­çš„è¡Œ...\n",
      "  å·²æŒ‘é¸: 50 ç­†\n",
      "  å·²æŒ‘é¸: 100 ç­†\n",
      "  å·²æŒ‘é¸: 150 ç­†\n",
      "  å·²æŒ‘é¸: 200 ç­†\n",
      "  å·²æŒ‘é¸: 250 ç­†\n",
      "\n",
      "âœ… è³‡æ–™æŒ‘é¸å®Œæˆï¼\n",
      "ğŸ“Š æˆåŠŸæŒ‘é¸: 250 ç­†è³‡æ–™\n",
      "âŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: name 'pd' is not defined\n",
      "ğŸ’¡ è«‹æª¢æŸ¥æ–‡ä»¶è·¯å¾‘æ˜¯å¦æ­£ç¢º\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“ å¾å¤§æ–‡ä»¶ä¸­æŒ‘é¸250ç­†è³‡æ–™é€²è¡Œè©•åˆ†\n",
    "print(\"ğŸ” å¾å¤§æ–‡ä»¶ä¸­æŒ‘é¸è³‡æ–™...\")\n",
    "\n",
    "import random\n",
    "\n",
    "# æ–‡ä»¶è·¯å¾‘\n",
    "file_path = \"/Users/edwardhuang/Documents/GitHub/LLM_Aug/data/CLUECorpusSmall.txt\"\n",
    "target_samples = 250\n",
    "\n",
    "print(f\"ğŸ“‚ ç›®æ¨™æ–‡ä»¶: {file_path}\")\n",
    "print(f\"ğŸ¯ ç›®æ¨™æ¨£æœ¬æ•¸: {target_samples}\")\n",
    "\n",
    "def count_lines_in_file(file_path):\n",
    "    \"\"\"è¨ˆç®—æ–‡ä»¶ç¸½è¡Œæ•¸\"\"\"\n",
    "    print(\"ğŸ“Š æ­£åœ¨è¨ˆç®—æ–‡ä»¶ç¸½è¡Œæ•¸...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            if count % 100000 == 0:\n",
    "                print(f\"  å·²è¨ˆç®—: {count:,} è¡Œ\")\n",
    "        return count\n",
    "\n",
    "def sample_lines_from_large_file(file_path, num_samples, total_lines=None):\n",
    "    \"\"\"å¾å¤§æ–‡ä»¶ä¸­éš¨æ©ŸæŒ‘é¸æŒ‡å®šæ•¸é‡çš„è¡Œ\"\"\"\n",
    "    \n",
    "    # å¦‚æœæ²’æœ‰æä¾›ç¸½è¡Œæ•¸ï¼Œå…ˆè¨ˆç®—\n",
    "    if total_lines is None:\n",
    "        total_lines = count_lines_in_file(file_path)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ æ–‡ä»¶ç¸½è¡Œæ•¸: {total_lines:,}\")\n",
    "    print(f\"ğŸ² éš¨æ©ŸæŒ‘é¸ {num_samples} è¡Œ...\")\n",
    "    \n",
    "    # ç”Ÿæˆéš¨æ©Ÿè¡Œè™Ÿ\n",
    "    if num_samples >= total_lines:\n",
    "        print(\"âš ï¸  ç›®æ¨™æ¨£æœ¬æ•¸å¤§æ–¼ç­‰æ–¼ç¸½è¡Œæ•¸ï¼Œå°‡è¿”å›æ‰€æœ‰è¡Œ\")\n",
    "        selected_lines = list(range(total_lines))\n",
    "    else:\n",
    "        selected_lines = sorted(random.sample(range(total_lines), num_samples))\n",
    "    \n",
    "    print(f\"âœ… å·²ç”Ÿæˆ {len(selected_lines)} å€‹éš¨æ©Ÿè¡Œè™Ÿ\")\n",
    "    \n",
    "    # è®€å–é¸ä¸­çš„è¡Œ\n",
    "    selected_data = []\n",
    "    current_line = 0\n",
    "    next_target_idx = 0\n",
    "    \n",
    "    print(\"ğŸ“– æ­£åœ¨è®€å–é¸ä¸­çš„è¡Œ...\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if next_target_idx < len(selected_lines) and current_line == selected_lines[next_target_idx]:\n",
    "                # æ¸…ç†æ–‡æœ¬\n",
    "                clean_text = line.strip()\n",
    "                if clean_text:  # åªä¿ç•™éç©ºè¡Œ\n",
    "                    selected_data.append({\n",
    "                        'line_number': current_line,\n",
    "                        'text': clean_text,\n",
    "                        'text_length': len(clean_text)\n",
    "                    })\n",
    "                next_target_idx += 1\n",
    "                \n",
    "                # é¡¯ç¤ºé€²åº¦\n",
    "                if len(selected_data) % 50 == 0:\n",
    "                    print(f\"  å·²æŒ‘é¸: {len(selected_data)} ç­†\")\n",
    "                \n",
    "                # å¦‚æœå·²ç¶“æ‰¾åˆ°æ‰€æœ‰ç›®æ¨™è¡Œï¼Œå¯ä»¥æå‰çµæŸ\n",
    "                if next_target_idx >= len(selected_lines):\n",
    "                    break\n",
    "            \n",
    "            current_line += 1\n",
    "    \n",
    "    return selected_data\n",
    "\n",
    "try:\n",
    "    # è¨­å®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§\n",
    "    random.seed(42)\n",
    "    \n",
    "    # æŒ‘é¸è³‡æ–™\n",
    "    selected_data = sample_lines_from_large_file(file_path, target_samples)\n",
    "    \n",
    "    print(f\"\\nâœ… è³‡æ–™æŒ‘é¸å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“Š æˆåŠŸæŒ‘é¸: {len(selected_data)} ç­†è³‡æ–™\")\n",
    "    \n",
    "    if selected_data:\n",
    "        # è½‰æ›ç‚ºDataFrame\n",
    "        selected_df = pd.DataFrame(selected_data)\n",
    "        \n",
    "        # é¡¯ç¤ºåŸºæœ¬çµ±è¨ˆ\n",
    "        print(f\"\\nğŸ“ˆ æ–‡æœ¬é•·åº¦çµ±è¨ˆ:\")\n",
    "        length_stats = selected_df['text_length'].describe()\n",
    "        for stat_name, value in length_stats.items():\n",
    "            print(f\"  {stat_name}: {value:.1f}\")\n",
    "        \n",
    "        # é¡¯ç¤ºå‰å¹¾ç­†ç¯„ä¾‹\n",
    "        print(f\"\\nğŸ“ å‰5ç­†è³‡æ–™ç¯„ä¾‹:\")\n",
    "        for i in range(min(5, len(selected_df))):\n",
    "            row = selected_df.iloc[i]\n",
    "            text = row['text']\n",
    "            preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            print(f\"ç¬¬{i+1}ç­† (è¡Œè™Ÿ:{row['line_number']}, {row['text_length']}å­—): {preview}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # å„²å­˜æŒ‘é¸çš„è³‡æ–™\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        save_dir = \"selected_data\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # å„²å­˜ç‚ºå¤šç¨®æ ¼å¼\n",
    "        base_filename = f\"{save_dir}/selected_250_samples_{timestamp}\"\n",
    "        \n",
    "        csv_filename = f\"{base_filename}.csv\"\n",
    "        json_filename = f\"{base_filename}.json\"\n",
    "        parquet_filename = f\"{base_filename}.parquet\"\n",
    "        \n",
    "        selected_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "        selected_df.to_json(json_filename, orient='records', force_ascii=False, indent=2)\n",
    "        selected_df.to_parquet(parquet_filename, index=False)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ è³‡æ–™å·²å„²å­˜:\")\n",
    "        print(f\"  ğŸ“„ CSV: {csv_filename}\")\n",
    "        print(f\"  ğŸ“‹ JSON: {json_filename}\")\n",
    "        print(f\"  ğŸ“¦ Parquet: {parquet_filename}\")\n",
    "        \n",
    "        # æª”æ¡ˆå¤§å°\n",
    "        for filename in [csv_filename, json_filename, parquet_filename]:\n",
    "            if os.path.exists(filename):\n",
    "                size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "                print(f\"    {os.path.basename(filename)}: {size_mb:.2f} MB\")\n",
    "        \n",
    "        # è¨­å®šå…¨åŸŸè®Šæ•¸\n",
    "        globals()['selected_large_file_df'] = selected_df\n",
    "        \n",
    "        print(f\"\\nğŸ‰ 250ç­†è³‡æ–™æŒ‘é¸å®Œæˆï¼\")\n",
    "        print(f\"ğŸ“‹ å¯ç”¨è®Šæ•¸: selected_large_file_df\")\n",
    "        print(f\"ğŸ¯ ç¾åœ¨å¯ä»¥ä½¿ç”¨é€™äº›è³‡æ–™é€²è¡Œå¥å­åˆ‡åˆ†å’Œå¤§é™¸ç”¨èªè©•åˆ†ï¼\")\n",
    "        \n",
    "        # é•·åº¦åˆ†å¸ƒåˆ†æ\n",
    "        print(f\"\\nğŸ“Š æ–‡æœ¬é•·åº¦åˆ†å¸ƒ:\")\n",
    "        length_ranges = [\n",
    "            (0, 50, \"çŸ­æ–‡æœ¬\"),\n",
    "            (50, 100, \"ä¸­çŸ­æ–‡æœ¬\"),\n",
    "            (100, 200, \"ä¸­ç­‰æ–‡æœ¬\"),\n",
    "            (200, 500, \"é•·æ–‡æœ¬\"),\n",
    "            (500, 1000, \"å¾ˆé•·æ–‡æœ¬\"),\n",
    "            (1000, float('inf'), \"è¶…é•·æ–‡æœ¬\")\n",
    "        ]\n",
    "        \n",
    "        for min_len, max_len, desc in length_ranges:\n",
    "            if max_len == float('inf'):\n",
    "                count = len(selected_df[selected_df['text_length'] >= min_len])\n",
    "            else:\n",
    "                count = len(selected_df[(selected_df['text_length'] >= min_len) & \n",
    "                                      (selected_df['text_length'] < max_len)])\n",
    "            percentage = count / len(selected_df) * 100\n",
    "            print(f\"  {desc} ({min_len}+å­—): {count} ç­† ({percentage:.1f}%)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âŒ æ²’æœ‰æˆåŠŸæŒ‘é¸åˆ°ä»»ä½•è³‡æ–™\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "    print(\"ğŸ’¡ è«‹æª¢æŸ¥æ–‡ä»¶è·¯å¾‘æ˜¯å¦æ­£ç¢º\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f041f7a",
   "metadata": {},
   "source": [
    "## ğŸ”„ è™•ç†æŒ‘é¸çš„250ç­†è³‡æ–™\n",
    "\n",
    "å°æŒ‘é¸çš„è³‡æ–™é€²è¡Œå¥å­ç´šåˆ¥åˆ‡åˆ†å’Œå¤§é™¸ç”¨èªè©•åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c88753f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ é–‹å§‹è™•ç†æŒ‘é¸çš„250ç­†è³‡æ–™...\n",
      "âŒ æ²’æœ‰æ‰¾åˆ°æŒ‘é¸çš„è³‡æ–™\n",
      "ğŸ’¡ è«‹å…ˆåŸ·è¡Œå‰é¢çš„è³‡æ–™æŒ‘é¸æ­¥é©Ÿ\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”„ è™•ç†æŒ‘é¸çš„250ç­†è³‡æ–™ - å¥å­åˆ‡åˆ† + å¤§é™¸ç”¨èªè©•åˆ†\n",
    "print(\"ğŸš€ é–‹å§‹è™•ç†æŒ‘é¸çš„250ç­†è³‡æ–™...\")\n",
    "\n",
    "if 'selected_large_file_df' in globals() and selected_large_file_df is not None:\n",
    "    print(f\"âœ… æ‰¾åˆ°æŒ‘é¸çš„è³‡æ–™: {len(selected_large_file_df)} ç­†\")\n",
    "    \n",
    "    # Step 1: å¥å­ç´šåˆ¥åˆ‡åˆ†\n",
    "    print(f\"\\nğŸ”ª æ­¥é©Ÿ1: åŸ·è¡Œå¥å­ç´šåˆ¥åˆ‡åˆ†...\")\n",
    "    \n",
    "    selected_split_df = process_text_splitting(\n",
    "        df=selected_large_file_df,\n",
    "        text_column='text',\n",
    "        min_length=10,\n",
    "        max_length=50\n",
    "    )\n",
    "    \n",
    "    if not selected_split_df.empty:\n",
    "        print(f\"\\nğŸ“Š åˆ‡åˆ†çµæœ:\")\n",
    "        print(f\"  åŸå§‹æ–‡æœ¬: {len(selected_large_file_df)} ç­†\")\n",
    "        print(f\"  åˆ‡åˆ†å¾Œå¥å­ç‰‡æ®µ: {len(selected_split_df)} ç­†\")\n",
    "        print(f\"  å¹³å‡æ¯æ–‡æœ¬ç”¢ç”Ÿ: {len(selected_split_df)/len(selected_large_file_df):.1f} å€‹ç‰‡æ®µ\")\n",
    "        \n",
    "        # é¡¯ç¤ºåˆ‡åˆ†ç¯„ä¾‹\n",
    "        print(f\"\\nğŸ“ åˆ‡åˆ†ç¯„ä¾‹ (å‰3å€‹åŸå§‹æ–‡æœ¬):\")\n",
    "        for orig_idx in selected_split_df['original_index'].unique()[:3]:\n",
    "            fragments = selected_split_df[selected_split_df['original_index'] == orig_idx]\n",
    "            original_text = selected_large_file_df.iloc[orig_idx]['text']\n",
    "            \n",
    "            print(f\"\\nåŸå§‹æ–‡æœ¬ #{orig_idx}: {original_text[:100]}{'...' if len(original_text) > 100 else ''}\")\n",
    "            print(f\"åˆ‡åˆ†ç‚º {len(fragments)} å€‹ç‰‡æ®µ:\")\n",
    "            \n",
    "            for i, (_, row) in enumerate(fragments.iterrows()):\n",
    "                print(f\"  ç‰‡æ®µ{i+1} ({row['fragment_length']}å­—): {row['text']}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # Step 2: å¤§é™¸ç”¨èªè©•åˆ†\n",
    "        print(f\"\\nğŸ¯ æ­¥é©Ÿ2: åŸ·è¡Œå¤§é™¸ç”¨èªè©•åˆ†...\")\n",
    "        \n",
    "        # è¨­å®šè©•åˆ†åƒæ•¸\n",
    "        SAMPLE_SIZE = min(100, len(selected_split_df))  # å¾åˆ‡åˆ†å¾Œçš„ç‰‡æ®µä¸­å–æ¨£è©•åˆ†\n",
    "        THRESHOLD = 3  # è©•åˆ†é–¾å€¼\n",
    "        \n",
    "        print(f\"ğŸ“Š è©•åˆ†åƒæ•¸:\")\n",
    "        print(f\"  è©•åˆ†æ¨£æœ¬æ•¸: {SAMPLE_SIZE}\")\n",
    "        print(f\"  ç¯©é¸é–¾å€¼: {THRESHOLD}/5\")\n",
    "        \n",
    "        # åŸ·è¡Œè©•åˆ†\n",
    "        results, authentic_results, generic_results = process_dataset(\n",
    "            df=selected_split_df,\n",
    "            text_col='text',\n",
    "            sample_size=SAMPLE_SIZE,\n",
    "            threshold=THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # çµ±è¨ˆçµæœ\n",
    "        print(f\"\\nğŸ“ˆ è©•åˆ†çµæœçµ±è¨ˆ:\")\n",
    "        print(f\"  ç¸½è©•åˆ†æ•¸: {len(results)}\")\n",
    "        print(f\"  âœ… çœŸæ­£å¤§é™¸ç”¨èª: {len(authentic_results)} ç­†\")\n",
    "        print(f\"  ğŸ—‘ï¸ é€šç”¨ç°¡é«”ä¸­æ–‡: {len(generic_results)} ç­†\")\n",
    "        if len(results) > 0:\n",
    "            print(f\"  ğŸ“Š å¤§é™¸ç”¨èªæ¯”ä¾‹: {len(authentic_results)/len(results)*100:.1f}%\")\n",
    "        \n",
    "        # é¡¯ç¤ºé«˜è³ªé‡ç¯„ä¾‹\n",
    "        if authentic_results:\n",
    "            print(f\"\\nğŸ† é«˜è³ªé‡å¤§é™¸ç”¨èªç‰‡æ®µç¯„ä¾‹:\")\n",
    "            for i, r in enumerate(authentic_results[:5]):\n",
    "                print(f\"ç¬¬{i+1}å (å¾—åˆ†:{r['scores']['ç¸½åˆ†']}/5): {r['text']}\")\n",
    "                if r['features']['mainland_terms']:\n",
    "                    print(f\"  å¤§é™¸ç‰¹æœ‰è©å½™: {', '.join(r['features']['mainland_terms'])}\")\n",
    "                print(\"-\" * 60)\n",
    "        \n",
    "        # å„²å­˜æœ€çµ‚çµæœ\n",
    "        print(f\"\\nğŸ’¾ å„²å­˜æœ€çµ‚çµæœ...\")\n",
    "        full_df, auth_df = save_results(results, authentic_results, generic_results)\n",
    "        \n",
    "        # å‰µå»ºå®Œæ•´è™•ç†çµæœæ‘˜è¦\n",
    "        summary_data = {\n",
    "            \"processing_summary\": {\n",
    "                \"original_samples\": len(selected_large_file_df),\n",
    "                \"split_fragments\": len(selected_split_df),\n",
    "                \"evaluated_fragments\": len(results),\n",
    "                \"high_quality_mainland\": len(authentic_results),\n",
    "                \"generic_chinese\": len(generic_results),\n",
    "                \"mainland_percentage\": len(authentic_results)/len(results)*100 if len(results) > 0 else 0\n",
    "            },\n",
    "            \"fragment_length_stats\": {\n",
    "                \"mean\": selected_split_df['fragment_length'].mean(),\n",
    "                \"min\": selected_split_df['fragment_length'].min(),\n",
    "                \"max\": selected_split_df['fragment_length'].max(),\n",
    "                \"median\": selected_split_df['fragment_length'].median()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # å„²å­˜è™•ç†æ‘˜è¦\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        summary_file = f\"processing_summary_{timestamp}.json\"\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"ğŸ“‹ è™•ç†æ‘˜è¦å·²å„²å­˜: {summary_file}\")\n",
    "        \n",
    "        # è¨­å®šå…¨åŸŸè®Šæ•¸\n",
    "        globals()['selected_split_df'] = selected_split_df\n",
    "        globals()['selected_mainland_results'] = results\n",
    "        globals()['selected_authentic_data'] = auth_df\n",
    "        \n",
    "        print(f\"\\nğŸ‰ 250ç­†è³‡æ–™å®Œæ•´è™•ç†æµç¨‹å®Œæˆï¼\")\n",
    "        print(f\"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:\")\n",
    "        print(f\"  åŸå§‹æŒ‘é¸è³‡æ–™: {len(selected_large_file_df)} ç­†\")\n",
    "        print(f\"  å¥å­åˆ‡åˆ†å¾Œ: {len(selected_split_df)} å€‹ç‰‡æ®µ\")\n",
    "        print(f\"  å¤§é™¸ç”¨èªè©•åˆ†: {len(results)} å€‹ç‰‡æ®µ\")\n",
    "        print(f\"  é«˜è³ªé‡å¤§é™¸ç”¨èª: {len(authentic_results)} å€‹ç‰‡æ®µ\")\n",
    "        print(f\"\\nğŸ“‹ å¯ç”¨è®Šæ•¸:\")\n",
    "        print(f\"  selected_large_file_df: åŸå§‹æŒ‘é¸çš„250ç­†è³‡æ–™\")\n",
    "        print(f\"  selected_split_df: å¥å­åˆ‡åˆ†å¾Œçš„ç‰‡æ®µè³‡æ–™\")\n",
    "        print(f\"  selected_mainland_results: å¤§é™¸ç”¨èªè©•åˆ†çµæœ\")\n",
    "        print(f\"  selected_authentic_data: é«˜è³ªé‡å¤§é™¸ç”¨èªç‰‡æ®µ\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ å¥å­åˆ‡åˆ†å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œå¾ŒçºŒè™•ç†\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ æ²’æœ‰æ‰¾åˆ°æŒ‘é¸çš„è³‡æ–™\")\n",
    "    print(\"ğŸ’¡ è«‹å…ˆåŸ·è¡Œå‰é¢çš„è³‡æ–™æŒ‘é¸æ­¥é©Ÿ\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bea9398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ ä¿®æ­£è³‡æ–™è¨­å®š...\n",
      "âœ… æ‰¾åˆ° selected_data: 250 ç­†\n",
      "ğŸ“Š DataFrame å»ºç«‹æˆåŠŸ: (250, 3)\n",
      "ğŸ“‹ æ¬„ä½: ['line_number', 'text', 'text_length']\n",
      "\n",
      "ğŸ“ å‰3ç­†è³‡æ–™:\n",
      "ç¬¬1ç­†: èº«ä¸ºä¸€ä¸ªå•èº«å°æ”»æ˜¯ä»€ä¹ˆä½“éªŒï¼Ÿé•œåƒé—®é¢˜èº«ä¸ºä¸€ä¸ªå•èº«å°å—æ˜¯ä¸€ç§æ€æ ·çš„ä½“éªŒï¼Ÿ1.çœ‹åˆ°é•¿å¾—ç˜¦ï¼Œè…¿å¾ˆå¥½çœ‹çš„ç”·ç”Ÿå°±ä¼šä¸€ç›´ç›¯ç€çœ‹ï¼Œç„¶åå°±æƒ³å»æ‘¸ä¸¤æŠŠã€‚2.å‘èª“çœ‹åˆ°å–œæ¬¢çš„ä¸€å®šè¦å»...\n",
      "ç¬¬2ç­†: æœ‹å‹æ›¿å¶åº†ç”Ÿ~~æ­£å·§åœ¨FOXTOWNåƒå®Œé¥­ï¼Œè·¯è¿‡è¿™å®¶åº—~~ä¾¿è¿›å»äº†~~çœ‹äº†ä¹‹åè§‰å¾—ç¯å¢ƒæ¯”æ—è¾¹çš„å¤œè‰²å¥½~~æ„Ÿè§‰ä¸é”™æ­£å¥½ææ´»åŠ¨ï¼Œè®¢äº†ä¸ªå¥—é¤å¯ä»¥åšä¸ªå¡åº§~~ä¸é”™ä¸é”™Â·...\n",
      "ç¬¬3ç­†: é”™è¯¯å¤ªå¤šå½±å“é˜…è¯»å¿ƒæƒ…\n",
      "\n",
      "ğŸ”„ é–‹å§‹åŸ·è¡Œå®Œæ•´è™•ç†æµç¨‹...\n",
      "\n",
      "ğŸ”ª æ­¥é©Ÿ1: åŸ·è¡Œå¥å­ç´šåˆ¥åˆ‡åˆ†...\n",
      "ğŸ“Š é–‹å§‹è™•ç†æ–‡æœ¬åˆ‡åˆ†...\n",
      "  åŸå§‹è³‡æ–™ç­†æ•¸: 250\n",
      "  æ–‡æœ¬æ¬„ä½: text\n",
      "  å¥å­ç‰‡æ®µé•·åº¦ç¯„åœ: 10-50 å­—\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "åˆ‡åˆ†é€²åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 263.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… æ–‡æœ¬åˆ‡åˆ†å®Œæˆï¼\n",
      "ğŸ“ˆ åˆ‡åˆ†çµ±è¨ˆ:\n",
      "  è™•ç†æ–‡æœ¬æ•¸: 240\n",
      "  ç”Ÿæˆå¥å­ç‰‡æ®µæ•¸: 2153\n",
      "  å¹³å‡æ¯æ–‡æœ¬ç‰‡æ®µæ•¸: 9.0\n",
      "\n",
      "ğŸ“ ç‰‡æ®µé•·åº¦çµ±è¨ˆ:\n",
      "  å¹³å‡é•·åº¦: 31.4 å­—\n",
      "  æœ€çŸ­ç‰‡æ®µ: 10 å­—\n",
      "  æœ€é•·ç‰‡æ®µ: 293 å­—\n",
      "  ä¸­ä½æ•¸é•·åº¦: 31.0 å­—\n",
      "\n",
      "ğŸ“Š ç‰‡æ®µé•·åº¦åˆ†å¸ƒ:\n",
      "  çŸ­ç‰‡æ®µ (10-20å­—): 514 å€‹ (23.9%)\n",
      "  ä¸­çŸ­ç‰‡æ®µ (20-30å­—): 509 å€‹ (23.6%)\n",
      "  ä¸­ç­‰ç‰‡æ®µ (30-40å­—): 516 å€‹ (24.0%)\n",
      "  é•·ç‰‡æ®µ (40-50å­—): 518 å€‹ (24.1%)\n",
      "  è¶…é•·ç‰‡æ®µ (50-100å­—): 84 å€‹ (3.9%)\n",
      "\n",
      "ğŸ“Š åˆ‡åˆ†çµæœ:\n",
      "  åŸå§‹æ–‡æœ¬: 250 ç­†\n",
      "  åˆ‡åˆ†å¾Œå¥å­ç‰‡æ®µ: 2153 ç­†\n",
      "  å¹³å‡æ¯æ–‡æœ¬ç”¢ç”Ÿ: 8.6 å€‹ç‰‡æ®µ\n",
      "\n",
      "ğŸ“ åˆ‡åˆ†ç¯„ä¾‹ (ç¬¬1å€‹åŸå§‹æ–‡æœ¬):\n",
      "åŸå§‹æ–‡æœ¬: èº«ä¸ºä¸€ä¸ªå•èº«å°æ”»æ˜¯ä»€ä¹ˆä½“éªŒï¼Ÿé•œåƒé—®é¢˜èº«ä¸ºä¸€ä¸ªå•èº«å°å—æ˜¯ä¸€ç§æ€æ ·çš„ä½“éªŒï¼Ÿ1.çœ‹åˆ°é•¿å¾—ç˜¦ï¼Œè…¿å¾ˆå¥½çœ‹çš„ç”·ç”Ÿå°±ä¼šä¸€ç›´ç›¯ç€çœ‹ï¼Œç„¶åå°±æƒ³å»æ‘¸ä¸¤æŠŠã€‚2.å‘èª“çœ‹åˆ°å–œæ¬¢çš„ä¸€å®šè¦å»è¦è”ç³»æ–¹å¼ï¼Œä½†æ˜¯ä»æ¥éƒ½æ²¡å‹‡æ°”ã€‚3.é•¿æœŸç”¨æ‰‹ã€‚çœŸçš„å·¨æƒ³è°ˆæ‹çˆ±äº†ï¼Œä½†æ˜¯ä¸ºå•¥åœ¨æˆéƒ½ä½œä¸ºä¸€ä¸ªæ”»è¿˜ä¼šå•èº«ï¼Œæˆ‘çœŸçš„æ²¡æƒ³é€šã€‚æˆ‘ä¹Ÿä¸çŸ®å•Šä¹Ÿæœ‰181ã€‚ä¹Ÿä¸èƒ–å•Šæˆ‘æ—¥ï¼Œæ€æ¥æƒ³å»æ˜¯å¤ªå®…äº†ï¼Œå¾ˆå¤šäººéƒ½ä»¥ä¸ºæˆ‘æ˜¯ç›´ç”·ã€‚å¯èƒ½çœŸçš„å¤ªç›´äº†ã€‚\n",
      "åˆ‡åˆ†ç‚º 7 å€‹ç‰‡æ®µ:\n",
      "  ç‰‡æ®µ1 (14å­—): èº«ä¸ºä¸€ä¸ªå•èº«å°æ”»æ˜¯ä»€ä¹ˆä½“éªŒï¼Ÿ\n",
      "  ç‰‡æ®µ2 (21å­—): é•œåƒé—®é¢˜èº«ä¸ºä¸€ä¸ªå•èº«å°å—æ˜¯ä¸€ç§æ€æ ·çš„ä½“éªŒï¼Ÿ\n",
      "  ç‰‡æ®µ3 (32å­—): 1.çœ‹åˆ°é•¿å¾—ç˜¦ï¼Œè…¿å¾ˆå¥½çœ‹çš„ç”·ç”Ÿå°±ä¼šä¸€ç›´ç›¯ç€çœ‹ï¼Œç„¶åå°±æƒ³å»æ‘¸ä¸¤æŠŠã€‚\n",
      "  ç‰‡æ®µ4 (28å­—): 2.å‘èª“çœ‹åˆ°å–œæ¬¢çš„ä¸€å®šè¦å»è¦è”ç³»æ–¹å¼ï¼Œä½†æ˜¯ä»æ¥éƒ½æ²¡å‹‡æ°”ã€‚\n",
      "  ç‰‡æ®µ5 (33å­—): çœŸçš„å·¨æƒ³è°ˆæ‹çˆ±äº†ï¼Œä½†æ˜¯ä¸ºå•¥åœ¨æˆéƒ½ä½œä¸ºä¸€ä¸ªæ”»è¿˜ä¼šå•èº«ï¼Œæˆ‘çœŸçš„æ²¡æƒ³é€šã€‚\n",
      "  ç‰‡æ®µ6 (11å­—): æˆ‘ä¹Ÿä¸çŸ®å•Šä¹Ÿæœ‰181ã€‚\n",
      "  ç‰‡æ®µ7 (27å­—): ä¹Ÿä¸èƒ–å•Šæˆ‘æ—¥ï¼Œæ€æ¥æƒ³å»æ˜¯å¤ªå®…äº†ï¼Œå¾ˆå¤šäººéƒ½ä»¥ä¸ºæˆ‘æ˜¯ç›´ç”·ã€‚\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ¯ æ­¥é©Ÿ2: åŸ·è¡Œå¤§é™¸ç”¨èªè©•åˆ†...\n",
      "ğŸ“Š è©•åˆ†åƒæ•¸:\n",
      "  è©•åˆ†æ¨£æœ¬æ•¸: 30\n",
      "  ç¯©é¸é–¾å€¼: 3/5\n",
      "ğŸ“Š è™•ç†è³‡æ–™é›†ï¼š2153 ç­†è¨˜éŒ„\n",
      "ğŸ“ æ–‡æœ¬æ¬„ä½ï¼štext\n",
      "ğŸ¯ æ¨£æœ¬å¤§å°ï¼š30\n",
      "âš–ï¸ ç¯©é¸é–¾å€¼ï¼š3/5\n",
      "\n",
      "ğŸ”„ é–‹å§‹ Ollama æ¨è«–...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ¨è«–é€²åº¦:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ç¬¬1ç­†: 1/5 - é€šç”¨ç°¡é«”ä¸­æ–‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ¨è«–é€²åº¦:   3%|â–         | 1/30 [00:15<07:25, 15.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ç¬¬2ç­†: 3/5 - çœŸæ­£å¤§é™¸ç”¨èª\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ¨è«–é€²åº¦:   7%|â–‹         | 2/30 [00:26<06:07, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ç¬¬3ç­†: 1/5 - é€šç”¨ç°¡é«”ä¸­æ–‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ¨è«–é€²åº¦:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [03:25<01:36,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ç¬¬21ç­†: 1/5 - é€šç”¨ç°¡é«”ä¸­æ–‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ¨è«–é€²åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [04:54<00:00,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ è©•åˆ†çµæœçµ±è¨ˆ:\n",
      "  ç¸½è©•åˆ†æ•¸: 30\n",
      "  âœ… çœŸæ­£å¤§é™¸ç”¨èª: 1 ç­†\n",
      "  ğŸ—‘ï¸ é€šç”¨ç°¡é«”ä¸­æ–‡: 29 ç­†\n",
      "  ğŸ“Š å¤§é™¸ç”¨èªæ¯”ä¾‹: 3.3%\n",
      "\n",
      "ğŸ† é«˜è³ªé‡å¤§é™¸ç”¨èªç‰‡æ®µç¯„ä¾‹:\n",
      "ç¬¬1å (å¾—åˆ†:3/5): ä½åœ¨ä»–ä»¬å®¶äºæ˜¯å“å°äº†ä¸€ä¸‹ä»–ä»¬çš„è‡ªåŠ©æ™šé¤ï¼Œå¤©åº­çš„åˆ›æ„æ˜¯ä¸é”™ï¼Œå’‹ä¸€çœ‹ä¹Ÿå¾ˆæ¼‚äº®ï¼Œä½†ä¸å¯å¦è®¤æœ‰ç‚¹æ—§äº†ã€‚\n",
      "  å¤§é™¸ç‰¹æœ‰è©å½™: å’‹\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ’¾ å„²å­˜è™•ç†çµæœ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# å„²å­˜çµæœ\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ’¾ å„²å­˜è™•ç†çµæœ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m full_df, auth_df = \u001b[43msave_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthentic_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneric_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# è¨­å®šå…¨åŸŸè®Šæ•¸\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[\u001b[33m'\u001b[39m\u001b[33mselected_large_file_df\u001b[39m\u001b[33m'\u001b[39m] = selected_large_file_df\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 149\u001b[39m, in \u001b[36msave_results\u001b[39m\u001b[34m(results, authentic_texts, generic_texts)\u001b[39m\n\u001b[32m    139\u001b[39m row = {\n\u001b[32m    140\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m: r[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    141\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtext_length\u001b[39m\u001b[33m'\u001b[39m: r[\u001b[33m'\u001b[39m\u001b[33mtext_length\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    145\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmainland_terms\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join(r[\u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmainland_terms\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    146\u001b[39m }\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# æ·»åŠ åˆ‡åˆ†ç›¸é—œæ¬„ä½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m original_row = \u001b[43mavailable_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m[r[\u001b[33m'\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33msource_type\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m original_row:\n\u001b[32m    151\u001b[39m     row[\u001b[33m'\u001b[39m\u001b[33msource_type\u001b[39m\u001b[33m'\u001b[39m] = original_row[\u001b[33m'\u001b[39m\u001b[33msource_type\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ ä¿®æ­£è³‡æ–™è¨­å®šä¸¦åŸ·è¡Œå®Œæ•´è™•ç†æµç¨‹\n",
    "print(\"ğŸ”§ ä¿®æ­£è³‡æ–™è¨­å®š...\")\n",
    "\n",
    "# æª¢æŸ¥ä¸¦è¨­å®šDataFrame\n",
    "if 'selected_data' in globals() and selected_data is not None:\n",
    "    print(f\"âœ… æ‰¾åˆ° selected_data: {len(selected_data)} ç­†\")\n",
    "    \n",
    "    # è½‰æ›ç‚ºDataFrame\n",
    "    selected_large_file_df = pd.DataFrame(selected_data)\n",
    "    \n",
    "    print(f\"ğŸ“Š DataFrame å»ºç«‹æˆåŠŸ: {selected_large_file_df.shape}\")\n",
    "    print(f\"ğŸ“‹ æ¬„ä½: {list(selected_large_file_df.columns)}\")\n",
    "    \n",
    "    # é¡¯ç¤ºå‰å¹¾ç­†ç¯„ä¾‹\n",
    "    print(f\"\\nğŸ“ å‰3ç­†è³‡æ–™:\")\n",
    "    for i in range(min(3, len(selected_large_file_df))):\n",
    "        row = selected_large_file_df.iloc[i]\n",
    "        text = row['text']\n",
    "        preview = text[:80] + \"...\" if len(text) > 80 else text\n",
    "        print(f\"ç¬¬{i+1}ç­†: {preview}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”„ é–‹å§‹åŸ·è¡Œå®Œæ•´è™•ç†æµç¨‹...\")\n",
    "    \n",
    "    # Step 1: å¥å­ç´šåˆ¥åˆ‡åˆ†\n",
    "    print(f\"\\nğŸ”ª æ­¥é©Ÿ1: åŸ·è¡Œå¥å­ç´šåˆ¥åˆ‡åˆ†...\")\n",
    "    \n",
    "    selected_split_df = process_text_splitting(\n",
    "        df=selected_large_file_df,\n",
    "        text_column='text',\n",
    "        min_length=10,\n",
    "        max_length=50\n",
    "    )\n",
    "    \n",
    "    if not selected_split_df.empty:\n",
    "        print(f\"\\nğŸ“Š åˆ‡åˆ†çµæœ:\")\n",
    "        print(f\"  åŸå§‹æ–‡æœ¬: {len(selected_large_file_df)} ç­†\")\n",
    "        print(f\"  åˆ‡åˆ†å¾Œå¥å­ç‰‡æ®µ: {len(selected_split_df)} ç­†\")\n",
    "        print(f\"  å¹³å‡æ¯æ–‡æœ¬ç”¢ç”Ÿ: {len(selected_split_df)/len(selected_large_file_df):.1f} å€‹ç‰‡æ®µ\")\n",
    "        \n",
    "        # é¡¯ç¤ºåˆ‡åˆ†ç¯„ä¾‹\n",
    "        print(f\"\\nğŸ“ åˆ‡åˆ†ç¯„ä¾‹ (ç¬¬1å€‹åŸå§‹æ–‡æœ¬):\")\n",
    "        first_orig_idx = selected_split_df['original_index'].iloc[0]\n",
    "        fragments = selected_split_df[selected_split_df['original_index'] == first_orig_idx]\n",
    "        original_text = selected_large_file_df.iloc[first_orig_idx]['text']\n",
    "        \n",
    "        print(f\"åŸå§‹æ–‡æœ¬: {original_text}\")\n",
    "        print(f\"åˆ‡åˆ†ç‚º {len(fragments)} å€‹ç‰‡æ®µ:\")\n",
    "        \n",
    "        for i, (_, row) in enumerate(fragments.iterrows()):\n",
    "            print(f\"  ç‰‡æ®µ{i+1} ({row['fragment_length']}å­—): {row['text']}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Step 2: å¤§é™¸ç”¨èªè©•åˆ† (å–è¼ƒå°çš„æ¨£æœ¬æ•¸é€²è¡Œæ¼”ç¤º)\n",
    "        print(f\"\\nğŸ¯ æ­¥é©Ÿ2: åŸ·è¡Œå¤§é™¸ç”¨èªè©•åˆ†...\")\n",
    "        \n",
    "        # è¨­å®šè©•åˆ†åƒæ•¸ - æ¸›å°‘æ¨£æœ¬æ•¸ä»¥åŠ å¿«è™•ç†\n",
    "        SAMPLE_SIZE = min(30, len(selected_split_df))  # å–30å€‹ç‰‡æ®µé€²è¡Œè©•åˆ†æ¼”ç¤º\n",
    "        THRESHOLD = 3  # è©•åˆ†é–¾å€¼\n",
    "        \n",
    "        print(f\"ğŸ“Š è©•åˆ†åƒæ•¸:\")\n",
    "        print(f\"  è©•åˆ†æ¨£æœ¬æ•¸: {SAMPLE_SIZE}\")\n",
    "        print(f\"  ç¯©é¸é–¾å€¼: {THRESHOLD}/5\")\n",
    "        \n",
    "        # åŸ·è¡Œè©•åˆ†\n",
    "        results, authentic_results, generic_results = process_dataset(\n",
    "            df=selected_split_df,\n",
    "            text_col='text',\n",
    "            sample_size=SAMPLE_SIZE,\n",
    "            threshold=THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # çµ±è¨ˆçµæœ\n",
    "        print(f\"\\nğŸ“ˆ è©•åˆ†çµæœçµ±è¨ˆ:\")\n",
    "        print(f\"  ç¸½è©•åˆ†æ•¸: {len(results)}\")\n",
    "        print(f\"  âœ… çœŸæ­£å¤§é™¸ç”¨èª: {len(authentic_results)} ç­†\")\n",
    "        print(f\"  ğŸ—‘ï¸ é€šç”¨ç°¡é«”ä¸­æ–‡: {len(generic_results)} ç­†\")\n",
    "        if len(results) > 0:\n",
    "            print(f\"  ğŸ“Š å¤§é™¸ç”¨èªæ¯”ä¾‹: {len(authentic_results)/len(results)*100:.1f}%\")\n",
    "        \n",
    "        # é¡¯ç¤ºé«˜è³ªé‡ç¯„ä¾‹\n",
    "        if authentic_results:\n",
    "            print(f\"\\nğŸ† é«˜è³ªé‡å¤§é™¸ç”¨èªç‰‡æ®µç¯„ä¾‹:\")\n",
    "            for i, r in enumerate(authentic_results[:3]):\n",
    "                print(f\"ç¬¬{i+1}å (å¾—åˆ†:{r['scores']['ç¸½åˆ†']}/5): {r['text']}\")\n",
    "                if r['features']['mainland_terms']:\n",
    "                    print(f\"  å¤§é™¸ç‰¹æœ‰è©å½™: {', '.join(r['features']['mainland_terms'])}\")\n",
    "                print(\"-\" * 60)\n",
    "        \n",
    "        # å„²å­˜çµæœ\n",
    "        print(f\"\\nğŸ’¾ å„²å­˜è™•ç†çµæœ...\")\n",
    "        full_df, auth_df = save_results(results, authentic_results, generic_results)\n",
    "        \n",
    "        # è¨­å®šå…¨åŸŸè®Šæ•¸\n",
    "        globals()['selected_large_file_df'] = selected_large_file_df\n",
    "        globals()['selected_split_df'] = selected_split_df\n",
    "        globals()['selected_mainland_results'] = results\n",
    "        globals()['selected_authentic_data'] = auth_df\n",
    "        \n",
    "        print(f\"\\nğŸ‰ 250ç­†è³‡æ–™å®Œæ•´è™•ç†æµç¨‹å®Œæˆï¼\")\n",
    "        print(f\"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:\")\n",
    "        print(f\"  åŸå§‹æŒ‘é¸è³‡æ–™: {len(selected_large_file_df)} ç­†\")\n",
    "        print(f\"  å¥å­åˆ‡åˆ†å¾Œ: {len(selected_split_df)} å€‹ç‰‡æ®µ\")\n",
    "        print(f\"  å¤§é™¸ç”¨èªè©•åˆ†: {len(results)} å€‹ç‰‡æ®µ\")\n",
    "        print(f\"  é«˜è³ªé‡å¤§é™¸ç”¨èª: {len(authentic_results)} å€‹ç‰‡æ®µ\")\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ å¯ç”¨è®Šæ•¸:\")\n",
    "        print(f\"  selected_large_file_df: åŸå§‹æŒ‘é¸çš„250ç­†è³‡æ–™\")\n",
    "        print(f\"  selected_split_df: å¥å­åˆ‡åˆ†å¾Œçš„ç‰‡æ®µè³‡æ–™\")\n",
    "        print(f\"  selected_mainland_results: å¤§é™¸ç”¨èªè©•åˆ†çµæœ\")\n",
    "        if auth_df is not None:\n",
    "            print(f\"  selected_authentic_data: é«˜è³ªé‡å¤§é™¸ç”¨èªç‰‡æ®µ ({len(auth_df)} ç­†)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ å¥å­åˆ‡åˆ†å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œå¾ŒçºŒè™•ç†\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ æ²’æœ‰æ‰¾åˆ° selected_data\")\n",
    "    print(\"ğŸ’¡ è«‹å…ˆåŸ·è¡Œå‰é¢çš„è³‡æ–™æŒ‘é¸æ­¥é©Ÿ\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ece484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ ç°¡åŒ–ç‰ˆçµæœç¸½çµå’Œå„²å­˜\n",
    "print(\"ğŸ¯ æ•´ç†å’Œå„²å­˜æœ€çµ‚çµæœ...\")\n",
    "\n",
    "# ç¢ºä¿æœ‰è©•åˆ†çµæœ\n",
    "if 'results' in locals() and 'authentic_results' in locals():\n",
    "    print(f\"âœ… æ‰¾åˆ°è©•åˆ†çµæœ\")\n",
    "    \n",
    "    # å‰µå»ºå®Œæ•´çµæœDataFrame\n",
    "    full_results_data = []\n",
    "    for r in results:\n",
    "        row = {\n",
    "            'text': r['text'],\n",
    "            'text_length': r['text_length'],\n",
    "            'category': r['category'],\n",
    "            'success': r['success'],\n",
    "            'authenticity_score': r['features']['authenticity_score'],\n",
    "            'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "        }\n",
    "        \n",
    "        if r['scores']:\n",
    "            row.update({f'score_{k}': v for k, v in r['scores'].items()})\n",
    "        full_results_data.append(row)\n",
    "    \n",
    "    full_results_df = pd.DataFrame(full_results_data)\n",
    "    \n",
    "    # å‰µå»ºé«˜è³ªé‡å¤§é™¸ç”¨èªDataFrame\n",
    "    if authentic_results:\n",
    "        authentic_data = []\n",
    "        for r in authentic_results:\n",
    "            auth_row = {\n",
    "                'text': r['text'],\n",
    "                'total_score': r['scores']['ç¸½åˆ†'],\n",
    "                'mainland_terms': ','.join(r['features']['mainland_terms']),\n",
    "                'text_length': r['text_length'],\n",
    "                'category': r['category']\n",
    "            }\n",
    "            \n",
    "            # æ·»åŠ è©³ç´°è©•åˆ†\n",
    "            if r['scores']:\n",
    "                for key, value in r['scores'].items():\n",
    "                    if key != 'ç¸½åˆ†':\n",
    "                        auth_row[f'score_{key}'] = value\n",
    "            \n",
    "            authentic_data.append(auth_row)\n",
    "        \n",
    "        authentic_df = pd.DataFrame(authentic_data)\n",
    "    else:\n",
    "        authentic_df = pd.DataFrame()\n",
    "    \n",
    "    # å„²å­˜çµæœ\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = \"evaluation_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # å„²å­˜å®Œæ•´çµæœ\n",
    "    full_results_file = f\"{results_dir}/full_evaluation_results_{timestamp}.csv\"\n",
    "    full_results_df.to_csv(full_results_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # å„²å­˜é«˜è³ªé‡çµæœ\n",
    "    if not authentic_df.empty:\n",
    "        authentic_file = f\"{results_dir}/high_quality_mainland_texts_{timestamp}.csv\"\n",
    "        authentic_df.to_csv(authentic_file, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        authentic_json = f\"{results_dir}/high_quality_mainland_texts_{timestamp}.json\"\n",
    "        authentic_df.to_json(authentic_json, orient='records', force_ascii=False, indent=2)\n",
    "    \n",
    "    # å‰µå»ºè™•ç†æ‘˜è¦\n",
    "    summary = {\n",
    "        \"processing_timestamp\": timestamp,\n",
    "        \"source_file\": \"/Users/edwardhuang/Documents/GitHub/LLM_Aug/data/CLUECorpusSmall.txt\",\n",
    "        \"original_samples\": len(selected_large_file_df),\n",
    "        \"split_fragments\": len(selected_split_df),\n",
    "        \"evaluated_fragments\": len(results),\n",
    "        \"high_quality_mainland\": len(authentic_results),\n",
    "        \"generic_chinese\": len(generic_results),\n",
    "        \"mainland_percentage\": len(authentic_results)/len(results)*100 if len(results) > 0 else 0,\n",
    "        \"fragment_length_stats\": {\n",
    "            \"mean\": float(selected_split_df['fragment_length'].mean()),\n",
    "            \"min\": int(selected_split_df['fragment_length'].min()),\n",
    "            \"max\": int(selected_split_df['fragment_length'].max()),\n",
    "            \"median\": float(selected_split_df['fragment_length'].median())\n",
    "        },\n",
    "        \"evaluation_sample_size\": len(results),\n",
    "        \"threshold_used\": 3\n",
    "    }\n",
    "    \n",
    "    # å„²å­˜æ‘˜è¦\n",
    "    summary_file = f\"{results_dir}/processing_summary_{timestamp}.json\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ çµæœå·²æˆåŠŸå„²å­˜:\")\n",
    "    print(f\"  ğŸ“Š å®Œæ•´è©•åˆ†çµæœ: {full_results_file}\")\n",
    "    if not authentic_df.empty:\n",
    "        print(f\"  ğŸ† é«˜è³ªé‡å¤§é™¸ç”¨èª: {authentic_file}\")\n",
    "        print(f\"  ğŸ“‹ JSONæ ¼å¼: {authentic_json}\")\n",
    "    print(f\"  ğŸ“‹ è™•ç†æ‘˜è¦: {summary_file}\")\n",
    "    \n",
    "    # é¡¯ç¤ºæª”æ¡ˆå¤§å°\n",
    "    print(f\"\\nğŸ“ æª”æ¡ˆå¤§å°:\")\n",
    "    for filename in [full_results_file, authentic_file if not authentic_df.empty else None, summary_file]:\n",
    "        if filename and os.path.exists(filename):\n",
    "            size_kb = os.path.getsize(filename) / 1024\n",
    "            print(f\"  {os.path.basename(filename)}: {size_kb:.2f} KB\")\n",
    "    \n",
    "    # è¨­å®šå…¨åŸŸè®Šæ•¸\n",
    "    globals()['selected_large_file_df'] = selected_large_file_df\n",
    "    globals()['selected_split_df'] = selected_split_df\n",
    "    globals()['selected_mainland_results'] = results\n",
    "    globals()['selected_authentic_data'] = authentic_df\n",
    "    globals()['evaluation_summary'] = summary\n",
    "    \n",
    "    print(f\"\\nğŸ‰ 250ç­†è³‡æ–™è©•åˆ†è™•ç†å®Œæˆï¼\")\n",
    "    print(f\"\\nğŸ“Š æœ€çµ‚çµ±è¨ˆæ‘˜è¦:\")\n",
    "    print(f\"  ğŸ“‚ åŸå§‹å¤§æ–‡ä»¶: CLUECorpusSmall.txt\")\n",
    "    print(f\"  ğŸ² éš¨æ©ŸæŒ‘é¸: {summary['original_samples']} ç­†è³‡æ–™\")\n",
    "    print(f\"  ğŸ”ª å¥å­åˆ‡åˆ†: {summary['split_fragments']} å€‹ç‰‡æ®µ\")\n",
    "    print(f\"  ğŸ¯ è©•åˆ†è™•ç†: {summary['evaluated_fragments']} å€‹ç‰‡æ®µ\")\n",
    "    print(f\"  âœ… é«˜è³ªé‡å¤§é™¸ç”¨èª: {summary['high_quality_mainland']} å€‹ç‰‡æ®µ\")\n",
    "    print(f\"  ğŸ“ˆ å¤§é™¸ç”¨èªæ¯”ä¾‹: {summary['mainland_percentage']:.1f}%\")\n",
    "    \n",
    "    if authentic_results:\n",
    "        print(f\"\\nğŸ† ç™¼ç¾çš„é«˜è³ªé‡å¤§é™¸ç”¨èªç‰‡æ®µ:\")\n",
    "        for i, r in enumerate(authentic_results):\n",
    "            print(f\"  {i+1}. (å¾—åˆ† {r['scores']['ç¸½åˆ†']}/5): {r['text']}\")\n",
    "            if r['features']['mainland_terms']:\n",
    "                print(f\"     å¤§é™¸ç‰¹æœ‰è©å½™: {', '.join(r['features']['mainland_terms'])}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ å¯ç”¨è®Šæ•¸:\")\n",
    "    print(f\"  selected_large_file_df: åŸå§‹250ç­†è³‡æ–™\")\n",
    "    print(f\"  selected_split_df: åˆ‡åˆ†å¾Œçš„å¥å­ç‰‡æ®µ\")\n",
    "    print(f\"  selected_mainland_results: è©•åˆ†çµæœ\")\n",
    "    print(f\"  selected_authentic_data: é«˜è³ªé‡å¤§é™¸ç”¨èª\")\n",
    "    print(f\"  evaluation_summary: è™•ç†æ‘˜è¦\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ æ²’æœ‰æ‰¾åˆ°è©•åˆ†çµæœ\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
