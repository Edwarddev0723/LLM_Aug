{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008ebe94",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d9d4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "import os\n",
    "import ollama\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712a89a",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c0fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 資料載入選項 ===\n",
      "使用本地檔案: True\n",
      "使用串流模式: False\n",
      "下載完整資料集: False\n",
      "\n",
      "📁 從本地檔案讀取資料...\n",
      "找到的檔案:\n",
      "  CSV 檔案: 1 個\n",
      "  JSON 檔案: 1 個\n",
      "  Parquet 檔案: 1 個\n",
      "\n",
      "📊 讀取最新的 Parquet 檔案: saved_datasets/clue_corpus_small_20250901_085816.parquet\n",
      "\n",
      "✅ 資料載入成功！\n",
      "📊 資料形狀: (1000, 1)\n",
      "📋 欄位名稱: ['text']\n",
      "\n",
      "📈 文本長度統計:\n",
      "count     1000.000000\n",
      "mean       300.899000\n",
      "std        785.763282\n",
      "min          5.000000\n",
      "25%         38.000000\n",
      "50%        105.500000\n",
      "75%        274.000000\n",
      "max      17020.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "📝 前 3 筆資料範例:\n",
      "範例 1 (132 字符): 130真是佩服这家店开这么久。尽管门面已经小了一圈，但还是开着不容易啊。我们不容易，老板也不容易。自助餐，你可以吃得比平时多，但决不能浪费。想吃回20元，那是不可能的，所以还是不要去了。菜真的很一般，...\n",
      "--------------------------------------------------------------------------------\n",
      "範例 2 (8 字符): 送货速度奇慢无比\n",
      "--------------------------------------------------------------------------------\n",
      "範例 3 (12 字符): 这是自己用过最好的用的了\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🎯 資料已準備就緒，可用於後續的 LLM 評分處理！\n"
     ]
    }
   ],
   "source": [
    "# 資料讀取選項\n",
    "# 您可以選擇以下任一種方式來載入資料：\n",
    "\n",
    "# 選項 1: 從已儲存的本地檔案讀取 (推薦，速度快)\n",
    "use_local_files = True\n",
    "\n",
    "# 選項 2: 從 Hugging Face 直接串流載入 (需要網路連線)\n",
    "use_streaming = False\n",
    "\n",
    "# 選項 3: 下載完整資料集 (檔案很大，不推薦)\n",
    "use_full_download = False\n",
    "\n",
    "print(\"=== 資料載入選項 ===\")\n",
    "print(f\"使用本地檔案: {use_local_files}\")\n",
    "print(f\"使用串流模式: {use_streaming}\")\n",
    "print(f\"下載完整資料集: {use_full_download}\")\n",
    "\n",
    "# 資料載入\n",
    "if use_local_files:\n",
    "    print(\"\\n📁 從本地檔案讀取資料...\")\n",
    "    \n",
    "    # 檢查已儲存的檔案\n",
    "    save_dir = \"saved_datasets\"\n",
    "    \n",
    "    if os.path.exists(save_dir):\n",
    "        import glob\n",
    "        \n",
    "        # 尋找可用的檔案\n",
    "        csv_files = glob.glob(f\"{save_dir}/*.csv\")\n",
    "        json_files = glob.glob(f\"{save_dir}/*.json\")\n",
    "        parquet_files = glob.glob(f\"{save_dir}/*.parquet\")\n",
    "        \n",
    "        print(f\"找到的檔案:\")\n",
    "        print(f\"  CSV 檔案: {len(csv_files)} 個\")\n",
    "        print(f\"  JSON 檔案: {len(json_files)} 個\")\n",
    "        print(f\"  Parquet 檔案: {len(parquet_files)} 個\")\n",
    "        \n",
    "        # 優先使用 Parquet 檔案 (最高效)\n",
    "        if parquet_files:\n",
    "            latest_file = max(parquet_files, key=os.path.getctime)\n",
    "            print(f\"\\n📊 讀取最新的 Parquet 檔案: {latest_file}\")\n",
    "            df = pd.read_parquet(latest_file)\n",
    "            \n",
    "        # 其次使用 CSV 檔案\n",
    "        elif csv_files:\n",
    "            latest_file = max(csv_files, key=os.path.getctime)\n",
    "            print(f\"\\n📊 讀取最新的 CSV 檔案: {latest_file}\")\n",
    "            df = pd.read_csv(latest_file)\n",
    "            \n",
    "        # 最後使用 JSON 檔案\n",
    "        elif json_files:\n",
    "            latest_file = max(json_files, key=os.path.getctime)\n",
    "            print(f\"\\n📊 讀取最新的 JSON 檔案: {latest_file}\")\n",
    "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ 沒有找到已儲存的資料檔案\")\n",
    "            print(\"請先執行資料下載和儲存的程式碼\")\n",
    "            df = None\n",
    "    else:\n",
    "        print(\"❌ 找不到 saved_datasets 目錄\")\n",
    "        print(\"請先執行資料下載和儲存的程式碼\")\n",
    "        df = None\n",
    "\n",
    "elif use_streaming:\n",
    "    print(\"\\n🌐 從 Hugging Face 串流載入資料...\")\n",
    "    \n",
    "    # 使用串流模式載入資料集\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\", streaming=True)\n",
    "    \n",
    "    # 設定要載入的樣本數量\n",
    "    num_samples = 1000\n",
    "    print(f\"載入前 {num_samples} 筆資料...\")\n",
    "    \n",
    "    # 收集資料\n",
    "    sample_data = []\n",
    "    for i, example in enumerate(dataset['train']):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        sample_data.append(example)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  已載入 {i + 1} 筆資料...\")\n",
    "    \n",
    "    # 轉換為 DataFrame\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    \n",
    "elif use_full_download:\n",
    "    print(\"\\n⬇️ 下載完整資料集...\")\n",
    "    print(\"警告：這將下載 13.7GB 的資料，可能需要很長時間\")\n",
    "    \n",
    "    # 下載完整資料集\n",
    "    dataset = load_dataset(\"austenjs/ClueCorpusSmallDataset\")\n",
    "    df = dataset['train'].to_pandas()\n",
    "\n",
    "else:\n",
    "    print(\"❌ 沒有選擇任何資料載入選項\")\n",
    "    df = None\n",
    "\n",
    "# 顯示資料資訊\n",
    "if df is not None:\n",
    "    print(f\"\\n✅ 資料載入成功！\")\n",
    "    print(f\"📊 資料形狀: {df.shape}\")\n",
    "    print(f\"📋 欄位名稱: {list(df.columns)}\")\n",
    "    \n",
    "    # 顯示基本統計\n",
    "    if 'text' in df.columns: # type: ignore\n",
    "        df['text_length'] = df['text'].str.len() # type: ignore\n",
    "        print(f\"\\n📈 文本長度統計:\")\n",
    "        print(df['text_length'].describe()) # type: ignore\n",
    "        \n",
    "        # 顯示前幾筆資料範例\n",
    "        print(f\"\\n📝 前 3 筆資料範例:\")\n",
    "        for i in range(min(3, len(df))): # type: ignore\n",
    "            text = df.iloc[i]['text']\n",
    "            # 顯示前100個字符\n",
    "            preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            print(f\"範例 {i+1} ({len(text)} 字符): {preview}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\n🎯 資料已準備就緒，可用於後續的 LLM 評分處理！\")\n",
    "else:\n",
    "    print(\"\\n❌ 資料載入失敗，請檢查設定並重新執行\")\n",
    "\n",
    "# 儲存到全域變數供後續使用\n",
    "globals()['dataset_df'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525fac3d",
   "metadata": {},
   "source": [
    "## LLM AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eb918cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 開始使用 Ollama GPT-OSS 20B 進行資料擴增...\n",
      "📋 設定參數:\n",
      "  模型: gpt-oss:20b\n",
      "  每個文本擴增數量: 3\n",
      "  處理文本數量: 50\n",
      "\n",
      "🔍 檢查 Ollama 模型...\n",
      "❌ Ollama 連接錯誤: 'name'\n",
      "請確保 Ollama 服務正在運行\n",
      "可以嘗試在終端機執行: ollama serve\n",
      "\n",
      "🔄 開始處理 50 個文本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "擴增進度: 100%|██████████| 150/150 [36:07<00:00, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 資料擴增完成！\n",
      "📊 擴增統計:\n",
      "  原始文本: 50 筆\n",
      "  擴增文本: 150 筆\n",
      "  總計: 200 筆\n",
      "  擴增倍率: 4.0x\n",
      "\n",
      "📈 資料來源分布:\n",
      "source\n",
      "original        50\n",
      "augmented_v1    50\n",
      "augmented_v2    50\n",
      "augmented_v3    50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "📝 擴增範例:\n",
      "\n",
      "ORIGINAL:\n",
      "  130真是佩服这家店开这么久。尽管门面已经小了一圈，但还是开着不容易啊。我们不容易，老板也不容易。自助餐，你可以吃得比平时多，但决不能浪费。想吃回20元，那是不可能的，所以还是不要去了。菜真的很一般，洗干净就好啦。什么都要另外付钱，一定要想好，别的不叫，只吃自助。\n",
      "\n",
      "AUGMENTED_V1:\n",
      "  真让人佩服这家店能经营这么久。虽然门面缩小了一圈，却依旧维持营业并不简单。我们和老板都不容易。自助餐可以吃得比平时多，但绝不能浪费。想要退回20元，根本不可能，还是别去吧。菜品真的很普通，干净就行。所有东西都要额外收费，务必三思，别点别的，只吃自助。\n",
      "\n",
      "AUGMENTED_V2:\n",
      "  我深感佩服这家店经营至今。尽管门面已略显简约，但仍不易维持。我们与老板同样面临诸多困难。自助餐可比平时多食，但务必避免浪费。若期望退回20元，恐怕无法实现，建议不再前往。菜品质量一般，只需洗净后食用。所有项目均需额外付费，请务必三思，若只想吃自助，请自行决定。\n",
      "\n",
      "AUGMENTED_V3:\n",
      "  这家店开了这么久，真让人佩服。门面虽已变小，但仍不易经营。我们和老板都不容易。自助餐可以比平时吃得多，但一定不能浪费。想要回20元是不可能的，还是别去了。菜品很一般，洗干净就行。所有东西都要另外付费，一定要想清楚，别叫别的菜，只吃自助。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "NDFrame.to_json() got an unexpected keyword argument 'ensure_ascii'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 198\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# JSON 格式\u001b[39;00m\n\u001b[32m    197\u001b[39m json_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m \u001b[43maugmented_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrecords\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# Parquet 格式\u001b[39;00m\n\u001b[32m    201\u001b[39m parquet_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: NDFrame.to_json() got an unexpected keyword argument 'ensure_ascii'"
     ]
    }
   ],
   "source": [
    "# 使用 Ollama GPT-OSS 20B 進行資料擴增\n",
    "\n",
    "print(\"🚀 開始使用 Ollama GPT-OSS 20B 進行資料擴增...\")\n",
    "\n",
    "# 檢查是否有載入的資料集\n",
    "if 'dataset_df' not in globals() or dataset_df is None:\n",
    "    print(\"❌ 沒有找到資料集，請先執行 GET DATA 部分\")\n",
    "else:\n",
    "    # 設定參數\n",
    "    model_name = \"gpt-oss:20b\"  # Ollama 模型名稱\n",
    "    num_augmentations_per_text = 3  # 每個原始文本生成的擴增數量\n",
    "    max_texts_to_process = 50  # 處理的文本數量 (可調整)\n",
    "    \n",
    "    print(f\"📋 設定參數:\")\n",
    "    print(f\"  模型: {model_name}\")\n",
    "    print(f\"  每個文本擴增數量: {num_augmentations_per_text}\")\n",
    "    print(f\"  處理文本數量: {max_texts_to_process}\")\n",
    "    \n",
    "    # 檢查 Ollama 連接\n",
    "    try:\n",
    "        # 測試 Ollama 連接\n",
    "        available_models = ollama.list()\n",
    "        print(f\"\\n🔍 檢查 Ollama 模型...\")\n",
    "        model_found = any(model_name in model['name'] for model in available_models['models'])\n",
    "        \n",
    "        if not model_found:\n",
    "            print(f\"⚠️  模型 {model_name} 未找到，嘗試拉取模型...\")\n",
    "            print(f\"正在下載 {model_name}，這可能需要一些時間...\")\n",
    "            ollama.pull(model_name)\n",
    "            print(f\"✅ 模型 {model_name} 下載完成\")\n",
    "        else:\n",
    "            print(f\"✅ 模型 {model_name} 已就緒\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Ollama 連接錯誤: {e}\")\n",
    "        print(\"請確保 Ollama 服務正在運行\")\n",
    "        print(\"可以嘗試在終端機執行: ollama serve\")\n",
    "        \n",
    "    # 定義資料擴增的提示模板\n",
    "    augmentation_prompts = [\n",
    "        # 提示1: 改寫保持原意\n",
    "        \"\"\"請將以下中文文本進行改寫，保持原意但使用不同的表達方式：\n",
    "\n",
    "原文本：{text}\n",
    "\n",
    "要求：\n",
    "1. 保持文本的核心意思不變\n",
    "2. 使用不同的詞彙和句式\n",
    "3. 保持簡體中文\n",
    "4. 輸出格式只需要改寫後的文本，不要其他說明\n",
    "\n",
    "改寫文本：\"\"\",\n",
    "\n",
    "        # 提示2: 風格轉換\n",
    "        \"\"\"請將以下文本轉換為更正式/更口語的表達方式：\n",
    "\n",
    "原文本：{text}\n",
    "\n",
    "要求：\n",
    "1. 如果原文比較口語，請轉為正式表達\n",
    "2. 如果原文比較正式，請轉為口語表達\n",
    "3. 保持文本的主要資訊\n",
    "4. 使用簡體中文\n",
    "5. 輸出格式只需要轉換後的文本，不要其他說明\n",
    "\n",
    "轉換文本：\"\"\",\n",
    "\n",
    "        # 提示3: 句式重組\n",
    "        \"\"\"請重新組織以下文本的句子結構，但保持相同的含義：\n",
    "\n",
    "原文本：{text}\n",
    "\n",
    "要求：\n",
    "1. 重新安排句子順序或結構\n",
    "2. 可以將長句拆分或短句合併\n",
    "3. 保持原始含義\n",
    "4. 使用簡體中文\n",
    "5. 輸出格式只需要重組後的文本，不要其他說明\n",
    "\n",
    "重組文本：\"\"\"\n",
    "    ]\n",
    "    \n",
    "    # 資料擴增函數\n",
    "    def augment_text(original_text, prompt_template):\n",
    "        \"\"\"使用 Ollama 擴增單個文本\"\"\"\n",
    "        try:\n",
    "            # 準備提示\n",
    "            full_prompt = prompt_template.format(text=original_text)\n",
    "            \n",
    "            # 調用 Ollama\n",
    "            response = ollama.generate(\n",
    "                model=model_name,\n",
    "                prompt=full_prompt,\n",
    "                options={\n",
    "                    'temperature': 0.7,\n",
    "                    'top_p': 0.9,\n",
    "                    'max_tokens': 1000\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # 提取生成的文本\n",
    "            augmented_text = response['response'].strip()\n",
    "            \n",
    "            # 清理文本 (移除可能的前綴)\n",
    "            augmented_text = re.sub(r'^(改寫文本：|轉換文本：|重組文本：)', '', augmented_text).strip()\n",
    "            \n",
    "            return augmented_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 文本擴增錯誤: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # 開始資料擴增\n",
    "    augmented_data = []\n",
    "    original_data = []\n",
    "    \n",
    "    # 準備原始資料\n",
    "    texts_to_process = dataset_df['text'].head(max_texts_to_process).tolist()\n",
    "    \n",
    "    print(f\"\\n🔄 開始處理 {len(texts_to_process)} 個文本...\")\n",
    "    \n",
    "    # 使用進度條\n",
    "    with tqdm(total=len(texts_to_process) * num_augmentations_per_text, desc=\"擴增進度\") as pbar:\n",
    "        \n",
    "        for idx, original_text in enumerate(texts_to_process):\n",
    "            # 添加原始文本\n",
    "            original_data.append({\n",
    "                'text': original_text,\n",
    "                'source': 'original',\n",
    "                'original_idx': idx\n",
    "            })\n",
    "            \n",
    "            # 對每個文本進行多次擴增\n",
    "            for aug_idx in range(num_augmentations_per_text):\n",
    "                # 選擇提示模板 (輪流使用)\n",
    "                prompt_idx = aug_idx % len(augmentation_prompts)\n",
    "                prompt = augmentation_prompts[prompt_idx]\n",
    "                \n",
    "                # 進行擴增\n",
    "                augmented_text = augment_text(original_text, prompt)\n",
    "                \n",
    "                if augmented_text and len(augmented_text.strip()) > 0:\n",
    "                    augmented_data.append({\n",
    "                        'text': augmented_text,\n",
    "                        'source': f'augmented_v{aug_idx + 1}',\n",
    "                        'original_idx': idx,\n",
    "                        'augmentation_method': f'prompt_{prompt_idx + 1}'\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"⚠️ 第 {idx} 個文本的第 {aug_idx + 1} 次擴增失敗\")\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # 添加小延遲避免過載\n",
    "                time.sleep(0.1)\n",
    "    \n",
    "    # 合併原始資料和擴增資料\n",
    "    all_data = original_data + augmented_data\n",
    "    \n",
    "    # 創建新的 DataFrame\n",
    "    augmented_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    print(f\"\\n✅ 資料擴增完成！\")\n",
    "    print(f\"📊 擴增統計:\")\n",
    "    print(f\"  原始文本: {len(original_data)} 筆\")\n",
    "    print(f\"  擴增文本: {len(augmented_data)} 筆\")\n",
    "    print(f\"  總計: {len(all_data)} 筆\")\n",
    "    print(f\"  擴增倍率: {len(all_data) / len(original_data):.1f}x\")\n",
    "    \n",
    "    # 顯示資料來源分布\n",
    "    print(f\"\\n📈 資料來源分布:\")\n",
    "    print(augmented_df['source'].value_counts())\n",
    "    \n",
    "    # 顯示範例\n",
    "    print(f\"\\n📝 擴增範例:\")\n",
    "    example_idx = 0\n",
    "    original_example = augmented_df[augmented_df['original_idx'] == example_idx]\n",
    "    \n",
    "    for i, row in original_example.iterrows():\n",
    "        print(f\"\\n{row['source'].upper()}:\")\n",
    "        text_preview = row['text'][:150] + \"...\" if len(row['text']) > 150 else row['text']\n",
    "        print(f\"  {text_preview}\")\n",
    "    \n",
    "    # 儲存擴增後的資料集\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    augmented_dir = \"augmented_datasets\"\n",
    "    os.makedirs(augmented_dir, exist_ok=True)\n",
    "    \n",
    "    # 儲存為多種格式\n",
    "    base_filename = f\"{augmented_dir}/clue_corpus_augmented_{timestamp}\"\n",
    "    \n",
    "    # CSV 格式\n",
    "    csv_filename = f\"{base_filename}.csv\"\n",
    "    augmented_df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    # JSON 格式\n",
    "    json_filename = f\"{base_filename}.json\"\n",
    "    augmented_df.to_json(json_filename, orient='records', ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Parquet 格式\n",
    "    parquet_filename = f\"{base_filename}.parquet\"\n",
    "    augmented_df.to_parquet(parquet_filename, index=False)\n",
    "    \n",
    "    print(f\"\\n💾 擴增資料集已儲存:\")\n",
    "    print(f\"  CSV: {csv_filename}\")\n",
    "    print(f\"  JSON: {json_filename}\")\n",
    "    print(f\"  Parquet: {parquet_filename}\")\n",
    "    \n",
    "    # 檔案大小\n",
    "    for filename in [csv_filename, json_filename, parquet_filename]:\n",
    "        size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"  {os.path.basename(filename)}: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # 儲存到全域變數\n",
    "    globals()['augmented_dataset_df'] = augmented_df\n",
    "    \n",
    "    print(f\"\\n🎯 擴增資料集已準備就緒，可用於訓練和評估！\")\n",
    "    print(f\"變數名稱: augmented_dataset_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa3e48ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 修正儲存格式並完成資料集儲存...\n",
      "❌ 找不到擴增資料集變數，請重新執行擴增程序\n"
     ]
    }
   ],
   "source": [
    "# 修正儲存問題並完成擴增資料集儲存\n",
    "print(\"🔧 修正儲存格式並完成資料集儲存...\")\n",
    "\n",
    "if 'augmented_dataset_df' in globals():\n",
    "    # 重新儲存\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    augmented_dir = \"augmented_datasets\"\n",
    "    os.makedirs(augmented_dir, exist_ok=True)\n",
    "    \n",
    "    base_filename = f\"{augmented_dir}/clue_corpus_augmented_{timestamp}\"\n",
    "    \n",
    "    # CSV 格式\n",
    "    csv_filename = f\"{base_filename}.csv\"\n",
    "    augmented_dataset_df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    # JSON 格式 (修正版本)\n",
    "    json_filename = f\"{base_filename}.json\"\n",
    "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(augmented_dataset_df.to_dict('records'), f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Parquet 格式\n",
    "    parquet_filename = f\"{base_filename}.parquet\"\n",
    "    augmented_dataset_df.to_parquet(parquet_filename, index=False)\n",
    "    \n",
    "    print(f\"✅ 擴增資料集儲存完成:\")\n",
    "    print(f\"  CSV: {csv_filename}\")\n",
    "    print(f\"  JSON: {json_filename}\")\n",
    "    print(f\"  Parquet: {parquet_filename}\")\n",
    "    \n",
    "    # 檔案大小統計\n",
    "    print(f\"\\n📁 檔案大小:\")\n",
    "    for filename in [csv_filename, json_filename, parquet_filename]:\n",
    "        if os.path.exists(filename):\n",
    "            size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "            print(f\"  {os.path.basename(filename)}: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # 最終統計\n",
    "    print(f\"\\n📊 最終擴增統計:\")\n",
    "    print(f\"  資料集形狀: {augmented_dataset_df.shape}\")\n",
    "    print(f\"  欄位: {list(augmented_dataset_df.columns)}\")\n",
    "    \n",
    "    # 品質檢查\n",
    "    print(f\"\\n🔍 資料品質檢查:\")\n",
    "    empty_texts = augmented_dataset_df['text'].str.strip().eq('').sum()\n",
    "    duplicate_texts = augmented_dataset_df.duplicated(subset=['text']).sum()\n",
    "    avg_length = augmented_dataset_df['text'].str.len().mean()\n",
    "    \n",
    "    print(f\"  空白文本: {empty_texts} 筆\")\n",
    "    print(f\"  重複文本: {duplicate_texts} 筆\")\n",
    "    print(f\"  平均文本長度: {avg_length:.1f} 字符\")\n",
    "    \n",
    "    # 擴增方法統計\n",
    "    if 'augmentation_method' in augmented_dataset_df.columns:\n",
    "        print(f\"\\n📈 擴增方法分布:\")\n",
    "        method_counts = augmented_dataset_df['augmentation_method'].value_counts()\n",
    "        for method, count in method_counts.items():\n",
    "            print(f\"  {method}: {count} 筆\")\n",
    "    \n",
    "    print(f\"\\n🎉 資料擴增專案完成！\")\n",
    "    print(f\"🎯 原始資料集: 50 筆 → 擴增資料集: {len(augmented_dataset_df)} 筆\")\n",
    "    print(f\"📈 擴增倍率: {len(augmented_dataset_df) / 50:.1f}x\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 找不到擴增資料集變數，請重新執行擴增程序\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73723d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 檢查擴增資料狀態...\n",
      "📂 找到擴增資料檔案: augmented_datasets/clue_corpus_augmented_20250901_094627.csv\n",
      "✅ 成功載入擴增資料集\n",
      "\n",
      "📊 擴增資料集統計:\n",
      "  資料形狀: (200, 4)\n",
      "  欄位: ['text', 'source', 'original_idx', 'augmentation_method']\n",
      "\n",
      "📈 資料來源分布:\n",
      "source\n",
      "original        50\n",
      "augmented_v1    50\n",
      "augmented_v2    50\n",
      "augmented_v3    50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "📏 文本長度統計:\n",
      "count     200.000000\n",
      "mean      327.555000\n",
      "std       423.366262\n",
      "min         8.000000\n",
      "25%        61.500000\n",
      "50%       224.500000\n",
      "75%       403.250000\n",
      "max      2468.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "📝 擴增範例對比:\n",
      "\n",
      "ORIGINAL:\n",
      "  130真是佩服这家店开这么久。尽管门面已经小了一圈，但还是开着不容易啊。我们不容易，老板也不容易。自助餐，你可以吃得比平时多，但决不能浪费。想吃回20元，那是不可能的，所以还是不要去了。菜真的很一般，...\n",
      "\n",
      "AUGMENTED_V1:\n",
      "  真让人佩服这家店能经营这么久。虽然门面缩小了一圈，却依旧维持营业并不简单。我们和老板都不容易。自助餐可以吃得比平时多，但绝不能浪费。想要退回20元，根本不可能，还是别去吧。菜品真的很普通，干净就行。所...\n",
      "\n",
      "AUGMENTED_V2:\n",
      "  我深感佩服这家店经营至今。尽管门面已略显简约，但仍不易维持。我们与老板同样面临诸多困难。自助餐可比平时多食，但务必避免浪费。若期望退回20元，恐怕无法实现，建议不再前往。菜品质量一般，只需洗净后食用。...\n",
      "\n",
      "AUGMENTED_V3:\n",
      "  这家店开了这么久，真让人佩服。门面虽已变小，但仍不易经营。我们和老板都不容易。自助餐可以比平时吃得多，但一定不能浪费。想要回20元是不可能的，还是别去了。菜品很一般，洗干净就行。所有东西都要另外付费，...\n",
      "\n",
      "🔍 資料品質檢查:\n",
      "  空白文本: 0 筆\n",
      "  重複文本: 1 筆\n",
      "\n",
      "🎯 擴增效果總結:\n",
      "  原始文本: 50 筆\n",
      "  擴增文本: 150 筆\n",
      "  總計: 200 筆\n",
      "  擴增倍率: 4.0x\n",
      "\n",
      "✅ 擴增資料集已載入並準備就緒！\n",
      "變數名稱: augmented_dataset_df\n"
     ]
    }
   ],
   "source": [
    "# 檢查並讀取已存在的擴增資料 (如果有的話)\n",
    "print(\"🔍 檢查擴增資料狀態...\")\n",
    "\n",
    "# 檢查是否有現有的擴增資料檔案\n",
    "augmented_dir = \"augmented_datasets\"\n",
    "if os.path.exists(augmented_dir):\n",
    "    import glob\n",
    "    \n",
    "    # 尋找最新的擴增資料檔案\n",
    "    parquet_files = glob.glob(f\"{augmented_dir}/*.parquet\")\n",
    "    csv_files = glob.glob(f\"{augmented_dir}/*.csv\")\n",
    "    \n",
    "    if parquet_files:\n",
    "        # 使用最新的 parquet 檔案\n",
    "        latest_file = max(parquet_files, key=os.path.getctime)\n",
    "        print(f\"📂 找到擴增資料檔案: {latest_file}\")\n",
    "        augmented_dataset_df = pd.read_parquet(latest_file)\n",
    "        print(f\"✅ 成功載入擴增資料集\")\n",
    "        \n",
    "    elif csv_files:\n",
    "        # 使用最新的 CSV 檔案\n",
    "        latest_file = max(csv_files, key=os.path.getctime)\n",
    "        print(f\"📂 找到擴增資料檔案: {latest_file}\")\n",
    "        augmented_dataset_df = pd.read_csv(latest_file)\n",
    "        print(f\"✅ 成功載入擴增資料集\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ 沒有找到擴增資料檔案\")\n",
    "        augmented_dataset_df = None\n",
    "else:\n",
    "    print(\"❌ 擴增資料目錄不存在\")\n",
    "    augmented_dataset_df = None\n",
    "\n",
    "# 如果成功載入，顯示統計資訊\n",
    "if augmented_dataset_df is not None:\n",
    "    print(f\"\\n📊 擴增資料集統計:\")\n",
    "    print(f\"  資料形狀: {augmented_dataset_df.shape}\")\n",
    "    print(f\"  欄位: {list(augmented_dataset_df.columns)}\")\n",
    "    \n",
    "    # 資料來源分布\n",
    "    if 'source' in augmented_dataset_df.columns:\n",
    "        print(f\"\\n📈 資料來源分布:\")\n",
    "        print(augmented_dataset_df['source'].value_counts())\n",
    "    \n",
    "    # 文本長度統計\n",
    "    augmented_dataset_df['text_length'] = augmented_dataset_df['text'].str.len()\n",
    "    print(f\"\\n📏 文本長度統計:\")\n",
    "    print(augmented_dataset_df['text_length'].describe())\n",
    "    \n",
    "    # 顯示一些範例\n",
    "    print(f\"\\n📝 擴增範例對比:\")\n",
    "    \n",
    "    # 找一個原始文本和其擴增版本\n",
    "    if 'original_idx' in augmented_dataset_df.columns:\n",
    "        sample_idx = 0\n",
    "        sample_data = augmented_dataset_df[augmented_dataset_df['original_idx'] == sample_idx]\n",
    "        \n",
    "        for i, row in sample_data.iterrows():\n",
    "            source = row['source']\n",
    "            text = row['text']\n",
    "            preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            print(f\"\\n{source.upper()}:\")\n",
    "            print(f\"  {preview}\")\n",
    "    \n",
    "    # 品質檢查\n",
    "    print(f\"\\n🔍 資料品質檢查:\")\n",
    "    empty_texts = augmented_dataset_df['text'].str.strip().eq('').sum()\n",
    "    duplicate_texts = augmented_dataset_df.duplicated(subset=['text']).sum()\n",
    "    \n",
    "    print(f\"  空白文本: {empty_texts} 筆\")\n",
    "    print(f\"  重複文本: {duplicate_texts} 筆\")\n",
    "    \n",
    "    # 計算擴增效果\n",
    "    original_count = (augmented_dataset_df['source'] == 'original').sum()\n",
    "    augmented_count = len(augmented_dataset_df) - original_count\n",
    "    \n",
    "    print(f\"\\n🎯 擴增效果總結:\")\n",
    "    print(f\"  原始文本: {original_count} 筆\")\n",
    "    print(f\"  擴增文本: {augmented_count} 筆\")\n",
    "    print(f\"  總計: {len(augmented_dataset_df)} 筆\")\n",
    "    print(f\"  擴增倍率: {len(augmented_dataset_df) / original_count:.1f}x\")\n",
    "    \n",
    "    # 儲存到全域變數\n",
    "    globals()['augmented_dataset_df'] = augmented_dataset_df\n",
    "    \n",
    "    print(f\"\\n✅ 擴增資料集已載入並準備就緒！\")\n",
    "    print(f\"變數名稱: augmented_dataset_df\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n❌ 無法載入擴增資料集\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4357b53",
   "metadata": {},
   "source": [
    "## 🎯 最終版大陸用語識別與篩選系統"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2c275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 啟動最終版大陸用語識別系統...\n",
      "============================================================\n",
      "✅ 使用 擴增資料集，共 200 筆記錄\n",
      "\n",
      "🎯 開始執行大陸用語篩選...\n",
      "📊 處理資料集：200 筆記錄\n",
      "📝 文本欄位：text\n",
      "🎯 樣本大小：50\n",
      "⚖️ 篩選閾值：3/5\n",
      "\n",
      "🔄 開始 Ollama 推論...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  第1筆: 1/5 - 通用簡體中文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:   2%|▏         | 1/50 [00:17<14:39, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  第2筆: 失敗 - 通用簡體中文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:   4%|▍         | 2/50 [07:34<3:31:27, 264.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  第3筆: 1/5 - 通用簡體中文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:  40%|████      | 20/50 [14:44<13:54, 27.80s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  第21筆: 1/5 - 通用簡體中文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推論進度:  74%|███████▍  | 37/50 [34:07<03:49, 17.62s/it]   "
     ]
    }
   ],
   "source": [
    "# 🎯 最終版大陸用語識別與篩選系統 - 使用 Ollama 推論並儲存結果\n",
    "print(\"🚀 啟動最終版大陸用語識別系統...\")\n",
    "\n",
    "# 定義大陸特有詞彙庫\n",
    "mainland_terms = {\n",
    "    \"計算機\": [\"電腦\"], \"軟件\": [\"軟體\"], \"硬件\": [\"硬體\"], \"網絡\": [\"網路\"], \n",
    "    \"數據\": [\"資料\"], \"程序\": [\"程式\"], \"信息\": [\"資訊\"], \"出租車\": [\"計程車\"],\n",
    "    \"公交車\": [\"公車\"], \"地鐵\": [\"捷運\"], \"質量\": [\"品質\"], \"服務員\": [\"服務生\"],\n",
    "    \"土豆\": [\"馬鈴薯\"], \"西紅柿\": [\"番茄\"], \"搞定\": [\"完成\"], \"挺\": [\"很\"],\n",
    "    \"咋\": [\"怎麼\"], \"啥\": [\"什麼\"], \"微信\": [\"\"], \"支付寶\": [\"\"], \"淘寶\": [\"\"]\n",
    "}\n",
    "\n",
    "# 大陸語法模式\n",
    "mainland_patterns = [r\"挺.*的\", r\"蠻.*的\", r\".*得很\", r\"咋.*\", r\"啥.*\"]\n",
    "\n",
    "def analyze_features(text):\n",
    "    \"\"\"快速特徵分析\"\"\"\n",
    "    mainland_count = sum(1 for term in mainland_terms if term in text)\n",
    "    pattern_count = sum(1 for pattern in mainland_patterns if re.search(pattern, text))\n",
    "    return {\n",
    "        \"mainland_terms\": [term for term in mainland_terms if term in text],\n",
    "        \"pattern_matches\": pattern_count,\n",
    "        \"authenticity_score\": mainland_count + pattern_count\n",
    "    }\n",
    "\n",
    "def mainland_score_ollama(text, model=\"gpt-oss:20b\"):\n",
    "    \"\"\"使用 Ollama 評分大陸用語特徵\"\"\"\n",
    "    prompt = f\"\"\"評估文本的大陸用語特徵，每項0或1分：\n",
    "\n",
    "文本：{text}\n",
    "\n",
    "評分標準：\n",
    "1. 大陸特有詞彙：計算機、軟件、出租車、地鐵等\n",
    "2. 大陸語法習慣：挺...的、蠻...的、咋樣等  \n",
    "3. 大陸口語表達：搞定、整、弄等\n",
    "4. 避免繁體用語：不含電腦、軟體、資料等\n",
    "5. 整體大陸化程度：綜合評估\n",
    "\n",
    "請按格式回答：\n",
    "大陸特有詞彙:0\n",
    "大陸語法習慣:0\n",
    "大陸口語表達:0\n",
    "避免繁體用語:1\n",
    "整體大陸化程度:0\n",
    "總分:1\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options={'temperature': 0.1, 'max_tokens': 100}\n",
    "        )\n",
    "        \n",
    "        # 解析回應\n",
    "        scores = {}\n",
    "        total = 0\n",
    "        categories = [\"大陸特有詞彙\", \"大陸語法習慣\", \"大陸口語表達\", \"避免繁體用語\", \"整體大陸化程度\"]\n",
    "        \n",
    "        for line in response['response'].split('\\n'):\n",
    "            for cat in categories:\n",
    "                if cat in line:\n",
    "                    match = re.search(r'[：:]\\s*([01])', line)\n",
    "                    if match:\n",
    "                        score = int(match.group(1))\n",
    "                        scores[cat] = score\n",
    "                        total += score\n",
    "        \n",
    "        if len(scores) == 5:\n",
    "            scores[\"總分\"] = total\n",
    "            return scores, response['response']\n",
    "        return None, response['response']\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def process_dataset(df, text_col='text', sample_size=100, threshold=3):\n",
    "    \"\"\"處理資料集進行大陸用語篩選\"\"\"\n",
    "    \n",
    "    print(f\"📊 處理資料集：{len(df)} 筆記錄\")\n",
    "    print(f\"📝 文本欄位：{text_col}\")\n",
    "    print(f\"🎯 樣本大小：{sample_size}\")\n",
    "    print(f\"⚖️ 篩選閾值：{threshold}/5\")\n",
    "    \n",
    "    # 取樣本\n",
    "    sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    texts = sample_df[text_col].tolist()\n",
    "    \n",
    "    # 執行推論\n",
    "    results = []\n",
    "    authentic_texts = []\n",
    "    generic_texts = []\n",
    "    \n",
    "    print(f\"\\n🔄 開始 Ollama 推論...\")\n",
    "    \n",
    "    for i, text in enumerate(tqdm(texts, desc=\"推論進度\")):\n",
    "        # 特徵分析\n",
    "        features = analyze_features(text)\n",
    "        \n",
    "        # Ollama 評分\n",
    "        scores, response = mainland_score_ollama(text)\n",
    "        \n",
    "        result = {\n",
    "            'index': i,\n",
    "            'text': text,\n",
    "            'text_length': len(text),\n",
    "            'features': features,\n",
    "            'scores': scores,\n",
    "            'response': response,\n",
    "            'success': scores is not None\n",
    "        }\n",
    "        \n",
    "        # 分類\n",
    "        if scores and scores.get(\"總分\", 0) >= threshold:\n",
    "            authentic_texts.append(result)\n",
    "            category = \"真正大陸用語\"\n",
    "        else:\n",
    "            generic_texts.append(result)\n",
    "            category = \"通用簡體中文\"\n",
    "        \n",
    "        result['category'] = category\n",
    "        results.append(result)\n",
    "        \n",
    "        # 顯示進度\n",
    "        if i % 20 == 0 or i < 3:\n",
    "            score_str = f\"{scores['總分']}/5\" if scores else \"失敗\"\n",
    "            print(f\"  第{i+1}筆: {score_str} - {category}\")\n",
    "        \n",
    "        time.sleep(0.2)  # 控制請求頻率\n",
    "    \n",
    "    return results, authentic_texts, generic_texts\n",
    "\n",
    "def save_results(results, authentic_texts, generic_texts):\n",
    "    \"\"\"儲存篩選結果\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. 完整結果\n",
    "    full_data = []\n",
    "    for r in results:\n",
    "        row = {\n",
    "            'text': r['text'],\n",
    "            'text_length': r['text_length'],\n",
    "            'category': r['category'],\n",
    "            'success': r['success'],\n",
    "            'authenticity_score': r['features']['authenticity_score'],\n",
    "            'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "        }\n",
    "        if r['scores']:\n",
    "            row.update({f'score_{k}': v for k, v in r['scores'].items()})\n",
    "        full_data.append(row)\n",
    "    \n",
    "    full_df = pd.DataFrame(full_data)\n",
    "    full_file = f\"mainland_filtering_complete_{timestamp}.csv\"\n",
    "    full_df.to_csv(full_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. 高質量大陸用語數據\n",
    "    if authentic_texts:\n",
    "        authentic_data = [{\n",
    "            'text': r['text'],\n",
    "            'total_score': r['scores']['總分'],\n",
    "            'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "        } for r in authentic_texts]\n",
    "        \n",
    "        auth_df = pd.DataFrame(authentic_data)\n",
    "        auth_csv = f\"authentic_mainland_texts_{timestamp}.csv\"\n",
    "        auth_json = f\"authentic_mainland_texts_{timestamp}.json\"\n",
    "        \n",
    "        auth_df.to_csv(auth_csv, index=False, encoding='utf-8-sig')\n",
    "        auth_df.to_json(auth_json, orient='records', force_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"💾 儲存完成:\")\n",
    "        print(f\"  📄 完整結果: {full_file}\")\n",
    "        print(f\"  ✅ 高質量數據: {auth_csv}\")\n",
    "        print(f\"  📋 JSON格式: {auth_json}\")\n",
    "        \n",
    "        return full_df, auth_df\n",
    "    \n",
    "    return full_df, None\n",
    "\n",
    "# 主要執行流程\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 檢查可用資料集\n",
    "available_data = None\n",
    "text_column = 'text'\n",
    "\n",
    "if 'augmented_dataset_df' in locals() and augmented_dataset_df is not None:\n",
    "    available_data = augmented_dataset_df\n",
    "    source_name = \"擴增資料集\"\n",
    "elif 'dataset_df' in locals() and dataset_df is not None:\n",
    "    available_data = dataset_df  \n",
    "    source_name = \"原始資料集\"\n",
    "\n",
    "if available_data is not None:\n",
    "    print(f\"✅ 使用 {source_name}，共 {len(available_data)} 筆記錄\")\n",
    "    \n",
    "    # 執行篩選（可調整參數）\n",
    "    SAMPLE_SIZE = 50    # 處理樣本數量\n",
    "    THRESHOLD = 3       # 篩選閾值\n",
    "    \n",
    "    print(f\"\\n🎯 開始執行大陸用語篩選...\")\n",
    "    results, authentic_results, generic_results = process_dataset(\n",
    "        df=available_data,\n",
    "        text_col=text_column,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        threshold=THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # 統計結果\n",
    "    print(f\"\\n📊 篩選結果統計:\")\n",
    "    print(f\"  ✅ 真正大陸用語: {len(authentic_results)} 筆\")\n",
    "    print(f\"  🗑️ 通用簡體中文: {len(generic_results)} 筆\")\n",
    "    print(f\"  📈 篩選率: {len(authentic_results)/len(results)*100:.1f}%\")\n",
    "    \n",
    "    # 顯示範例\n",
    "    if authentic_results:\n",
    "        print(f\"\\n📝 高質量大陸用語範例:\")\n",
    "        for i, r in enumerate(authentic_results[:3]):\n",
    "            preview = r['text'][:60] + \"...\" if len(r['text']) > 60 else r['text']\n",
    "            print(f\"  {i+1}. (得分:{r['scores']['總分']}) {preview}\")\n",
    "    \n",
    "    # 儲存結果\n",
    "    print(f\"\\n💾 儲存結果...\")\n",
    "    full_df, auth_df = save_results(results, authentic_results, generic_results)\n",
    "    \n",
    "    # 設定全域變數\n",
    "    globals()['mainland_filtering_results'] = results\n",
    "    globals()['authentic_mainland_data'] = auth_df\n",
    "    \n",
    "    print(f\"\\n🎉 大陸用語識別與篩選完成！\")\n",
    "    print(f\"📋 可用變數: mainland_filtering_results, authentic_mainland_data\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 沒有找到可用的資料集\")\n",
    "    print(\"💡 請先執行前面的資料載入或擴增步驟\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3199cb7",
   "metadata": {},
   "source": [
    "## 🧪 30筆真實資料測試腳本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff5f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 30筆真實資料測試腳本 - 大陸用語識別系統驗證\n",
    "print(\"🧪 啟動30筆真實資料測試...\")\n",
    "\n",
    "# 測試用的30筆真實資料 - 包含大陸用語和通用簡體中文\n",
    "test_data = [\n",
    "    # 明顯大陸用語 (1-10)\n",
    "    \"我的計算機軟件出了問題，需要重新安裝系統。\",\n",
    "    \"打車去機場挺方便的，出租車很多。\",\n",
    "    \"咋樣？這個質量還可以嗎？\",\n",
    "    \"數據庫的硬件配置需要升級了。\",\n",
    "    \"地鐵站裡的服務員態度蠻好的。\",\n",
    "    \"用微信支付很方便，比現金快多了。\",\n",
    "    \"晚飯想吃土豆燉牛肉，再來個西紅柿雞蛋湯。\",\n",
    "    \"網絡連接有問題，程序運行不了。\",\n",
    "    \"這事兒搞定了嗎？別忘了發信息給我。\",\n",
    "    \"啥時候能到？公交車堵車了。\",\n",
    "    \n",
    "    # 中等大陸特徵 (11-20)\n",
    "    \"今天天氣挺不錯的，適合出去走走。\",\n",
    "    \"這個價格蠻合理的，性價比很高。\",\n",
    "    \"辦公室的電腦需要安裝新軟件。\",\n",
    "    \"淘寶上買東西很方便，快遞也快。\",\n",
    "    \"地下停車場的車位挺緊張的。\",\n",
    "    \"這個問題搞不定，需要請教專家。\",\n",
    "    \"支付寶轉賬很安全，手續費也低。\",\n",
    "    \"質量控制做得不錯，產品很穩定。\",\n",
    "    \"數據分析的結果挺有意思的。\",\n",
    "    \"網上購物越來越流行了。\",\n",
    "    \n",
    "    # 通用簡體中文 (21-30) \n",
    "    \"今天的會議討論了很多重要議題。\",\n",
    "    \"學習新技術需要持續的努力和實踐。\",\n",
    "    \"這本書的內容很豐富，值得仔細閱讀。\",\n",
    "    \"運動對身體健康非常重要。\",\n",
    "    \"時間管理是提高工作效率的關鍵。\",\n",
    "    \"環境保護是我們共同的責任。\",\n",
    "    \"科技發展改變了我們的生活方式。\",\n",
    "    \"教育是社會進步的重要推動力。\",\n",
    "    \"藝術創作需要靈感和技巧的結合。\",\n",
    "    \"團隊合作能夠創造更大的價值。\"\n",
    "]\n",
    "\n",
    "# 手動標註的標準答案 (用於評估準確性)\n",
    "ground_truth = [\n",
    "    # 明顯大陸用語 (1-10) - 應該得分 >= 3\n",
    "    True, True, True, True, True, True, True, True, True, True,\n",
    "    # 中等大陸特徵 (11-20) - 可能得分 2-3\n",
    "    True, True, False, True, False, False, True, False, False, False,\n",
    "    # 通用簡體中文 (21-30) - 應該得分 <= 2\n",
    "    False, False, False, False, False, False, False, False, False, False\n",
    "]\n",
    "\n",
    "print(f\"📊 測試資料統計:\")\n",
    "print(f\"  總測試樣本: {len(test_data)} 筆\")\n",
    "print(f\"  預期大陸用語: {sum(ground_truth)} 筆\")\n",
    "print(f\"  預期通用中文: {len(ground_truth) - sum(ground_truth)} 筆\")\n",
    "\n",
    "# 重複使用現有的分析函數\n",
    "def analyze_features(text):\n",
    "    \"\"\"快速特徵分析\"\"\"\n",
    "    mainland_terms = {\n",
    "        \"計算機\": [\"電腦\"], \"軟件\": [\"軟體\"], \"硬件\": [\"硬體\"], \"網絡\": [\"網路\"], \n",
    "        \"數據\": [\"資料\"], \"程序\": [\"程式\"], \"信息\": [\"資訊\"], \"出租車\": [\"計程車\"],\n",
    "        \"公交車\": [\"公車\"], \"地鐵\": [\"捷運\"], \"質量\": [\"品質\"], \"服務員\": [\"服務生\"],\n",
    "        \"土豆\": [\"馬鈴薯\"], \"西紅柿\": [\"番茄\"], \"搞定\": [\"完成\"], \"挺\": [\"很\"],\n",
    "        \"咋\": [\"怎麼\"], \"啥\": [\"什麼\"], \"微信\": [\"\"], \"支付寶\": [\"\"], \"淘寶\": [\"\"]\n",
    "    }\n",
    "    \n",
    "    mainland_patterns = [r\"挺.*的\", r\"蠻.*的\", r\".*得很\", r\"咋.*\", r\"啥.*\"]\n",
    "    \n",
    "    mainland_count = sum(1 for term in mainland_terms if term in text)\n",
    "    pattern_count = sum(1 for pattern in mainland_patterns if re.search(pattern, text))\n",
    "    found_terms = [term for term in mainland_terms if term in text]\n",
    "    \n",
    "    return {\n",
    "        \"mainland_terms\": found_terms,\n",
    "        \"pattern_matches\": pattern_count,\n",
    "        \"authenticity_score\": mainland_count + pattern_count\n",
    "    }\n",
    "\n",
    "def mainland_score_ollama_test(text, model=\"gpt-oss:20b\"):\n",
    "    \"\"\"測試用的 Ollama 評分函數\"\"\"\n",
    "    prompt = f\"\"\"評估以下中文文本的大陸用語特徵，按照5個標準各給0或1分：\n",
    "\n",
    "文本：{text}\n",
    "\n",
    "評分標準：\n",
    "1. 大陸特有詞彙 (計算機、軟件、出租車、地鐵等): 0/1分\n",
    "2. 大陸語法習慣 (挺...的、蠻...的、咋樣等): 0/1分  \n",
    "3. 大陸口語表達 (搞定、整、弄等): 0/1分\n",
    "4. 避免繁體用語 (不含電腦、軟體、資料等): 0/1分\n",
    "5. 整體大陸化程度 (綜合判斷): 0/1分\n",
    "\n",
    "請嚴格按照以下格式回答，每行一個分數：\n",
    "大陸特有詞彙:0\n",
    "大陸語法習慣:0\n",
    "大陸口語表達:0\n",
    "避免繁體用語:0\n",
    "整體大陸化程度:0\n",
    "總分:0\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options={'temperature': 0.1, 'max_tokens': 150}\n",
    "        )\n",
    "        \n",
    "        # 解析回應\n",
    "        scores = {}\n",
    "        categories = [\"大陸特有詞彙\", \"大陸語法習慣\", \"大陸口語表達\", \"避免繁體用語\", \"整體大陸化程度\"]\n",
    "        \n",
    "        response_text = response['response']\n",
    "        \n",
    "        for cat in categories:\n",
    "            pattern = rf\"{cat}[：:]\\s*([01])\"\n",
    "            match = re.search(pattern, response_text)\n",
    "            if match:\n",
    "                scores[cat] = int(match.group(1))\n",
    "        \n",
    "        # 計算總分\n",
    "        if len(scores) == 5:\n",
    "            total = sum(scores.values())\n",
    "            scores[\"總分\"] = total\n",
    "            return scores, response_text, True\n",
    "        else:\n",
    "            return None, response_text, False\n",
    "            \n",
    "    except Exception as e:\n",
    "        return None, str(e), False\n",
    "\n",
    "# 執行測試\n",
    "print(f\"\\n🔄 開始執行測試...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_results = []\n",
    "correct_predictions = 0\n",
    "total_processed = 0\n",
    "\n",
    "# 測試每一筆資料\n",
    "for i, text in enumerate(tqdm(test_data, desc=\"測試進度\")):\n",
    "    print(f\"\\n第 {i+1} 筆測試:\")\n",
    "    print(f\"文本: {text[:50]}{'...' if len(text) > 50 else ''}\")\n",
    "    \n",
    "    # 特徵分析\n",
    "    features = analyze_features(text)\n",
    "    print(f\"特徵: {features['mainland_terms']} (模式:{features['pattern_matches']})\")\n",
    "    \n",
    "    # Ollama 評分\n",
    "    scores, response, success = mainland_score_ollama_test(text)\n",
    "    \n",
    "    if success and scores:\n",
    "        total_score = scores[\"總分\"]\n",
    "        predicted_mainland = total_score >= 3  # 閾值為3\n",
    "        actual_mainland = ground_truth[i]\n",
    "        \n",
    "        # 判斷預測是否正確\n",
    "        is_correct = predicted_mainland == actual_mainland\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        print(f\"評分: {total_score}/5 → {'大陸用語' if predicted_mainland else '通用中文'}\")\n",
    "        print(f\"實際: {'大陸用語' if actual_mainland else '通用中文'} → {'✅' if is_correct else '❌'}\")\n",
    "        \n",
    "        # 詳細分數\n",
    "        score_details = \" | \".join([f\"{k}:{v}\" for k, v in scores.items() if k != \"總分\"])\n",
    "        print(f\"詳細: {score_details}\")\n",
    "        \n",
    "        total_processed += 1\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ 評分失敗: {response}\")\n",
    "        predicted_mainland = None\n",
    "        is_correct = False\n",
    "    \n",
    "    # 儲存結果\n",
    "    result = {\n",
    "        'index': i,\n",
    "        'text': text,\n",
    "        'features': features,\n",
    "        'scores': scores,\n",
    "        'predicted_mainland': predicted_mainland,\n",
    "        'actual_mainland': ground_truth[i],\n",
    "        'correct': is_correct,\n",
    "        'success': success\n",
    "    }\n",
    "    test_results.append(result)\n",
    "    \n",
    "    time.sleep(0.2)  # 控制請求頻率\n",
    "\n",
    "# 計算測試結果統計\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 測試結果統計:\")\n",
    "\n",
    "if total_processed > 0:\n",
    "    accuracy = correct_predictions / total_processed * 100\n",
    "    print(f\"✅ 總體準確率: {accuracy:.1f}% ({correct_predictions}/{total_processed})\")\n",
    "    \n",
    "    # 分類統計\n",
    "    true_positives = sum(1 for r in test_results if r['predicted_mainland'] and r['actual_mainland'] and r['success'])\n",
    "    true_negatives = sum(1 for r in test_results if not r['predicted_mainland'] and not r['actual_mainland'] and r['success'])\n",
    "    false_positives = sum(1 for r in test_results if r['predicted_mainland'] and not r['actual_mainland'] and r['success'])\n",
    "    false_negatives = sum(1 for r in test_results if not r['predicted_mainland'] and r['actual_mainland'] and r['success'])\n",
    "    \n",
    "    print(f\"\\n📈 分類詳細統計:\")\n",
    "    print(f\"  真陽性 (正確識別大陸用語): {true_positives}\")\n",
    "    print(f\"  真陰性 (正確識別通用中文): {true_negatives}\")\n",
    "    print(f\"  假陽性 (誤判為大陸用語): {false_positives}\")\n",
    "    print(f\"  假陰性 (遺漏大陸用語): {false_negatives}\")\n",
    "    \n",
    "    # 計算精確率和召回率\n",
    "    if (true_positives + false_positives) > 0:\n",
    "        precision = true_positives / (true_positives + false_positives) * 100\n",
    "        print(f\"  精確率: {precision:.1f}%\")\n",
    "    \n",
    "    if (true_positives + false_negatives) > 0:\n",
    "        recall = true_positives / (true_positives + false_negatives) * 100\n",
    "        print(f\"  召回率: {recall:.1f}%\")\n",
    "\n",
    "# 分析錯誤案例\n",
    "print(f\"\\n🔍 錯誤案例分析:\")\n",
    "errors = [r for r in test_results if not r['correct'] and r['success']]\n",
    "\n",
    "if errors:\n",
    "    print(f\"發現 {len(errors)} 個錯誤案例:\")\n",
    "    for err in errors[:5]:  # 只顯示前5個錯誤\n",
    "        text_preview = err['text'][:40] + \"...\" if len(err['text']) > 40 else err['text']\n",
    "        predicted = \"大陸\" if err['predicted_mainland'] else \"通用\"\n",
    "        actual = \"大陸\" if err['actual_mainland'] else \"通用\"\n",
    "        score = err['scores']['總分'] if err['scores'] else \"N/A\"\n",
    "        print(f\"  ❌ 預測:{predicted} | 實際:{actual} | 分數:{score} | {text_preview}\")\n",
    "else:\n",
    "    print(\"🎉 沒有發現錯誤案例！\")\n",
    "\n",
    "# 儲存測試結果\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "test_results_file = f\"test_results_30samples_{timestamp}.csv\"\n",
    "\n",
    "# 準備儲存的資料\n",
    "save_data = []\n",
    "for r in test_results:\n",
    "    row = {\n",
    "        'text': r['text'],\n",
    "        'predicted_mainland': r['predicted_mainland'],\n",
    "        'actual_mainland': r['actual_mainland'],\n",
    "        'correct': r['correct'],\n",
    "        'authenticity_score': r['features']['authenticity_score'],\n",
    "        'mainland_terms': ','.join(r['features']['mainland_terms'])\n",
    "    }\n",
    "    \n",
    "    if r['scores']:\n",
    "        row.update({f'score_{k.replace(\":\", \"_\")}': v for k, v in r['scores'].items()})\n",
    "    \n",
    "    save_data.append(row)\n",
    "\n",
    "# 儲存為CSV\n",
    "test_df = pd.DataFrame(save_data)\n",
    "test_df.to_csv(test_results_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n💾 測試結果已儲存: {test_results_file}\")\n",
    "print(f\"📁 檔案大小: {os.path.getsize(test_results_file)/1024:.1f} KB\")\n",
    "\n",
    "# 儲存到全域變數\n",
    "globals()['test_results_30'] = test_results\n",
    "globals()['test_dataframe_30'] = test_df\n",
    "\n",
    "print(f\"\\n🎯 30筆真實資料測試完成！\")\n",
    "print(f\"📋 變數: test_results_30, test_dataframe_30\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
