{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1352d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装必要的依赖包\n",
    "# 如果您还没有安装这些包，请运行以下命令：\n",
    "# !pip install ollama requests\n",
    "\n",
    "import ollama\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ede089b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ollama服务正在运行\n",
      "可用模型数量: 1\n",
      "- gpt-oss:20b\n"
     ]
    }
   ],
   "source": [
    "# 检查Ollama服务是否运行\n",
    "try:\n",
    "    response = requests.get('http://localhost:11434/api/tags')\n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ Ollama服务正在运行\")\n",
    "        available_models = response.json()\n",
    "        print(f\"可用模型数量: {len(available_models.get('models', []))}\")\n",
    "        for model in available_models.get('models', []):\n",
    "            print(f\"- {model['name']}\")\n",
    "    else:\n",
    "        print(\"❌ Ollama服务未响应\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"❌ 无法连接到Ollama服务，请确保Ollama已启动\")\n",
    "    print(\"请在终端运行: ollama serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb78b167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户: 请写一首关于秋天的短诗\n",
      "AI助手正在思考中...\n",
      "--------------------------------------------------\n",
      "秋秋风风起起，，黄黄叶叶漫漫天天  \n",
      "  \n",
      "金金桂桂飘飘香香，，古古道道低低回回  \n",
      "  \n",
      "夕夕阳阳西西下下，，映映照照残残云云  \n",
      "  \n",
      "暮暮色色沉沉沉沉，，秋秋意意浓浓郁郁  \n",
      "  \n",
      "归归心心似似鹿鹿，，轻轻踏踏落落叶叶。。\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 流式推理示例（实时显示生成的文本）\n",
    "def streaming_chat(prompt, model_name=\"gpt-oss:20b\"):\n",
    "    \"\"\"\n",
    "    使用Ollama进行流式文本生成\n",
    "    \"\"\"\n",
    "    print(f\"AI助手正在思考中...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        stream = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            stream=True,\n",
    "        )\n",
    "        \n",
    "        full_response = \"\"\n",
    "        for chunk in stream:\n",
    "            content = chunk['message']['content']\n",
    "            print(content, end='', flush=True)\n",
    "            full_response += content\n",
    "            \n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        return full_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"错误: {e}\"\n",
    "\n",
    "# 测试流式推理\n",
    "test_prompt2 = \"请写一首关于秋天的短诗\"\n",
    "print(f\"用户: {test_prompt2}\")\n",
    "response2 = streaming_chat(test_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5be4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高级配置和参数调整\n",
    "def advanced_chat(prompt, model_name=\"gpt-oss:20b\", **options):\n",
    "    \"\"\"\n",
    "    使用自定义参数进行推理\n",
    "    \n",
    "    常用参数:\n",
    "    - temperature: 控制输出的随机性 (0.0-1.0)\n",
    "    - top_p: 核采样参数 (0.0-1.0)\n",
    "    - top_k: 限制考虑的token数量\n",
    "    - num_predict: 最大生成token数\n",
    "    - repeat_penalty: 重复惩罚 (1.0为无惩罚)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            options=options\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"错误: {e}\"\n",
    "\n",
    "# 测试不同参数设置\n",
    "test_prompt3 = \"请创造性地描述一个未来城市\"\n",
    "\n",
    "print(\"🔥 高创造性设置 (temperature=0.9):\")\n",
    "print(\"-\" * 50)\n",
    "creative_response = advanced_chat(\n",
    "    test_prompt3, \n",
    "    temperature=0.9,\n",
    "    top_p=0.9,\n",
    "    num_predict=200\n",
    ")\n",
    "print(creative_response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print(\"🧊 保守设置 (temperature=0.1):\")\n",
    "print(\"-\" * 50)\n",
    "conservative_response = advanced_chat(\n",
    "    test_prompt3,\n",
    "    temperature=0.1,\n",
    "    top_p=0.5,\n",
    "    num_predict=200\n",
    ")\n",
    "print(conservative_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多轮对话示例\n",
    "class OllamaChat:\n",
    "    def __init__(self, model_name=\"gpt-oss:20b\"):\n",
    "        self.model_name = model_name\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def chat(self, user_message, **options):\n",
    "        \"\"\"发送消息并获取回复\"\"\"\n",
    "        # 添加用户消息到历史\n",
    "        self.conversation_history.append({\n",
    "            'role': 'user',\n",
    "            'content': user_message\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # 使用完整的对话历史进行推理\n",
    "            response = ollama.chat(\n",
    "                model=self.model_name,\n",
    "                messages=self.conversation_history,\n",
    "                options=options\n",
    "            )\n",
    "            \n",
    "            # 添加AI回复到历史\n",
    "            ai_message = response['message']['content']\n",
    "            self.conversation_history.append({\n",
    "                'role': 'assistant',\n",
    "                'content': ai_message\n",
    "            })\n",
    "            \n",
    "            return ai_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"错误: {e}\"\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"清除对话历史\"\"\"\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def get_history(self):\n",
    "        \"\"\"获取对话历史\"\"\"\n",
    "        return self.conversation_history\n",
    "\n",
    "# 创建聊天实例\n",
    "chatbot = OllamaChat()\n",
    "\n",
    "# 进行多轮对话\n",
    "print(\"开始多轮对话测试:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 第一轮\n",
    "user_msg1 = \"你好，我想学习Python编程\"\n",
    "response1 = chatbot.chat(user_msg1, temperature=0.7)\n",
    "print(f\"用户: {user_msg1}\")\n",
    "print(f\"AI: {response1}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 第二轮（基于上下文）\n",
    "user_msg2 = \"请推荐一些适合初学者的Python书籍\"\n",
    "response2 = chatbot.chat(user_msg2, temperature=0.7)\n",
    "print(f\"用户: {user_msg2}\")\n",
    "print(f\"AI: {response2}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 第三轮（继续上下文）\n",
    "user_msg3 = \"那么应该从哪本书开始呢？\"\n",
    "response3 = chatbot.chat(user_msg3, temperature=0.7)\n",
    "print(f\"用户: {user_msg3}\")\n",
    "print(f\"AI: {response3}\")\n",
    "\n",
    "print(f\"\\n对话历史长度: {len(chatbot.get_history())} 条消息\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实用工具函数\n",
    "import time\n",
    "\n",
    "def get_model_info(model_name=\"gpt-oss:20b\"):\n",
    "    \"\"\"获取模型详细信息\"\"\"\n",
    "    try:\n",
    "        models = ollama.list()\n",
    "        for model in models['models']:\n",
    "            if model['name'].startswith(model_name):\n",
    "                print(f\"模型名称: {model['name']}\")\n",
    "                print(f\"大小: {model.get('size', 'Unknown')}\")\n",
    "                print(f\"修改时间: {model.get('modified_at', 'Unknown')}\")\n",
    "                return model\n",
    "        print(f\"未找到模型: {model_name}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"错误: {e}\")\n",
    "        return None\n",
    "\n",
    "def benchmark_model(model_name=\"gpt-oss:20b\", prompt=\"Hello, how are you?\"):\n",
    "    \"\"\"简单的性能基准测试\"\"\"\n",
    "    print(f\"对模型 {model_name} 进行性能测试...\")\n",
    "    print(f\"测试提示: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        response_text = response['message']['content']\n",
    "        response_length = len(response_text)\n",
    "        \n",
    "        print(f\"响应时间: {response_time:.2f} 秒\")\n",
    "        print(f\"响应长度: {response_length} 字符\")\n",
    "        print(f\"生成速度: {response_length/response_time:.2f} 字符/秒\")\n",
    "        print(f\"响应内容: {response_text[:100]}...\")\n",
    "        \n",
    "        return {\n",
    "            'response_time': response_time,\n",
    "            'response_length': response_length,\n",
    "            'response_text': response_text\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"测试失败: {e}\")\n",
    "        return None\n",
    "\n",
    "# 获取模型信息\n",
    "print(\"📊 模型信息:\")\n",
    "get_model_info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# 进行性能测试\n",
    "print(\"🚀 性能测试:\")\n",
    "benchmark_result = benchmark_model(prompt=\"用一句话解释量子计算\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
