{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1352d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…\n",
    "# å¦‚æœæ‚¨è¿˜æ²¡æœ‰å®‰è£…è¿™äº›åŒ…ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n",
    "# !pip install ollama requests\n",
    "\n",
    "import ollama\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ede089b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ\n",
      "å¯ç”¨æ¨¡å‹æ•°é‡: 1\n",
      "- gpt-oss:20b\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ\n",
    "try:\n",
    "    response = requests.get('http://localhost:11434/api/tags')\n",
    "    if response.status_code == 200:\n",
    "        print(\"âœ… OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ\")\n",
    "        available_models = response.json()\n",
    "        print(f\"å¯ç”¨æ¨¡å‹æ•°é‡: {len(available_models.get('models', []))}\")\n",
    "        for model in available_models.get('models', []):\n",
    "            print(f\"- {model['name']}\")\n",
    "    else:\n",
    "        print(\"âŒ OllamaæœåŠ¡æœªå“åº”\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œè¯·ç¡®ä¿Ollamaå·²å¯åŠ¨\")\n",
    "    print(\"è¯·åœ¨ç»ˆç«¯è¿è¡Œ: ollama serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb78b167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç”¨æˆ·: è¯·å†™ä¸€é¦–å…³äºç§‹å¤©çš„çŸ­è¯—\n",
      "AIåŠ©æ‰‹æ­£åœ¨æ€è€ƒä¸­...\n",
      "--------------------------------------------------\n",
      "ç§‹ç§‹é£é£èµ·èµ·ï¼Œï¼Œé»„é»„å¶å¶æ¼«æ¼«å¤©å¤©  \n",
      "  \n",
      "é‡‘é‡‘æ¡‚æ¡‚é£˜é£˜é¦™é¦™ï¼Œï¼Œå¤å¤é“é“ä½ä½å›å›  \n",
      "  \n",
      "å¤•å¤•é˜³é˜³è¥¿è¥¿ä¸‹ä¸‹ï¼Œï¼Œæ˜ æ˜ ç…§ç…§æ®‹æ®‹äº‘äº‘  \n",
      "  \n",
      "æš®æš®è‰²è‰²æ²‰æ²‰æ²‰æ²‰ï¼Œï¼Œç§‹ç§‹æ„æ„æµ“æµ“éƒéƒ  \n",
      "  \n",
      "å½’å½’å¿ƒå¿ƒä¼¼ä¼¼é¹¿é¹¿ï¼Œï¼Œè½»è½»è¸è¸è½è½å¶å¶ã€‚ã€‚\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# æµå¼æ¨ç†ç¤ºä¾‹ï¼ˆå®æ—¶æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬ï¼‰\n",
    "def streaming_chat(prompt, model_name=\"gpt-oss:20b\"):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨Ollamaè¿›è¡Œæµå¼æ–‡æœ¬ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    print(f\"AIåŠ©æ‰‹æ­£åœ¨æ€è€ƒä¸­...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        stream = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            stream=True,\n",
    "        )\n",
    "        \n",
    "        full_response = \"\"\n",
    "        for chunk in stream:\n",
    "            content = chunk['message']['content']\n",
    "            print(content, end='', flush=True)\n",
    "            full_response += content\n",
    "            \n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        return full_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"é”™è¯¯: {e}\"\n",
    "\n",
    "# æµ‹è¯•æµå¼æ¨ç†\n",
    "test_prompt2 = \"è¯·å†™ä¸€é¦–å…³äºç§‹å¤©çš„çŸ­è¯—\"\n",
    "print(f\"ç”¨æˆ·: {test_prompt2}\")\n",
    "response2 = streaming_chat(test_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5be4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é«˜çº§é…ç½®å’Œå‚æ•°è°ƒæ•´\n",
    "def advanced_chat(prompt, model_name=\"gpt-oss:20b\", **options):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°è¿›è¡Œæ¨ç†\n",
    "    \n",
    "    å¸¸ç”¨å‚æ•°:\n",
    "    - temperature: æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ (0.0-1.0)\n",
    "    - top_p: æ ¸é‡‡æ ·å‚æ•° (0.0-1.0)\n",
    "    - top_k: é™åˆ¶è€ƒè™‘çš„tokenæ•°é‡\n",
    "    - num_predict: æœ€å¤§ç”Ÿæˆtokenæ•°\n",
    "    - repeat_penalty: é‡å¤æƒ©ç½š (1.0ä¸ºæ— æƒ©ç½š)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            options=options\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"é”™è¯¯: {e}\"\n",
    "\n",
    "# æµ‹è¯•ä¸åŒå‚æ•°è®¾ç½®\n",
    "test_prompt3 = \"è¯·åˆ›é€ æ€§åœ°æè¿°ä¸€ä¸ªæœªæ¥åŸå¸‚\"\n",
    "\n",
    "print(\"ğŸ”¥ é«˜åˆ›é€ æ€§è®¾ç½® (temperature=0.9):\")\n",
    "print(\"-\" * 50)\n",
    "creative_response = advanced_chat(\n",
    "    test_prompt3, \n",
    "    temperature=0.9,\n",
    "    top_p=0.9,\n",
    "    num_predict=200\n",
    ")\n",
    "print(creative_response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print(\"ğŸ§Š ä¿å®ˆè®¾ç½® (temperature=0.1):\")\n",
    "print(\"-\" * 50)\n",
    "conservative_response = advanced_chat(\n",
    "    test_prompt3,\n",
    "    temperature=0.1,\n",
    "    top_p=0.5,\n",
    "    num_predict=200\n",
    ")\n",
    "print(conservative_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šè½®å¯¹è¯ç¤ºä¾‹\n",
    "class OllamaChat:\n",
    "    def __init__(self, model_name=\"gpt-oss:20b\"):\n",
    "        self.model_name = model_name\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def chat(self, user_message, **options):\n",
    "        \"\"\"å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤\"\"\"\n",
    "        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²\n",
    "        self.conversation_history.append({\n",
    "            'role': 'user',\n",
    "            'content': user_message\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # ä½¿ç”¨å®Œæ•´çš„å¯¹è¯å†å²è¿›è¡Œæ¨ç†\n",
    "            response = ollama.chat(\n",
    "                model=self.model_name,\n",
    "                messages=self.conversation_history,\n",
    "                options=options\n",
    "            )\n",
    "            \n",
    "            # æ·»åŠ AIå›å¤åˆ°å†å²\n",
    "            ai_message = response['message']['content']\n",
    "            self.conversation_history.append({\n",
    "                'role': 'assistant',\n",
    "                'content': ai_message\n",
    "            })\n",
    "            \n",
    "            return ai_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"é”™è¯¯: {e}\"\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"æ¸…é™¤å¯¹è¯å†å²\"\"\"\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def get_history(self):\n",
    "        \"\"\"è·å–å¯¹è¯å†å²\"\"\"\n",
    "        return self.conversation_history\n",
    "\n",
    "# åˆ›å»ºèŠå¤©å®ä¾‹\n",
    "chatbot = OllamaChat()\n",
    "\n",
    "# è¿›è¡Œå¤šè½®å¯¹è¯\n",
    "print(\"å¼€å§‹å¤šè½®å¯¹è¯æµ‹è¯•:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ç¬¬ä¸€è½®\n",
    "user_msg1 = \"ä½ å¥½ï¼Œæˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹\"\n",
    "response1 = chatbot.chat(user_msg1, temperature=0.7)\n",
    "print(f\"ç”¨æˆ·: {user_msg1}\")\n",
    "print(f\"AI: {response1}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ç¬¬äºŒè½®ï¼ˆåŸºäºä¸Šä¸‹æ–‡ï¼‰\n",
    "user_msg2 = \"è¯·æ¨èä¸€äº›é€‚åˆåˆå­¦è€…çš„Pythonä¹¦ç±\"\n",
    "response2 = chatbot.chat(user_msg2, temperature=0.7)\n",
    "print(f\"ç”¨æˆ·: {user_msg2}\")\n",
    "print(f\"AI: {response2}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ç¬¬ä¸‰è½®ï¼ˆç»§ç»­ä¸Šä¸‹æ–‡ï¼‰\n",
    "user_msg3 = \"é‚£ä¹ˆåº”è¯¥ä»å“ªæœ¬ä¹¦å¼€å§‹å‘¢ï¼Ÿ\"\n",
    "response3 = chatbot.chat(user_msg3, temperature=0.7)\n",
    "print(f\"ç”¨æˆ·: {user_msg3}\")\n",
    "print(f\"AI: {response3}\")\n",
    "\n",
    "print(f\"\\nå¯¹è¯å†å²é•¿åº¦: {len(chatbot.get_history())} æ¡æ¶ˆæ¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®ç”¨å·¥å…·å‡½æ•°\n",
    "import time\n",
    "\n",
    "def get_model_info(model_name=\"gpt-oss:20b\"):\n",
    "    \"\"\"è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯\"\"\"\n",
    "    try:\n",
    "        models = ollama.list()\n",
    "        for model in models['models']:\n",
    "            if model['name'].startswith(model_name):\n",
    "                print(f\"æ¨¡å‹åç§°: {model['name']}\")\n",
    "                print(f\"å¤§å°: {model.get('size', 'Unknown')}\")\n",
    "                print(f\"ä¿®æ”¹æ—¶é—´: {model.get('modified_at', 'Unknown')}\")\n",
    "                return model\n",
    "        print(f\"æœªæ‰¾åˆ°æ¨¡å‹: {model_name}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"é”™è¯¯: {e}\")\n",
    "        return None\n",
    "\n",
    "def benchmark_model(model_name=\"gpt-oss:20b\", prompt=\"Hello, how are you?\"):\n",
    "    \"\"\"ç®€å•çš„æ€§èƒ½åŸºå‡†æµ‹è¯•\"\"\"\n",
    "    print(f\"å¯¹æ¨¡å‹ {model_name} è¿›è¡Œæ€§èƒ½æµ‹è¯•...\")\n",
    "    print(f\"æµ‹è¯•æç¤º: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        response_text = response['message']['content']\n",
    "        response_length = len(response_text)\n",
    "        \n",
    "        print(f\"å“åº”æ—¶é—´: {response_time:.2f} ç§’\")\n",
    "        print(f\"å“åº”é•¿åº¦: {response_length} å­—ç¬¦\")\n",
    "        print(f\"ç”Ÿæˆé€Ÿåº¦: {response_length/response_time:.2f} å­—ç¬¦/ç§’\")\n",
    "        print(f\"å“åº”å†…å®¹: {response_text[:100]}...\")\n",
    "        \n",
    "        return {\n",
    "            'response_time': response_time,\n",
    "            'response_length': response_length,\n",
    "            'response_text': response_text\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"æµ‹è¯•å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "# è·å–æ¨¡å‹ä¿¡æ¯\n",
    "print(\"ğŸ“Š æ¨¡å‹ä¿¡æ¯:\")\n",
    "get_model_info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# è¿›è¡Œæ€§èƒ½æµ‹è¯•\n",
    "print(\"ğŸš€ æ€§èƒ½æµ‹è¯•:\")\n",
    "benchmark_result = benchmark_model(prompt=\"ç”¨ä¸€å¥è¯è§£é‡Šé‡å­è®¡ç®—\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
