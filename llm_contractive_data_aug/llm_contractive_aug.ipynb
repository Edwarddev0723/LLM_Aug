{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f0bb94",
   "metadata": {},
   "source": [
    "# SMS ç›¸ä¼¼æ€§é…å° - LLM è¼”åŠ©å°æ¯”å­¸ç¿’æ­£æ¨£æœ¬ç”Ÿæˆ\n",
    "\n",
    "æ­¤ notebook ä½¿ç”¨ LLM (Together AI) ä¾†æ‰¾å‡ºç°¡è¨Šæ•¸æ“šä¸­å…©å…©ç›¸ä¼¼çš„å…§å®¹ï¼Œç‚ºå°æ¯”å­¸ç¿’ç”Ÿæˆæ­£æ¨£æœ¬å°ã€‚\n",
    "\n",
    "## æµç¨‹æ¦‚è¿°\n",
    "1. è¼‰å…¥ SMS æ•¸æ“š\n",
    "2. ä½¿ç”¨ LLM æ¯”è¼ƒç°¡è¨Šç›¸ä¼¼æ€§\n",
    "3. ç”Ÿæˆç›¸ä¼¼ç°¡è¨Šå°çš„ CSV æª”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb01baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from together import Together\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from itertools import combinations\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eb61081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²æˆåŠŸåˆå§‹åŒ– 4 å€‹ Together AI å®¢æˆ¶ç«¯\n",
      "API Key 1: 1f3b522748...\n",
      "API Key 2: 542d70c17d...\n",
      "API Key 3: 713f17cd14...\n",
      "API Key 4: db7765986a...\n"
     ]
    }
   ],
   "source": [
    "# è¨­ç½® Together AI API å®¢æˆ¶ç«¯\n",
    "# è®€å– API keys\n",
    "try:\n",
    "    with open(\"data_game/together_api_key.txt\", \"r\") as f:\n",
    "        api_keys = [line.strip() for line in f.readlines() if line.strip()]\n",
    "except FileNotFoundError:\n",
    "    print(\"è«‹ç¢ºä¿ data_game/together_api_key.txt æ–‡ä»¶å­˜åœ¨\")\n",
    "    raise\n",
    "\n",
    "# ç¢ºä¿æœ‰è¶³å¤ çš„ API key\n",
    "if len(api_keys) < 4:\n",
    "    raise ValueError(\"è«‹åœ¨ together_api_key.txt æ–‡ä»¶ä¸­æä¾›å››å€‹ API keyï¼Œæ¯è¡Œä¸€å€‹\")\n",
    "\n",
    "# å‰µå»ºå››å€‹ Together å®¢æˆ¶ç«¯é€²è¡Œè² è¼‰å‡è¡¡\n",
    "together_clients = [Together(api_key=key) for key in api_keys]\n",
    "\n",
    "# è¿½è¹¤ç•¶å‰ä½¿ç”¨å“ªå€‹å®¢æˆ¶ç«¯\n",
    "_current_client_index = 0\n",
    "\n",
    "def get_current_together_client():\n",
    "    \"\"\"å¾ªç’°è¿”å›å››å€‹ Together å®¢æˆ¶ç«¯\"\"\"\n",
    "    global _current_client_index\n",
    "    current_client = together_clients[_current_client_index]\n",
    "    _current_client_index = (_current_client_index + 1) % len(together_clients)\n",
    "    return current_client\n",
    "\n",
    "print(f\"å·²æˆåŠŸåˆå§‹åŒ– {len(together_clients)} å€‹ Together AI å®¢æˆ¶ç«¯\")\n",
    "for i, key in enumerate(api_keys):\n",
    "    print(f\"API Key {i+1}: {key[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c9688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¼‰å…¥äº† 209481 ç­†ç°¡è¨Šæ•¸æ“š\n",
      "\n",
      "æ•¸æ“šçµæ§‹:\n",
      "   sms_id                                           sms_body  label  name_flg\n",
      "0  162569  è¦ªæ„›çš„å®¶é•·æ‚¨å¥½ï¼Œå¯¶å¯¶å³å°‡æ»¿ä¸€æ­²ï¼Œè¨˜å¾—å®‰æ’å›å…’ç§‘æ–½æ‰“éº»ç–¹è…®è…ºç‚å¾·åœ‹éº»ç–¹(MMR)ç–«è‹—ï¼Œä¸¦æ”œå¸¶å¥...    NaN       NaN\n",
      "1  314614  å¯Œé‚¦å¸³æˆ¶è½‰å¸³å®Œæˆï¼šæ‚¨æ–¼05/28ä¸Šåˆ10:14è½‰å‡ºNT$4,200å…ƒè‡³ç‹é“éŠ€è¡Œå¸³æˆ¶æœ«å››ç¢¼71...    NaN       NaN\n",
      "2  355174  ã€æ¸¬é©—é€šçŸ¥ã€‘å¼µç¿åº­å­¸å“¡ï¼šæœ¬é€±äº”ç‚ºã€Œè‡ªç„¶ç§‘æ¢ç©¶èˆ‡å¯¦ä½œèƒ½åŠ›è©•é‡ã€ï¼Œå…±90åˆ†é˜ï¼Œè«‹æ”œå¸¶å¯¦é©—ç­†è¨˜èˆ‡è­·...    NaN       NaN\n",
      "3   28258  è©¹ç¿å“²å›ï¼Œæ‚¨å¥½ï¼šæé†’æ‚¨5æœˆåˆ†æ‰‹æ©Ÿåˆ†æœŸæ¬¾NT$2,100è‡³ä»Šå°šæœªå…¥å¸³ï¼Œç³»çµ±å·²ç™¼é€3æ¬¡é€šçŸ¥æœªå›è¦†...    NaN       NaN\n",
      "4  376466  å°Šæ•¬çš„å®¢æˆ¶ï¼Œé€™æ˜¯ä¾†è‡ªä¸­ä¿¡éŠ€è¡Œçš„æé†’ã€‚æ‚¨çš„è²¸æ¬¾æ¬¾é …å·²éæœŸï¼Œè«‹æ‚¨å„˜é€Ÿå„Ÿé‚„ã€‚è‹¥å·²ç¹³æ¬¾ï¼Œè«‹ç„¡éœ€ç†æœƒæ­¤...    NaN       NaN\n",
      "\n",
      "æ¬„ä½: ['sms_id', 'sms_body', 'label', 'name_flg']\n",
      "\n",
      "éæ¿¾å¾Œå‰©é¤˜ 209481 ç­†æœ‰æ•ˆç°¡è¨Š\n",
      "\n",
      "=== Label åˆ†å¸ƒåˆ†æ ===\n",
      "label\n",
      "1.0    1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "æ—…éŠé¡å‹ç°¡è¨Š (label=1): 1000 ç­†\n",
      "éæ—…éŠé¡å‹ç°¡è¨Š (labelâ‰ 1): 208481 ç­†\n",
      "\n",
      "=== æ—…éŠé¡å‹ç°¡è¨Šæ¨£æœ¬ ===\n",
      "ID 254284: ä¾†å ´ç•°åœ‹æ–‡åŒ–ä¹‹æ—…ï¼ã€Œç•°åŸŸé¢¨æƒ…ã€æ—…è¡Œç¤¾æ¨å‡ºçš„ä¸ƒæ—¥å°åº¦ä¹‹æ—…ï¼Œå¸¶æ‚¨æ¢ç´¢å¾·é‡Œã€é˜¿æ ¼æ‹‰ç­‰åœ°ï¼Œæ„Ÿå—å°åº¦çš„æ­·å²èˆ‡æ–‡åŒ–åº•è˜Šã€‚å ±åå³äº«æ—©é³¥å„ªæƒ ï¼Œç«‹åˆ»å ±åï¼Œé–‹å•Ÿé€™å ´ç²¾å½©çš„å°åº¦ä¹‹æ—…ï¼å ±åç†±ç·šï¼š02-23335588ã€‚\n",
      "ID 229496: æéº—èŠ¬å°å§ï¼Œæ‚¨çš„ã€Œæ—¥æœ¬æ²–ç¹©æµ·å³¶ä¹‹æ—…ã€å·²ç¢ºèªï¼Œæ‰€æœ‰è¡Œç¨‹èˆ‡ä½å®¿å°‡æ–¼å‡ºç™¼å‰ä¸‰å¤©ç™¼é€è‡³æ‚¨çš„é›»å­ä¿¡ç®±ã€‚\n",
      "ID 44913: ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ç‹å­ç¶­èˆ‡æ—éƒèŒ¹å ±ååƒåŠ ã€ŒåŒ—æµ·é“é›ªåœ°æº«æ³‰å‡æœŸã€ï¼Œæå®¥å¸†ä¹Ÿå¼·çƒˆæ¨è–¦æ»‘é›ªèª²ç¨‹ï¼‹å¤œé–“æ³¡æ¹¯é«”é©—ï¼Œé™æ™‚æ—©é³¥ç¾æŠ˜$2000ï¼\n",
      "\n",
      "=== éæ—…éŠé¡å‹ç°¡è¨Šæ¨£æœ¬ ===\n",
      "ID 162569: è¦ªæ„›çš„å®¶é•·æ‚¨å¥½ï¼Œå¯¶å¯¶å³å°‡æ»¿ä¸€æ­²ï¼Œè¨˜å¾—å®‰æ’å›å…’ç§‘æ–½æ‰“éº»ç–¹è…®è…ºç‚å¾·åœ‹éº»ç–¹(MMR)ç–«è‹—ï¼Œä¸¦æ”œå¸¶å¥ä¿å¡èˆ‡å…’ç«¥å¥åº·æ‰‹å†Šã€‚\n",
      "ID 314614: å¯Œé‚¦å¸³æˆ¶è½‰å¸³å®Œæˆï¼šæ‚¨æ–¼05/28ä¸Šåˆ10:14è½‰å‡ºNT$4,200å…ƒè‡³ç‹é“éŠ€è¡Œå¸³æˆ¶æœ«å››ç¢¼7183ï¼Œäº¤æ˜“ç·¨è™Ÿå·²åŒæ­¥å…¥å¸³ã€‚\n",
      "ID 355174: ã€æ¸¬é©—é€šçŸ¥ã€‘å¼µç¿åº­å­¸å“¡ï¼šæœ¬é€±äº”ç‚ºã€Œè‡ªç„¶ç§‘æ¢ç©¶èˆ‡å¯¦ä½œèƒ½åŠ›è©•é‡ã€ï¼Œå…±90åˆ†é˜ï¼Œè«‹æ”œå¸¶å¯¦é©—ç­†è¨˜èˆ‡è­·ç›®é¡è‡³ç†åŒ–æ•™å®¤åƒåŠ ã€‚\n"
     ]
    }
   ],
   "source": [
    "# è¼‰å…¥ SMS æ•¸æ“šä¸¦åˆ†æ label åˆ†å¸ƒ\n",
    "df = pd.read_csv(\"datagame_sms_stage1.csv\")\n",
    "print(f\"è¼‰å…¥äº† {len(df)} ç­†ç°¡è¨Šæ•¸æ“š\")\n",
    "print(\"\\næ•¸æ“šçµæ§‹:\")\n",
    "print(df.head())\n",
    "print(f\"\\næ¬„ä½: {list(df.columns)}\")\n",
    "\n",
    "# åªä¿ç•™æœ‰ç°¡è¨Šå…§å®¹çš„æ•¸æ“š\n",
    "df = df.dropna(subset=['sms_body'])\n",
    "print(f\"\\néæ¿¾å¾Œå‰©é¤˜ {len(df)} ç­†æœ‰æ•ˆç°¡è¨Š\")\n",
    "\n",
    "# åˆ†æ label åˆ†å¸ƒ\n",
    "print(f\"\\n=== Label åˆ†å¸ƒåˆ†æ ===\")\n",
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "# åˆ†é›¢æ—…éŠé¡å‹å’Œéæ—…éŠé¡å‹ç°¡è¨Š\n",
    "travel_sms = df[df['label'] == 1].copy()\n",
    "non_travel_sms = df[df['label'] != 1].copy()\n",
    "\n",
    "print(f\"\\næ—…éŠé¡å‹ç°¡è¨Š (label=1): {len(travel_sms)} ç­†\")\n",
    "print(f\"éæ—…éŠé¡å‹ç°¡è¨Š (labelâ‰ 1): {len(non_travel_sms)} ç­†\")\n",
    "\n",
    "# é¡¯ç¤ºä¸€äº›æ¨£æœ¬\n",
    "print(f\"\\n=== æ—…éŠé¡å‹ç°¡è¨Šæ¨£æœ¬ ===\")\n",
    "for i, row in travel_sms.head(3).iterrows():\n",
    "    print(f\"ID {row['sms_id']}: {row['sms_body']}\")\n",
    "\n",
    "print(f\"\\n=== éæ—…éŠé¡å‹ç°¡è¨Šæ¨£æœ¬ ===\")\n",
    "for i, row in non_travel_sms.head(3).iterrows():\n",
    "    print(f\"ID {row['sms_id']}: {row['sms_body']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71f34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰¹é‡ç›¸ä¼¼æ€§åˆ¤æ–·åŠŸèƒ½å·²è¨­ç½®å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# é™æµè¨­ç½®\n",
    "RPM = 60                # requests/min\n",
    "WINDOW = 60             # sec\n",
    "MAX_RETRY = 3           # retries\n",
    "MODEL_ID = \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n",
    "\n",
    "# è«‹æ±‚ç´€éŒ„\n",
    "_req_log = []\n",
    "\n",
    "def _throttle():\n",
    "    \"\"\"ç°¡å–®çš„é™æµæ©Ÿåˆ¶\"\"\"\n",
    "    now = time.time()\n",
    "    # æ¸…ç†è¶…éæ™‚é–“çª—å£çš„è¨˜éŒ„\n",
    "    _req_log[:] = [t for t in _req_log if now - t < WINDOW]\n",
    "    \n",
    "    # æª¢æŸ¥æ˜¯å¦éœ€è¦ç­‰å¾…\n",
    "    if len(_req_log) >= RPM:\n",
    "        wait_time = WINDOW - (now - _req_log[0])\n",
    "        if wait_time > 0:\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    _req_log.append(now)\n",
    "\n",
    "# è¨­è¨ˆæ‰¹é‡ç›¸ä¼¼æ€§åˆ¤æ–·çš„ prompt\n",
    "batch_similarity_prompt_template = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬ç›¸ä¼¼æ€§åˆ¤æ–·åŠ©æ‰‹ã€‚è«‹ä»”ç´°æ¯”è¼ƒä»¥ä¸‹3å€‹ç°¡è¨Šçš„å…§å®¹ï¼Œæ‰¾å‡ºå…¶ä¸­ç›¸ä¼¼çš„ç°¡è¨Šå°ã€‚\n",
    "    åˆ¤æ–·æ¨™æº–:\n",
    "    1. ä¸»é¡Œç›¸ä¼¼æ€§ï¼šå…©å€‹ç°¡è¨Šæ˜¯å¦è¨è«–ç›¸åŒæˆ–ç›¸é—œçš„ä¸»é¡Œ\n",
    "    2. æ„åœ–ç›¸ä¼¼æ€§ï¼šå…©å€‹ç°¡è¨Šçš„ç›®çš„æ˜¯å¦ç›¸ä¼¼ï¼ˆå¦‚ï¼šé€šçŸ¥ã€æ¨å»£ã€æé†’ç­‰ï¼‰\n",
    "    3. å…§å®¹ç›¸ä¼¼æ€§ï¼šå…·é«”å…§å®¹æ˜¯å¦æœ‰é‡ç–Šæˆ–ç›¸é—œæ€§\n",
    "    æ³¨æ„ï¼š\n",
    "    - å³ä½¿æªè©ä¸åŒï¼Œä½†å¦‚æœä¸»é¡Œå’Œæ„åœ–ç›¸ä¼¼ï¼Œä¹Ÿæ‡‰åˆ¤æ–·ç‚ºç›¸ä¼¼\n",
    "    - åªæ˜¯æ ¼å¼ç›¸ä¼¼ä½†å…§å®¹å®Œå…¨ä¸åŒçš„ç°¡è¨Šä¸æ‡‰åˆ¤æ–·ç‚ºç›¸ä¼¼\n",
    "    - è€ƒæ…®ç°¡è¨Šçš„å¯¦éš›å«ç¾©è€Œéè¡¨é¢æ–‡å­—\n",
    "    è«‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼Œæ¯è¡Œä¸€å€‹æ¯”è¼ƒçµæœï¼š\n",
    "    A-B: 1 (å¦‚æœç°¡è¨ŠAå’ŒBç›¸ä¼¼) æˆ– 0 (å¦‚æœä¸ç›¸ä¼¼)\n",
    "    A-C: 1 (å¦‚æœç°¡è¨ŠAå’ŒCç›¸ä¼¼) æˆ– 0 (å¦‚æœä¸ç›¸ä¼¼)  \n",
    "    B-C: 1 (å¦‚æœç°¡è¨ŠBå’ŒCç›¸ä¼¼) æˆ– 0 (å¦‚æœä¸ç›¸ä¼¼)\n",
    "    ç°¡è¨ŠA: {sms_a}\n",
    "    ç°¡è¨ŠB: {sms_b}\n",
    "    ç°¡è¨ŠC: {sms_c}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def check_batch_sms_similarity(sms_list: List[Tuple[int, str]]) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ LLM æ‰¹é‡åˆ¤æ–·3å€‹ç°¡è¨Šçš„ç›¸ä¼¼æ€§\n",
    "    \n",
    "    Args:\n",
    "        sms_list: [(sms_id, sms_body), ...] æœ€å¤š3å€‹ç°¡è¨Š\n",
    "    \n",
    "    Returns:\n",
    "        ç›¸ä¼¼ç°¡è¨Šå°çš„åˆ—è¡¨ [(id1, id2), ...]\n",
    "    \"\"\"\n",
    "    if len(sms_list) != 3:\n",
    "        raise ValueError(\"æ‰¹é‡æ¯”è¼ƒéœ€è¦æ°å¥½3å€‹ç°¡è¨Š\")\n",
    "    \n",
    "    (id_a, sms_a), (id_b, sms_b), (id_c, sms_c) = sms_list\n",
    "    \n",
    "    prompt = batch_similarity_prompt_template.format(\n",
    "        sms_a=sms_a, \n",
    "        sms_b=sms_b, \n",
    "        sms_c=sms_c\n",
    "    )\n",
    "    \n",
    "    client = get_current_together_client()\n",
    "    \n",
    "    for attempt in range(MAX_RETRY):\n",
    "        try:\n",
    "            _throttle()\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_ID,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=20,  # å¢åŠ tokenæ•¸é‡ä»¥æ‡‰å°å¤šè¡Œè¼¸å‡º\n",
    "                temperature=0.0,\n",
    "                stream=False,\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # è§£æçµæœ\n",
    "            similar_pairs = []\n",
    "            lines = result.split('\\n')\n",
    "            \n",
    "            # è§£ææ ¼å¼ï¼šA-B: 1, A-C: 0, B-C: 1\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if ':' in line:\n",
    "                    pair_part, similarity_part = line.split(':', 1)\n",
    "                    pair_part = pair_part.strip()\n",
    "                    similarity = similarity_part.strip()\n",
    "                    \n",
    "                    if similarity == '1':\n",
    "                        if pair_part.upper() == 'A-B':\n",
    "                            similar_pairs.append((id_a, id_b))\n",
    "                        elif pair_part.upper() == 'A-C':\n",
    "                            similar_pairs.append((id_a, id_c))\n",
    "                        elif pair_part.upper() == 'B-C':\n",
    "                            similar_pairs.append((id_b, id_c))\n",
    "            \n",
    "            return similar_pairs\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt + 1 >= MAX_RETRY:\n",
    "                print(f\"Error after {MAX_RETRY} attempts: {e}\")\n",
    "                print(f\"Raw response: {result if 'result' in locals() else 'No response'}\")\n",
    "                return []  # å¤±æ•—æ™‚è¿”å›ç©ºåˆ—è¡¨\n",
    "            \n",
    "            print(f\"Attempt {attempt + 1} failed: {e}. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "    \n",
    "    return []\n",
    "\n",
    "# ä¿ç•™åŸå§‹å–®å°æ¯”è¼ƒå‡½æ•¸ä½œç‚ºå‚™ç”¨\n",
    "def check_sms_similarity(sms1: str, sms2: str) -> str:\n",
    "    \"\"\"ä½¿ç”¨ LLM åˆ¤æ–·å…©å€‹ç°¡è¨Šæ˜¯å¦ç›¸ä¼¼ï¼ˆå–®å°æ¯”è¼ƒï¼‰\"\"\"\n",
    "    similarity_prompt_template = textwrap.dedent(\n",
    "        \"\"\"\n",
    "        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬ç›¸ä¼¼æ€§åˆ¤æ–·åŠ©æ‰‹ã€‚è«‹ä»”ç´°æ¯”è¼ƒä»¥ä¸‹å…©å€‹ç°¡è¨Šçš„å…§å®¹ï¼Œåˆ¤æ–·å®ƒå€‘æ˜¯å¦åœ¨èªç¾©ä¸Šç›¸ä¼¼ã€‚\n",
    "\n",
    "        åˆ¤æ–·æ¨™æº–:\n",
    "        1. ä¸»é¡Œç›¸ä¼¼æ€§ï¼šå…©å€‹ç°¡è¨Šæ˜¯å¦è¨è«–ç›¸åŒæˆ–ç›¸é—œçš„ä¸»é¡Œ\n",
    "        2. æ„åœ–ç›¸ä¼¼æ€§ï¼šå…©å€‹ç°¡è¨Šçš„ç›®çš„æ˜¯å¦ç›¸ä¼¼ï¼ˆå¦‚ï¼šé€šçŸ¥ã€æ¨å»£ã€æé†’ç­‰ï¼‰\n",
    "        3. å…§å®¹ç›¸ä¼¼æ€§ï¼šå…·é«”å…§å®¹æ˜¯å¦æœ‰é‡ç–Šæˆ–ç›¸é—œæ€§\n",
    "        \n",
    "        æ³¨æ„ï¼š\n",
    "        - å³ä½¿æªè©ä¸åŒï¼Œä½†å¦‚æœä¸»é¡Œå’Œæ„åœ–ç›¸ä¼¼ï¼Œä¹Ÿæ‡‰åˆ¤æ–·ç‚ºç›¸ä¼¼\n",
    "        - åªæ˜¯æ ¼å¼ç›¸ä¼¼ä½†å…§å®¹å®Œå…¨ä¸åŒçš„ç°¡è¨Šä¸æ‡‰åˆ¤æ–·ç‚ºç›¸ä¼¼\n",
    "        - è€ƒæ…®ç°¡è¨Šçš„å¯¦éš›å«ç¾©è€Œéè¡¨é¢æ–‡å­—\n",
    "\n",
    "        è«‹åªå›ç­” \"1\"ï¼ˆç›¸ä¼¼ï¼‰æˆ– \"0\"ï¼ˆä¸ç›¸ä¼¼ï¼‰ï¼Œä¸è¦è¼¸å‡ºå…¶ä»–å…§å®¹ã€‚\n",
    "\n",
    "        ç°¡è¨Š1: {sms1}\n",
    "        \n",
    "        ç°¡è¨Š2: {sms2}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    prompt = similarity_prompt_template.format(sms1=sms1, sms2=sms2)\n",
    "    \n",
    "    client = get_current_together_client()\n",
    "    \n",
    "    for attempt in range(MAX_RETRY):\n",
    "        try:\n",
    "            _throttle()\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_ID,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=4,\n",
    "                temperature=0.0,\n",
    "                stream=False,\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content.strip()\n",
    "            if result in [\"0\", \"1\"]:\n",
    "                return result\n",
    "            else:\n",
    "                print(f\"Warning: Unexpected output '{result}', treating as not similar\")\n",
    "                return \"0\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt + 1 >= MAX_RETRY:\n",
    "                print(f\"Error after {MAX_RETRY} attempts: {e}\")\n",
    "                return \"0\"  # å¤±æ•—æ™‚é è¨­ç‚ºä¸ç›¸ä¼¼\n",
    "            \n",
    "            print(f\"Attempt {attempt + 1} failed: {e}. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "    \n",
    "    return \"0\"\n",
    "\n",
    "print(\"æ‰¹é‡ç›¸ä¼¼æ€§åˆ¤æ–·åŠŸèƒ½å·²è¨­ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3366560c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰¹é‡åŒé¡å‹é…å°åŠŸèƒ½å·²è¨­ç½®å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# æ‰¹é‡åŒé¡å‹ç°¡è¨Šé…å°å‡½æ•¸\n",
    "def find_same_type_pairs_batch(df_subset: pd.DataFrame, max_pairs: int = None, description: str = \"\") -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    åœ¨åŒé¡å‹ç°¡è¨Šä¸­ä½¿ç”¨æ‰¹é‡æ¯”è¼ƒæ‰¾å‡ºç›¸ä¼¼å°\n",
    "    \n",
    "    Args:\n",
    "        df_subset: åŒé¡å‹çš„ç°¡è¨Š DataFrame\n",
    "        max_pairs: æœ€å¤§è¦æ‰¾çš„ç›¸ä¼¼å°æ•¸é‡\n",
    "        description: æè¿°æ–‡å­—\n",
    "    \n",
    "    Returns:\n",
    "        ç›¸ä¼¼ç°¡è¨Šå°çš„åˆ—è¡¨ [(id1, id2), ...]\n",
    "    \"\"\"\n",
    "    if len(df_subset) < 3:\n",
    "        print(f\"{description}: æ•¸æ“šé‡ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘3ç­†ï¼‰ï¼Œæ”¹ç”¨å–®å°æ¯”è¼ƒ\")\n",
    "        return find_same_type_pairs_single(df_subset, max_pairs, description)\n",
    "    \n",
    "    similar_pairs = []\n",
    "    df_list = list(df_subset.iterrows())\n",
    "    \n",
    "    if max_pairs is None:\n",
    "        max_pairs = min(len(df_subset) * (len(df_subset) - 1) // 2, 1000)  # é è¨­æœ€å¤š1000å°\n",
    "    \n",
    "    print(f\"\\n=== {description} (æ‰¹é‡æ¨¡å¼) ===\")\n",
    "    print(f\"å°‡æ‰¹é‡æ¯”è¼ƒ {len(df_subset)} ç­†ç°¡è¨Š\")\n",
    "    print(f\"ç›®æ¨™æ‰¾åˆ° {max_pairs} å€‹ç›¸ä¼¼å°\")\n",
    "    \n",
    "    # è¨ˆç®—éœ€è¦çš„æ‰¹æ¬¡æ•¸é‡ï¼ˆæ¯3å€‹ç°¡è¨Šç‚ºä¸€æ‰¹ï¼‰\n",
    "    total_batches = len(df_list) // 3\n",
    "    if len(df_list) % 3 != 0:\n",
    "        total_batches += 1\n",
    "    \n",
    "    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦\n",
    "    with tqdm(total=min(total_batches, max_pairs), desc=f\"{description}æ‰¹é‡æ¯”è¼ƒé€²åº¦\") as pbar:\n",
    "        batch_count = 0\n",
    "        \n",
    "        # æ‰¹é‡è™•ç†ï¼šæ¯3å€‹ç°¡è¨Šç‚ºä¸€çµ„\n",
    "        for i in range(0, len(df_list), 3):\n",
    "            if len(similar_pairs) >= max_pairs:\n",
    "                break\n",
    "            \n",
    "            # å–3å€‹ç°¡è¨Šé€²è¡Œæ‰¹é‡æ¯”è¼ƒ\n",
    "            batch = df_list[i:i+3]\n",
    "            if len(batch) == 3:\n",
    "                # æº–å‚™æ‰¹é‡æ¯”è¼ƒçš„æ•¸æ“š\n",
    "                sms_batch = [(row['sms_id'], row['sms_body']) for idx, row in batch]\n",
    "                \n",
    "                # é€²è¡Œæ‰¹é‡ç›¸ä¼¼æ€§åˆ¤æ–·\n",
    "                batch_pairs = check_batch_sms_similarity(sms_batch)\n",
    "                \n",
    "                # æ·»åŠ æ‰¾åˆ°çš„ç›¸ä¼¼å°\n",
    "                for pair in batch_pairs:\n",
    "                    if len(similar_pairs) < max_pairs:\n",
    "                        similar_pairs.append(pair)\n",
    "                        \n",
    "                        # æ‰¾åˆ°ä¸¦é¡¯ç¤ºç°¡è¨Šå…§å®¹\n",
    "                        id1, id2 = pair\n",
    "                        sms1 = df_subset[df_subset['sms_id'] == id1]['sms_body'].iloc[0]\n",
    "                        sms2 = df_subset[df_subset['sms_id'] == id2]['sms_body'].iloc[0]\n",
    "\n",
    "                \n",
    "                batch_count += 1\n",
    "                pbar.update(1)\n",
    "            \n",
    "            # è™•ç†å‰©é¤˜çš„1-2å€‹ç°¡è¨Šï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰\n",
    "            elif len(batch) == 2 and len(similar_pairs) < max_pairs:\n",
    "                # å°å‰©é¤˜çš„2å€‹ç°¡è¨Šé€²è¡Œå–®å°æ¯”è¼ƒ\n",
    "                row1, row2 = batch[0][1], batch[1][1]\n",
    "                is_similar = check_sms_similarity(row1['sms_body'], row2['sms_body'])\n",
    "                \n",
    "                if is_similar == \"1\":\n",
    "                    similar_pairs.append((row1['sms_id'], row2['sms_id']))\n",
    "                \n",
    "        \n",
    "        # å¦‚æœé‚„éœ€è¦æ›´å¤šç›¸ä¼¼å°ï¼Œç¹¼çºŒé€²è¡Œäº¤å‰æ‰¹é‡æ¯”è¼ƒ\n",
    "        if len(similar_pairs) < max_pairs and len(df_list) >= 6:\n",
    "            print(f\"\\né€²è¡Œäº¤å‰æ‰¹é‡æ¯”è¼ƒä»¥æ‰¾åˆ°æ›´å¤šç›¸ä¼¼å°...\")\n",
    "            cross_batch_count = 0\n",
    "            max_cross_batches = min(50, max_pairs - len(similar_pairs))  # é™åˆ¶äº¤å‰æ¯”è¼ƒæ¬¡æ•¸\n",
    "            \n",
    "            for i in range(0, len(df_list)-3, 6):  # æ¯6å€‹ç°¡è¨Šå–å…©çµ„é€²è¡Œäº¤å‰æ¯”è¼ƒ\n",
    "                if len(similar_pairs) >= max_pairs or cross_batch_count >= max_cross_batches:\n",
    "                    break\n",
    "                \n",
    "                # å–ç¬¬ä¸€çµ„3å€‹ç°¡è¨Š\n",
    "                batch1 = df_list[i:i+3]\n",
    "                # å–ç¬¬äºŒçµ„3å€‹ç°¡è¨Š\n",
    "                batch2 = df_list[i+3:i+6] if i+6 <= len(df_list) else df_list[i+3:]\n",
    "                \n",
    "                if len(batch1) == 3 and len(batch2) >= 2:\n",
    "                    # å‰µå»ºæ··åˆæ‰¹æ¬¡é€²è¡Œæ¯”è¼ƒ\n",
    "                    for j in range(len(batch2)):\n",
    "                        if len(similar_pairs) >= max_pairs:\n",
    "                            break\n",
    "                        \n",
    "                        # å–batch1çš„å‰2å€‹ + batch2çš„ç¬¬jå€‹çµ„æˆæ–°çš„æ‰¹æ¬¡\n",
    "                        mixed_batch = [batch1[0], batch1[1], batch2[j]]\n",
    "                        sms_batch = [(row['sms_id'], row['sms_body']) for idx, row in mixed_batch]\n",
    "                        \n",
    "                        batch_pairs = check_batch_sms_similarity(sms_batch)\n",
    "                        \n",
    "                        for pair in batch_pairs:\n",
    "                            if len(similar_pairs) < max_pairs:\n",
    "                                # æª¢æŸ¥æ˜¯å¦ç‚ºæ–°çš„ç›¸ä¼¼å°\n",
    "                                if pair not in similar_pairs and (pair[1], pair[0]) not in similar_pairs:\n",
    "                                    similar_pairs.append(pair)\n",
    "                                    \n",
    "                                    id1, id2 = pair\n",
    "                                    sms1 = df_subset[df_subset['sms_id'] == id1]['sms_body'].iloc[0]\n",
    "                                    sms2 = df_subset[df_subset['sms_id'] == id2]['sms_body'].iloc[0]\n",
    "                                    \n",
    "                cross_batch_count += 1\n",
    "                pbar.update(1)\n",
    "    \n",
    "    print(f\"\\n{description}å®Œæˆï¼ç¸½å…±æ‰¾åˆ° {len(similar_pairs)} å€‹ç›¸ä¼¼å°\")\n",
    "    return similar_pairs\n",
    "\n",
    "# å–®å°æ¯”è¼ƒå‡½æ•¸ï¼ˆå‚™ç”¨ï¼‰\n",
    "def find_same_type_pairs_single(df_subset: pd.DataFrame, max_pairs: int = None, description: str = \"\") -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    åœ¨åŒé¡å‹ç°¡è¨Šä¸­ä½¿ç”¨å–®å°æ¯”è¼ƒæ‰¾å‡ºç›¸ä¼¼å°ï¼ˆå‚™ç”¨æ–¹æ³•ï¼‰\n",
    "    \"\"\"\n",
    "    if len(df_subset) < 2:\n",
    "        print(f\"{description}: æ•¸æ“šé‡ä¸è¶³ï¼Œç„¡æ³•é…å°\")\n",
    "        return []\n",
    "    \n",
    "    similar_pairs = []\n",
    "    total_combinations = len(df_subset) * (len(df_subset) - 1) // 2\n",
    "    \n",
    "    if max_pairs is None:\n",
    "        max_pairs = min(total_combinations, 1000)\n",
    "    \n",
    "    print(f\"\\n=== {description} (å–®å°æ¨¡å¼) ===\")\n",
    "    print(f\"å°‡æ¯”è¼ƒ {len(df_subset)} ç­†ç°¡è¨Šï¼Œç¸½å…± {total_combinations} å€‹çµ„åˆ\")\n",
    "    print(f\"ç›®æ¨™æ‰¾åˆ° {max_pairs} å€‹ç›¸ä¼¼å°\")\n",
    "    \n",
    "    with tqdm(total=min(total_combinations, max_pairs * 5), desc=f\"{description}æ¯”è¼ƒé€²åº¦\") as pbar:\n",
    "        compared = 0\n",
    "        \n",
    "        for i, row1 in df_subset.iterrows():\n",
    "            if len(similar_pairs) >= max_pairs:\n",
    "                break\n",
    "                \n",
    "            for j, row2 in df_subset.iterrows():\n",
    "                if i >= j:\n",
    "                    continue\n",
    "                \n",
    "                compared += 1\n",
    "                pbar.update(1)\n",
    "                \n",
    "                if len(similar_pairs) >= max_pairs:\n",
    "                    break\n",
    "                \n",
    "                if compared > max_pairs * 5:\n",
    "                    break\n",
    "                \n",
    "                is_similar = check_sms_similarity(row1['sms_body'], row2['sms_body'])\n",
    "                \n",
    "                if is_similar == \"1\":\n",
    "                    similar_pairs.append((row1['sms_id'], row2['sms_id']))\n",
    "                    print(f\"\\næ‰¾åˆ°ç¬¬ {len(similar_pairs)} å€‹{description}ç›¸ä¼¼å°:\")\n",
    "                    print(f\"ID {row1['sms_id']}: {row1['sms_body'][:50]}...\")\n",
    "                    print(f\"ID {row2['sms_id']}: {row2['sms_body'][:50]}...\")\n",
    "                    \n",
    "            if len(similar_pairs) >= max_pairs or compared > max_pairs * 5:\n",
    "                break\n",
    "    \n",
    "    print(f\"\\n{description}å®Œæˆï¼ç¸½å…±æ‰¾åˆ° {len(similar_pairs)} å€‹ç›¸ä¼¼å°\")\n",
    "    return similar_pairs\n",
    "\n",
    "# è¨­ç½®é è¨­ä½¿ç”¨æ‰¹é‡æ¯”è¼ƒ\n",
    "find_same_type_pairs = find_same_type_pairs_batch\n",
    "\n",
    "print(\"æ‰¹é‡åŒé¡å‹é…å°åŠŸèƒ½å·²è¨­ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e14a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é–‹å§‹åŸ·è¡Œåˆ†é¡é…å°ç­–ç•¥...\n",
      "\n",
      "ğŸ–ï¸ ç¬¬ä¸€éšæ®µï¼šæ—…éŠé¡å‹ç°¡è¨Šé…å°\n",
      "\n",
      "=== æ—…éŠé¡å‹ç°¡è¨Š (æ‰¹é‡æ¨¡å¼) ===\n",
      "å°‡æ‰¹é‡æ¯”è¼ƒ 1000 ç­†ç°¡è¨Š\n",
      "ç›®æ¨™æ‰¾åˆ° 400 å€‹ç›¸ä¼¼å°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ—…éŠé¡å‹ç°¡è¨Šæ‰¹é‡æ¯”è¼ƒé€²åº¦:   0%|          | 0/334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ—…éŠé¡å‹ç°¡è¨Šæ‰¹é‡æ¯”è¼ƒé€²åº¦:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 201/334 [50:11<33:12, 14.98s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ—…éŠé¡å‹ç°¡è¨Šå®Œæˆï¼ç¸½å…±æ‰¾åˆ° 400 å€‹ç›¸ä¼¼å°\n",
      "\n",
      "ğŸ¢ ç¬¬äºŒéšæ®µï¼šéæ—…éŠé¡å‹ç°¡è¨Šé…å°\n",
      "å¾ 208481 ç­†éæ—…éŠç°¡è¨Šä¸­æ¡æ¨£ 40000 ç­†\n",
      "\n",
      "=== éæ—…éŠé¡å‹ç°¡è¨Š (æ‰¹é‡æ¨¡å¼) ===\n",
      "å°‡æ‰¹é‡æ¯”è¼ƒ 40000 ç­†ç°¡è¨Š\n",
      "ç›®æ¨™æ‰¾åˆ° 4000 å€‹ç›¸ä¼¼å°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "éæ—…éŠé¡å‹ç°¡è¨Šæ‰¹é‡æ¯”è¼ƒé€²åº¦:  19%|â–ˆâ–‰        | 755/4000 [3:23:25<26:50:13, 29.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error code: 429 - {\"message\": \"You have reached the rate limit specific to this model meta-llama/Llama-3.3-70B-Instruct-Turbo-Free. The maximum rate limit for this model is 6.0 queries and 60000 tokens per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\", \"type_\": \"model_rate_limit\"}. Retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "éæ—…éŠé¡å‹ç°¡è¨Šæ‰¹é‡æ¯”è¼ƒé€²åº¦:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2635/4000 [8:53:59<10:41:00, 28.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error code: 429 - {\"message\": \"You have reached the rate limit specific to this model meta-llama/Llama-3.3-70B-Instruct-Turbo-Free. The maximum rate limit for this model is 6.0 queries and 60000 tokens per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\", \"type_\": \"model_rate_limit\"}. Retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "éæ—…éŠé¡å‹ç°¡è¨Šæ‰¹é‡æ¯”è¼ƒé€²åº¦: 4734it [17:07:45, 120.68s/it]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error code: 429 - {\"message\": \"You have reached the rate limit specific to this model meta-llama/Llama-3.3-70B-Instruct-Turbo-Free. The maximum rate limit for this model is 6.0 queries and 60000 tokens per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\", \"type_\": \"model_rate_limit\"}. Retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "éæ—…éŠé¡å‹ç°¡è¨Šæ‰¹é‡æ¯”è¼ƒé€²åº¦: 4737it [17:10:34, 72.95s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error code: 429 - {\"message\": \"You have reached the rate limit specific to this model meta-llama/Llama-3.3-70B-Instruct-Turbo-Free. The maximum rate limit for this model is 6.0 queries and 60000 tokens per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\", \"type_\": \"model_rate_limit\"}. Retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "éæ—…éŠé¡å‹ç°¡è¨Šæ‰¹é‡æ¯”è¼ƒé€²åº¦: 4839it [17:51:33, 46.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error code: 429 - {\"message\": \"You have reached the rate limit specific to this model meta-llama/Llama-3.3-70B-Instruct-Turbo-Free. The maximum rate limit for this model is 6.0 queries and 60000 tokens per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\", \"type_\": \"model_rate_limit\"}. Retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "éæ—…éŠé¡å‹ç°¡è¨Šæ‰¹é‡æ¯”è¼ƒé€²åº¦: 5107it [20:01:55, 119.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error code: 503 - The server is overloaded or not ready yet.. Retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "éæ—…éŠé¡å‹ç°¡è¨Šæ‰¹é‡æ¯”è¼ƒé€²åº¦: 5808it [25:27:24, 34.00s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error code: 429 - {\"message\": \"You have reached the rate limit specific to this model meta-llama/Llama-3.3-70B-Instruct-Turbo-Free. The maximum rate limit for this model is 6.0 queries and 60000 tokens per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\", \"type_\": \"model_rate_limit\"}. Retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "éæ—…éŠé¡å‹ç°¡è¨Šæ‰¹é‡æ¯”è¼ƒé€²åº¦: 5813it [25:32:46, 57.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error code: 503 - The server is overloaded or not ready yet.. Retrying in 60 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "éæ—…éŠé¡å‹ç°¡è¨Šæ‰¹é‡æ¯”è¼ƒé€²åº¦: 7113it [32:57:29, 10.09s/it] "
     ]
    }
   ],
   "source": [
    "# åŸ·è¡Œåˆ†é¡é…å°ç­–ç•¥\n",
    "print(\"é–‹å§‹åŸ·è¡Œåˆ†é¡é…å°ç­–ç•¥...\")\n",
    "\n",
    "# ========== ç¬¬ä¸€éšæ®µï¼šæ—…éŠé¡å‹ç°¡è¨Šé…å° ==========\n",
    "print(f\"\\nğŸ–ï¸ ç¬¬ä¸€éšæ®µï¼šæ—…éŠé¡å‹ç°¡è¨Šé…å°\")\n",
    "travel_pairs = find_same_type_pairs(\n",
    "    df_subset=travel_sms,\n",
    "    max_pairs=400,  # ç›¡é‡æ‰¾å‡ºæ‰€æœ‰æ—…éŠé¡å‹çš„ç›¸ä¼¼å°\n",
    "    description=\"æ—…éŠé¡å‹ç°¡è¨Š\"\n",
    ")\n",
    "\n",
    "# ========== ç¬¬äºŒéšæ®µï¼šéæ—…éŠé¡å‹ç°¡è¨Šé…å° ==========\n",
    "print(f\"\\nğŸ¢ ç¬¬äºŒéšæ®µï¼šéæ—…éŠé¡å‹ç°¡è¨Šé…å°\")\n",
    "\n",
    "# å¾éæ—…éŠé¡å‹ä¸­æ¡æ¨£800ç­†\n",
    "NON_TRAVEL_SAMPLE_SIZE = 40000\n",
    "if len(non_travel_sms) > NON_TRAVEL_SAMPLE_SIZE:\n",
    "    non_travel_sample = non_travel_sms.sample(n=NON_TRAVEL_SAMPLE_SIZE, random_state=42)\n",
    "    print(f\"å¾ {len(non_travel_sms)} ç­†éæ—…éŠç°¡è¨Šä¸­æ¡æ¨£ {NON_TRAVEL_SAMPLE_SIZE} ç­†\")\n",
    "else:\n",
    "    non_travel_sample = non_travel_sms\n",
    "    print(f\"ä½¿ç”¨å…¨éƒ¨ {len(non_travel_sms)} ç­†éæ—…éŠç°¡è¨Š\")\n",
    "\n",
    "non_travel_pairs = find_same_type_pairs(\n",
    "    df_subset=non_travel_sample,\n",
    "    max_pairs=4000,  \n",
    "    description=\"éæ—…éŠé¡å‹ç°¡è¨Š\"\n",
    ")\n",
    "\n",
    "# ========== åˆä½µçµæœ ==========\n",
    "all_similar_pairs = travel_pairs + non_travel_pairs\n",
    "\n",
    "print(f\"\\nğŸ“Š === ç¸½çµæœ ===\")\n",
    "print(f\"æ—…éŠé¡å‹ç›¸ä¼¼å°: {len(travel_pairs)} å€‹\")\n",
    "print(f\"éæ—…éŠé¡å‹ç›¸ä¼¼å°: {len(non_travel_pairs)} å€‹\")\n",
    "print(f\"ç¸½ç›¸ä¼¼å°: {len(all_similar_pairs)} å€‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606ec018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ—…éŠé¡å‹ç›¸ä¼¼å°å·²ä¿å­˜åˆ°: sms_travel_similar_pairs.csv (400 å°)\n",
      "éæ—…éŠé¡å‹ç›¸ä¼¼å°å·²ä¿å­˜åˆ°: sms_non_travel_similar_pairs.csv (2000 å°)\n",
      "å®Œæ•´çµæœå·²ä¿å­˜åˆ°: sms_all_similar_pairs_with_content.csv (2400 å°)\n",
      "ç°¡æ½”ç‰ˆçµæœå·²ä¿å­˜åˆ°: sms_all_similar_pairs.csv\n",
      "\n",
      "çµæœé è¦½:\n",
      "     ç°¡è¨Šid  ç›¸ä¼¼ç°¡è¨Šid  é¡å‹\n",
      "0  254284   44913  æ—…éŠ\n",
      "1   73455   30973  æ—…éŠ\n",
      "2   73455  358354  æ—…éŠ\n",
      "3   30973  358354  æ—…éŠ\n",
      "4  134878   13188  æ—…éŠ\n",
      "5  296772   37977  æ—…éŠ\n",
      "6  296772  384483  æ—…éŠ\n",
      "7   37977  384483  æ—…éŠ\n",
      "8  303927  221770  æ—…éŠ\n",
      "9  248068  411326  æ—…éŠ\n"
     ]
    }
   ],
   "source": [
    "# ä¿å­˜çµæœåˆ° CSVï¼ˆåˆ†é¡ç‰ˆæœ¬ï¼‰\n",
    "def save_categorized_pairs_to_csv(travel_pairs: List[Tuple[int, int]], \n",
    "                                  non_travel_pairs: List[Tuple[int, int]], \n",
    "                                  df: pd.DataFrame):\n",
    "    \"\"\"ä¿å­˜åˆ†é¡çš„ç›¸ä¼¼å°åˆ° CSV æª”æ¡ˆ\"\"\"\n",
    "    \n",
    "    # ä¿å­˜æ—…éŠé¡å‹ç›¸ä¼¼å°\n",
    "    if travel_pairs:\n",
    "        travel_df = pd.DataFrame(travel_pairs, columns=[\"ç°¡è¨Šid\", \"ç›¸ä¼¼ç°¡è¨Šid\"])\n",
    "        travel_df['é¡å‹'] = 'æ—…éŠ'\n",
    "        travel_df.to_csv(\"sms_travel_similar_pairs.csv\", index=False, encoding='utf-8-sig')\n",
    "        print(f\"æ—…éŠé¡å‹ç›¸ä¼¼å°å·²ä¿å­˜åˆ°: sms_travel_similar_pairs.csv ({len(travel_pairs)} å°)\")\n",
    "    \n",
    "    # ä¿å­˜éæ—…éŠé¡å‹ç›¸ä¼¼å°\n",
    "    if non_travel_pairs:\n",
    "        non_travel_df = pd.DataFrame(non_travel_pairs, columns=[\"ç°¡è¨Šid\", \"ç›¸ä¼¼ç°¡è¨Šid\"])\n",
    "        non_travel_df['é¡å‹'] = 'éæ—…éŠ'\n",
    "        non_travel_df.to_csv(\"sms_non_travel_similar_pairs.csv\", index=False, encoding='utf-8-sig')\n",
    "        print(f\"éæ—…éŠé¡å‹ç›¸ä¼¼å°å·²ä¿å­˜åˆ°: sms_non_travel_similar_pairs.csv ({len(non_travel_pairs)} å°)\")\n",
    "    \n",
    "    # ä¿å­˜åˆä½µçµæœ\n",
    "    all_pairs = travel_pairs + non_travel_pairs\n",
    "    if all_pairs:\n",
    "        # å‰µå»ºå®Œæ•´çš„çµæœ DataFrame\n",
    "        travel_df_full = pd.DataFrame(travel_pairs, columns=[\"ç°¡è¨Šid\", \"ç›¸ä¼¼ç°¡è¨Šid\"])\n",
    "        travel_df_full['é¡å‹'] = 'æ—…éŠ'\n",
    "        \n",
    "        non_travel_df_full = pd.DataFrame(non_travel_pairs, columns=[\"ç°¡è¨Šid\", \"ç›¸ä¼¼ç°¡è¨Šid\"])\n",
    "        non_travel_df_full['é¡å‹'] = 'éæ—…éŠ'\n",
    "        \n",
    "        combined_df = pd.concat([travel_df_full, non_travel_df_full], ignore_index=True)\n",
    "        \n",
    "        # æ·»åŠ ç°¡è¨Šå…§å®¹ï¼ˆå¯é¸ï¼‰\n",
    "        def get_sms_content(sms_id):\n",
    "            content = df[df['sms_id'] == sms_id]['sms_body']\n",
    "            return content.iloc[0] if len(content) > 0 else \"æœªæ‰¾åˆ°\"\n",
    "        \n",
    "        combined_df['ç°¡è¨Šå…§å®¹'] = combined_df['ç°¡è¨Šid'].apply(get_sms_content)\n",
    "        combined_df['ç›¸ä¼¼ç°¡è¨Šå…§å®¹'] = combined_df['ç›¸ä¼¼ç°¡è¨Šid'].apply(get_sms_content)\n",
    "        \n",
    "        # ä¿å­˜å®Œæ•´çµæœ\n",
    "        combined_df.to_csv(\"sms_all_similar_pairs_with_content.csv\", index=False, encoding='utf-8-sig')\n",
    "        print(f\"å®Œæ•´çµæœå·²ä¿å­˜åˆ°: sms_all_similar_pairs_with_content.csv ({len(all_pairs)} å°)\")\n",
    "        \n",
    "        # ä¿å­˜ç°¡æ½”ç‰ˆæœ¬ï¼ˆåªæœ‰IDï¼‰\n",
    "        simple_df = combined_df[['ç°¡è¨Šid', 'ç›¸ä¼¼ç°¡è¨Šid', 'é¡å‹']]\n",
    "        simple_df.to_csv(\"sms_all_similar_pairs.csv\", index=False, encoding='utf-8-sig')\n",
    "        print(f\"ç°¡æ½”ç‰ˆçµæœå·²ä¿å­˜åˆ°: sms_all_similar_pairs.csv\")\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ä¿å­˜çµæœ\n",
    "if travel_pairs or non_travel_pairs:\n",
    "    result_df = save_categorized_pairs_to_csv(travel_pairs, non_travel_pairs, df)\n",
    "    if result_df is not None:\n",
    "        print(\"\\nçµæœé è¦½:\")\n",
    "        print(result_df[['ç°¡è¨Šid', 'ç›¸ä¼¼ç°¡è¨Šid', 'é¡å‹']].head(10))\n",
    "else:\n",
    "    print(\"æœªæ‰¾åˆ°ä»»ä½•ç›¸ä¼¼å°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "142ad7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== åˆ†é¡ç›¸ä¼¼å°åˆ†æçµæœ ===\n",
      "æ—…éŠé¡å‹ç›¸ä¼¼å°: 400 å€‹\n",
      "éæ—…éŠé¡å‹ç›¸ä¼¼å°: 2000 å€‹\n",
      "ç¸½ç›¸ä¼¼å°: 2400 å€‹\n",
      "\n",
      "ğŸ–ï¸ === æ—…éŠé¡å‹ç›¸ä¼¼å°æ¨£æœ¬ ===\n",
      "\n",
      "--- æ—…éŠç›¸ä¼¼å° 1 ---\n",
      "ID 37977: æ‚¨çš„å‡ºåœ‹æ—…éŠè¨ˆç•«å·²ç¶“å®Œæˆå®‰æ’ï¼Œæ‰€æœ‰çš„ç´°ç¯€éƒ½å·²ç¶“ç¢ºèªï¼æ©Ÿç¥¨å’Œä½å®¿å·²ç¶“é è¨‚æˆåŠŸï¼Œæ¥é€æœå‹™ä¹Ÿå·²ç¶“å®‰æ’å¥½ã€‚è«‹åœ¨å‡ºç™¼å‰å†æ¬¡æª¢æŸ¥æ‚¨çš„è­·ç…§æœ‰æ•ˆæœŸï¼Œä¸¦ç¢ºèªæ‰€æœ‰æ—…è¡Œè³‡æ–™ã€‚å¦‚æœæœ‰ä»»ä½•å•é¡Œæˆ–éœ€æ±‚ï¼Œéš¨æ™‚è¯çµ¡æˆ‘å€‘ï¼Œæˆ‘å€‘å°‡å…¨ç¨‹ç‚ºæ‚¨æä¾›å”åŠ©ï¼Œç¥æ‚¨æ—…é€”æ„‰å¿«ï¼\n",
      "ID 384483: æ‚¨çš„å‡ºåœ‹æ—…ç¨‹å·²é †åˆ©å®‰æ’ï¼Œæ„Ÿè¬æ‚¨é¸æ“‡æ˜Ÿéš›æ—…è¡Œç¤¾ã€‚æˆ‘å€‘å·²ç‚ºæ‚¨é è¨‚å¥½æ©Ÿç¥¨èˆ‡ä½å®¿ï¼Œè«‹è¨˜å¾—æå‰æŠµé”æ©Ÿå ´è¾¦ç†ç™»æ©Ÿæ‰‹çºŒï¼Œä¸¦æº–å‚™å¥½è­·ç…§åŠå…¶ä»–æ—…è¡Œæ–‡ä»¶ã€‚\n",
      "--------------------------------------------------\n",
      "\n",
      "--- æ—…éŠç›¸ä¼¼å° 2 ---\n",
      "ID 333656: è¦ªæ„›çš„é¡§å®¢ï¼Œæ‚¨çš„è¥¿ç­ç‰™å‡æœŸå·²ç¶“å®‰æ’å¦¥ç•¶ã€‚è«‹ç¢ºä¿åœ¨å‡ºç™¼å‰æº–å‚™å¥½æ‰€æœ‰å¿…è¦çš„æ–‡ä»¶ï¼Œä¸¦æå‰åˆ°é”æ©Ÿå ´è¾¦ç†ç™»æ©Ÿæ‰‹çºŒã€‚å¦‚æœæ‚¨éœ€è¦ä»»ä½•å¹«åŠ©ï¼Œéš¨æ™‚è¯çµ¡æˆ‘å€‘çš„å®¢æœï¼Œç¥æ‚¨åœ¨è¥¿ç­ç‰™åº¦éä¸€æ®µç¾å¥½çš„æ™‚å…‰ã€‚\n",
      "ID 60433: å¥‡é‡æ—…è¡Œç¤¾é€šçŸ¥ï¼Œæ‚¨çš„æ¾³å¤§åˆ©äºæ‚‰å°¼èˆ‡å¤§å ¡ç¤ä¹‹æ—…å·²ç¶“å®Œæˆå®‰æ’ï¼Œæ‚¨å°‡æ–¼2024å¹´7æœˆ25æ—¥å‡ºç™¼ï¼Œè¡Œç¨‹åŒ…æ‹¬éŠè¦½æ‚‰å°¼æ­ŒåŠ‡é™¢ã€æµ·æ¸¯å¤§æ©‹ï¼Œä¸¦å‰å¾€å¤§å ¡ç¤é«”é©—æµ®æ½›ã€‚è«‹æ”œå¸¶èˆ’é©çš„é‹å­å’Œæ³³è¡£ï¼Œç‚ºæ‚¨æº–å‚™ä¸€å ´é›£å¿˜çš„æµ·æ´‹ä¹‹æ—…ã€‚\n",
      "--------------------------------------------------\n",
      "\n",
      "--- æ—…éŠç›¸ä¼¼å° 3 ---\n",
      "ID 55667: æ‚¨çš„æ‚‰å°¼ä¹‹æ—…å·²ç¢ºèªï¼Œå‡ºç™¼æ™‚é–“ç‚º2024å¹´8æœˆ5æ—¥ï¼Œèˆªç­ç·¨è™ŸCX-102ï¼Œè«‹æº–æ™‚ç™»æ©Ÿï¼Œç¥æ‚¨æœ‰å€‹æ„‰å¿«çš„å‡æœŸï¼\n",
      "ID 370056: ã€æ—…å¤©ä¸‹ã€‘æ„Ÿè¬æ‚¨é¸æ“‡æˆ‘å€‘çš„æœå‹™ï¼æ‚¨çš„æ©Ÿç¥¨èˆ‡ä½å®¿å·²ç¶“ç¢ºèªã€‚è«‹æ³¨æ„å‡ºç™¼å‰çš„æº–å‚™äº‹é …ï¼Œå¦‚æœ‰ä»»ä½•å•é¡Œï¼Œè«‹éš¨æ™‚è¯çµ¡æˆ‘å€‘çš„å®¢æœéƒ¨é–€ï¼Œæˆ‘å€‘æœƒæä¾›å…¨æ–¹ä½å”åŠ©ã€‚\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ¢ === éæ—…éŠé¡å‹ç›¸ä¼¼å°æ¨£æœ¬ ===\n",
      "\n",
      "--- éæ—…éŠç›¸ä¼¼å° 1 ---\n",
      "ID 167818: æ„Ÿè¬æ‚¨ä½¿ç”¨ä¾†ç¦éŠ€è¡Œçš„ç·šä¸Šæ”¯ä»˜æœå‹™ï¼Œæ‚¨çš„ä»˜æ¬¾å·²é †åˆ©å®Œæˆã€‚è‹¥æœ‰å…¶ä»–ä»˜æ¬¾æˆ–å¸³å‹™å•é¡Œï¼Œè«‹ä¸åèˆ‡æˆ‘å€‘çš„å®¢æœåœ˜éšŠè¯ç¹«ï¼Œæˆ‘å€‘å°‡ç«­èª ç‚ºæ‚¨æœå‹™ã€‚\n",
      "ID 409228: æ‚¨å¥½ï¼Œæ„Ÿè¬æ‚¨åœ¨æ˜Ÿå…‰è³¼ç‰©ç¶²çš„è¨‚è³¼ï¼æ‚¨çš„åŒ…è£¹å·²é †åˆ©å‡ºè²¨ï¼Œé è¨ˆæ–¼3å€‹å·¥ä½œæ—¥å…§é€é”æ‚¨æŒ‡å®šçš„åœ°å€ã€‚è«‹æ³¨æ„æŸ¥æ”¶ï¼Œè‹¥é‡åˆ°é…é€å•é¡Œï¼Œè«‹å³æ™‚è¯çµ¡æˆ‘å€‘çš„å®¢æœå°ˆç·šï¼Œæˆ‘å€‘å°‡ç‚ºæ‚¨æä¾›å”åŠ©ã€‚è‹¥é¸æ“‡è¶…å•†å–è²¨ï¼Œè«‹åœ¨ä¸‰å¤©å…§é ˜å–ï¼ŒéæœŸå°‡é€€å›ï¼Œè¬è¬æ‚¨çš„åˆä½œã€‚\n",
      "--------------------------------------------------\n",
      "\n",
      "--- éæ—…éŠç›¸ä¼¼å° 2 ---\n",
      "ID 27786: é³³å‡°é‡‘æ¥­é€šçŸ¥ï¼šæ„Ÿè¬æ‚¨ä½¿ç”¨æœ¬å…¬å¸è²¸æ¬¾æœå‹™ï¼ŒæƒŸè¿‘æœŸæœ‰æœŸæ¬¾é²å»¶ç´€éŒ„å°šæœªè™•ç†ï¼Œè«‹ç›¡é€Ÿæ–¼æœ¬é€±å…§å®Œæˆè£œç¹³ã€‚è‹¥ä»æœ‰å›°é›£å¯ç”³è«‹å¯¬é™æˆ–é‡å¯©æ–¹æ¡ˆï¼Œè©³æ´½å®¢æœå”åŠ©è©•ä¼°ã€‚\n",
      "ID 114315: ä¸­æ³°èè³‡é—œå¿ƒæ‚¨ï¼šæ‚¨è»Šè²¸å¸³è™Ÿæœ«å››ç¢¼0912è‡ª5/25èµ·å·²é€¾æœŸï¼Œè«‹æ–¼ä¸‰æ—¥å…§ç¹³æ¸…ï¼Œé¿å…ç”¢ç”Ÿæ»¯ç´é‡‘èˆ‡ä¿¡ç”¨å—æã€‚è‹¥å·²ç¹³æ¬¾ï¼Œè«‹å¿½ç•¥æœ¬è¨Šæ¯ã€‚\n",
      "--------------------------------------------------\n",
      "\n",
      "--- éæ—…éŠç›¸ä¼¼å° 3 ---\n",
      "ID 85767: å°Šæ•¬çš„å®¢æˆ¶ï¼Œé€™æ˜¯æ‚¨åœ¨å°ç£éŠ€è¡Œçš„å‚¬æ¬¾é€šçŸ¥ã€‚æ‚¨çš„è²¸æ¬¾æ¬¾é …å°šæœªç¹³ç´ï¼Œç‚ºäº†é¿å…å½±éŸ¿æ‚¨çš„ä¿¡ç”¨è¨˜éŒ„ï¼Œè«‹å„˜é€Ÿç¹³æ¬¾ã€‚å¦‚æœæ‚¨å·²ç¹³æ¬¾ï¼Œè«‹å¿½ç•¥æ­¤è¨Šæ¯ã€‚å¦‚éœ€å”å•†é‚„æ¬¾è¨ˆåŠƒæˆ–æœ‰ä»»ä½•ç–‘å•ï¼Œè«‹éš¨æ™‚è¯ç¹«æˆ‘å€‘çš„å®¢æœéƒ¨é–€ã€‚\n",
      "ID 363448: æ‚¨å¥½ï¼Œé€™æ˜¯ä¾†è‡ªæ°¸è±éŠ€è¡Œçš„ä»˜æ¬¾æé†’ã€‚æ‚¨çš„è²¸æ¬¾å¸³å–®é‡‘é¡å°šæœªç¹³æ¸…ï¼Œè«‹å„˜é€Ÿå®Œæˆç¹³æ¬¾ã€‚è‹¥æœ‰ä»»ä½•å•é¡Œï¼Œè«‹éš¨æ™‚è¯çµ¡æˆ‘å€‘çš„å®¢æœå°ˆç·šï¼Œæˆ‘å€‘å°‡ç‚ºæ‚¨æä¾›å°ˆæ¥­å”åŠ©ï¼Œè¬è¬ï¼\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ“ˆ === çµ±è¨ˆåˆ†æ ===\n",
      "æ—…éŠç°¡è¨Šè¦†è“‹ç‡: 51.1% (511/1000)\n",
      "éæ—…éŠç°¡è¨Šè¦†è“‹ç‡: 19.3% (3858/20000)\n"
     ]
    }
   ],
   "source": [
    "# é©—è­‰å’Œåˆ†æçµæœï¼ˆåˆ†é¡ç‰ˆæœ¬ï¼‰\n",
    "def analyze_categorized_pairs(travel_pairs: List[Tuple[int, int]], \n",
    "                             non_travel_pairs: List[Tuple[int, int]], \n",
    "                             df: pd.DataFrame):\n",
    "    \"\"\"åˆ†æåˆ†é¡ç›¸ä¼¼å°çš„è³ªé‡\"\"\"\n",
    "    \n",
    "    print(f\"=== åˆ†é¡ç›¸ä¼¼å°åˆ†æçµæœ ===\")\n",
    "    print(f\"æ—…éŠé¡å‹ç›¸ä¼¼å°: {len(travel_pairs)} å€‹\")\n",
    "    print(f\"éæ—…éŠé¡å‹ç›¸ä¼¼å°: {len(non_travel_pairs)} å€‹\")\n",
    "    print(f\"ç¸½ç›¸ä¼¼å°: {len(travel_pairs) + len(non_travel_pairs)} å€‹\")\n",
    "    \n",
    "    # åˆ†ææ—…éŠé¡å‹ç›¸ä¼¼å°\n",
    "    if travel_pairs:\n",
    "        print(f\"\\nğŸ–ï¸ === æ—…éŠé¡å‹ç›¸ä¼¼å°æ¨£æœ¬ ===\")\n",
    "        import random\n",
    "        sample_size = min(3, len(travel_pairs))\n",
    "        sample_pairs = random.sample(travel_pairs, sample_size)\n",
    "        \n",
    "        for i, (id1, id2) in enumerate(sample_pairs, 1):\n",
    "            sms1 = df[df['sms_id'] == id1]['sms_body'].iloc[0]\n",
    "            sms2 = df[df['sms_id'] == id2]['sms_body'].iloc[0]\n",
    "            \n",
    "            print(f\"\\n--- æ—…éŠç›¸ä¼¼å° {i} ---\")\n",
    "            print(f\"ID {id1}: {sms1}\")\n",
    "            print(f\"ID {id2}: {sms2}\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # åˆ†æéæ—…éŠé¡å‹ç›¸ä¼¼å°\n",
    "    if non_travel_pairs:\n",
    "        print(f\"\\nğŸ¢ === éæ—…éŠé¡å‹ç›¸ä¼¼å°æ¨£æœ¬ ===\")\n",
    "        import random\n",
    "        sample_size = min(3, len(non_travel_pairs))\n",
    "        sample_pairs = random.sample(non_travel_pairs, sample_size)\n",
    "        \n",
    "        for i, (id1, id2) in enumerate(sample_pairs, 1):\n",
    "            sms1 = df[df['sms_id'] == id1]['sms_body'].iloc[0]\n",
    "            sms2 = df[df['sms_id'] == id2]['sms_body'].iloc[0]\n",
    "            \n",
    "            print(f\"\\n--- éæ—…éŠç›¸ä¼¼å° {i} ---\")\n",
    "            print(f\"ID {id1}: {sms1}\")\n",
    "            print(f\"ID {id2}: {sms2}\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # çµ±è¨ˆåˆ†æ\n",
    "    if travel_pairs or non_travel_pairs:\n",
    "        print(f\"\\nğŸ“ˆ === çµ±è¨ˆåˆ†æ ===\")\n",
    "        \n",
    "        # æ—…éŠé¡å‹è¦†è“‹ç‡\n",
    "        if travel_pairs:\n",
    "            travel_ids_in_pairs = set()\n",
    "            for id1, id2 in travel_pairs:\n",
    "                travel_ids_in_pairs.add(id1)\n",
    "                travel_ids_in_pairs.add(id2)\n",
    "            travel_coverage = len(travel_ids_in_pairs) / len(travel_sms) * 100\n",
    "            print(f\"æ—…éŠç°¡è¨Šè¦†è“‹ç‡: {travel_coverage:.1f}% ({len(travel_ids_in_pairs)}/{len(travel_sms)})\")\n",
    "        \n",
    "        # éæ—…éŠé¡å‹è¦†è“‹ç‡\n",
    "        if non_travel_pairs:\n",
    "            non_travel_ids_in_pairs = set()\n",
    "            for id1, id2 in non_travel_pairs:\n",
    "                non_travel_ids_in_pairs.add(id1)\n",
    "                non_travel_ids_in_pairs.add(id2)\n",
    "            non_travel_coverage = len(non_travel_ids_in_pairs) / len(non_travel_sample) * 100\n",
    "            print(f\"éæ—…éŠç°¡è¨Šè¦†è“‹ç‡: {non_travel_coverage:.1f}% ({len(non_travel_ids_in_pairs)}/{len(non_travel_sample)})\")\n",
    "\n",
    "# åˆ†æçµæœ\n",
    "if travel_pairs or non_travel_pairs:\n",
    "    analyze_categorized_pairs(travel_pairs, non_travel_pairs, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e55acf1",
   "metadata": {},
   "source": [
    "## negative gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67299ab",
   "metadata": {},
   "source": [
    "## è² æ¨£æœ¬ç”Ÿæˆ\n",
    "\n",
    "ä½¿ç”¨ LLM æ‰¹é‡è©•ä¼°ï¼ˆ5é¸2ï¼‰æ–¹å¼ç”Ÿæˆé«˜è³ªé‡è² æ¨£æœ¬ã€‚\n",
    "\n",
    "### ğŸ”§ åƒæ•¸æ§åˆ¶\n",
    "\n",
    "- **è² æ¨£æœ¬æ•¸é‡**ï¼šå¯ä»¥åœ¨ä¸‹é¢çš„ cell ä¸­èª¿æ•´ `NEGATIVE_PAIRS_COUNT` è®Šæ•¸\n",
    "- **æ‰¹é‡ç­–ç•¥**ï¼šæ¯5å€‹å€™é¸å°é¸å‡ºæœ€ä¸åŒ¹é…çš„2å€‹\n",
    "- **é…å°ç­–ç•¥**ï¼š60% è·¨é¡å‹ + 40% åŒé¡å‹éš¨æ©Ÿé…å°\n",
    "- **è³ªé‡ä¿è­‰**ï¼šLLM è©•ä¼°ç¢ºä¿è² æ¨£æœ¬çœŸæ­£ä¸åŒ¹é…\n",
    "\n",
    "### ğŸ’¡ å»ºè­°æ•¸é‡\n",
    "\n",
    "- **å°è¦æ¨¡æ¸¬è©¦**ï¼š50-100 å€‹è² æ¨£æœ¬\n",
    "- **ä¸­ç­‰è¦æ¨¡**ï¼š500-1000 å€‹è² æ¨£æœ¬  \n",
    "- **å¤§è¦æ¨¡è¨“ç·´**ï¼š1000+ å€‹è² æ¨£æœ¬\n",
    "\n",
    "æ ¹æ“šæ‚¨çš„æ­£æ¨£æœ¬æ•¸é‡ï¼Œå»ºè­°è² æ¨£æœ¬æ•¸é‡ç‚ºæ­£æ¨£æœ¬çš„ 0.5-2 å€ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4e04b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== è² æ¨£æœ¬æ•¸é‡é…ç½®é¸é … ===\n",
      "é è¨­é…ç½®é¸é …:\n",
      "  æ¸¬è©¦: 50 å€‹è² æ¨£æœ¬\n",
      "  å°è¦æ¨¡: 200 å€‹è² æ¨£æœ¬\n",
      "  ä¸­è¦æ¨¡: 500 å€‹è² æ¨£æœ¬\n",
      "  å¤§è¦æ¨¡: 1000 å€‹è² æ¨£æœ¬\n",
      "  è¶…å¤§è¦æ¨¡: 2000 å€‹è² æ¨£æœ¬\n",
      "\n",
      "âœ… ä½¿ç”¨è‡ªå®šç¾©é…ç½®: 1200 å€‹è² æ¨£æœ¬\n",
      "\n",
      "ğŸ’¡ æç¤º: è«‹å…ˆåŸ·è¡Œæ­£æ¨£æœ¬ç”Ÿæˆä»¥ç²å¾—æ›´æº–ç¢ºçš„å»ºè­°\n",
      "\n",
      "ğŸš€ æº–å‚™ç”Ÿæˆ 1200 å€‹è² æ¨£æœ¬...\n"
     ]
    }
   ],
   "source": [
    "# è² æ¨£æœ¬æ•¸é‡é…ç½® - å¿«é€Ÿé¸æ“‡\n",
    "print(\"=== è² æ¨£æœ¬æ•¸é‡é…ç½®é¸é … ===\")\n",
    "\n",
    "# ğŸ”§ é¸æ“‡ä¸€å€‹é è¨­é…ç½®ï¼Œæˆ–è‡ªå®šç¾©æ•¸é‡\n",
    "config_options = {\n",
    "    \"æ¸¬è©¦\": 50,\n",
    "    \"å°è¦æ¨¡\": 200, \n",
    "    \"ä¸­è¦æ¨¡\": 500,\n",
    "    \"å¤§è¦æ¨¡\": 1000,\n",
    "    \"è¶…å¤§è¦æ¨¡\": 2000\n",
    "}\n",
    "\n",
    "print(\"é è¨­é…ç½®é¸é …:\")\n",
    "for name, count in config_options.items():\n",
    "    print(f\"  {name}: {count} å€‹è² æ¨£æœ¬\")\n",
    "\n",
    "# ğŸ¯ åœ¨é€™è£¡é¸æ“‡æ‚¨æƒ³è¦çš„é…ç½®\n",
    "#SELECTED_CONFIG = \"ä¸­è¦æ¨¡\"  # ğŸ“ ä¿®æ”¹é€™è£¡é¸æ“‡ä¸åŒé…ç½®\n",
    "# æˆ–è€…ç›´æ¥è¨­å®šè‡ªå®šç¾©æ•¸é‡\n",
    "CUSTOM_NEGATIVE_COUNT = 1200  # ğŸ“ å–æ¶ˆè¨»é‡‹ä¸¦è¨­å®šè‡ªå®šç¾©æ•¸é‡\n",
    "\n",
    "# æ‡‰ç”¨é…ç½®\n",
    "if 'CUSTOM_NEGATIVE_COUNT' in locals():\n",
    "    NEGATIVE_PAIRS_COUNT = CUSTOM_NEGATIVE_COUNT\n",
    "    print(f\"\\nâœ… ä½¿ç”¨è‡ªå®šç¾©é…ç½®: {NEGATIVE_PAIRS_COUNT} å€‹è² æ¨£æœ¬\")\n",
    "elif SELECTED_CONFIG in config_options:\n",
    "    NEGATIVE_PAIRS_COUNT = config_options[SELECTED_CONFIG]\n",
    "    print(f\"\\nâœ… ä½¿ç”¨ '{SELECTED_CONFIG}' é…ç½®: {NEGATIVE_PAIRS_COUNT} å€‹è² æ¨£æœ¬\")\n",
    "else:\n",
    "    NEGATIVE_PAIRS_COUNT = 500  # é è¨­å€¼\n",
    "    print(f\"\\nâš ï¸  é…ç½®ç„¡æ•ˆï¼Œä½¿ç”¨é è¨­å€¼: {NEGATIVE_PAIRS_COUNT} å€‹è² æ¨£æœ¬\")\n",
    "\n",
    "# æ ¹æ“šç¾æœ‰æ­£æ¨£æœ¬æ•¸é‡çµ¦å‡ºå»ºè­°\n",
    "if 'travel_pairs' in locals() and 'non_travel_pairs' in locals():\n",
    "    total_positive = len(travel_pairs) + len(non_travel_pairs)\n",
    "    ratio = NEGATIVE_PAIRS_COUNT / total_positive if total_positive > 0 else 0\n",
    "    \n",
    "    print(f\"\\nğŸ“Š === æ¨£æœ¬å¹³è¡¡åˆ†æ ===\")\n",
    "    print(f\"æ­£æ¨£æœ¬ç¸½æ•¸: {total_positive}\")\n",
    "    print(f\"é è¨ˆè² æ¨£æœ¬: {NEGATIVE_PAIRS_COUNT}\")\n",
    "    print(f\"è² æ­£æ¯”ä¾‹: {ratio:.2f} (è² æ¨£æœ¬/æ­£æ¨£æœ¬)\")\n",
    "    \n",
    "    if ratio < 0.5:\n",
    "        print(\"ğŸ’¡ å»ºè­°: è² æ¨£æœ¬è¼ƒå°‘ï¼Œå¯èƒ½éœ€è¦å¢åŠ ä»¥å¹³è¡¡æ•¸æ“šé›†\")\n",
    "    elif ratio > 2.0:\n",
    "        print(\"ğŸ’¡ å»ºè­°: è² æ¨£æœ¬è¼ƒå¤šï¼Œå¯èƒ½å½±éŸ¿è¨“ç·´æ•ˆç‡\")\n",
    "    else:\n",
    "        print(\"âœ… è² æ­£æ¨£æœ¬æ¯”ä¾‹åˆç†\")\n",
    "else:\n",
    "    print(\"\\nğŸ’¡ æç¤º: è«‹å…ˆåŸ·è¡Œæ­£æ¨£æœ¬ç”Ÿæˆä»¥ç²å¾—æ›´æº–ç¢ºçš„å»ºè­°\")\n",
    "\n",
    "print(f\"\\nğŸš€ æº–å‚™ç”Ÿæˆ {NEGATIVE_PAIRS_COUNT} å€‹è² æ¨£æœ¬...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f630544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é–‹å§‹ç”Ÿæˆé«˜è³ªé‡è² æ¨£æœ¬...\n",
      "ğŸ¯ ç›®æ¨™ç”Ÿæˆ: 1200 å€‹è² æ¨£æœ¬å°\n",
      "ğŸ“… é ä¼°è™•ç†æ™‚é–“: 40.0 åˆ†é˜ (600 æ‰¹æ¬¡, 600 APIèª¿ç”¨)\n",
      "=== è² æ¨£æœ¬ç”Ÿæˆç­–ç•¥ ===\n",
      "æ—…éŠé¡å‹ç°¡è¨Š: 1000 ç­†\n",
      "éæ—…éŠé¡å‹ç°¡è¨Š: 208481 ç­†\n",
      "ç›®æ¨™ç”Ÿæˆè² æ¨£æœ¬å°: 1200 å€‹\n",
      "\n",
      "é–‹å§‹ç”Ÿæˆè² æ¨£æœ¬ï¼Œé è¨ˆéœ€è¦ 600 æ‰¹æ¬¡è™•ç†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è² æ¨£æœ¬ç”Ÿæˆé€²åº¦:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1192/1200 [1:04:45<00:26,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è² æ¨£æœ¬ç”Ÿæˆå®Œæˆï¼ç¸½å…±ç”Ÿæˆ 1192 å€‹è² æ¨£æœ¬å°\n",
      "\n",
      "ğŸ“Š === è² æ¨£æœ¬ç”Ÿæˆçµæœ ===\n",
      "âœ… æˆåŠŸç”Ÿæˆ: 1192 å€‹è² æ¨£æœ¬å°\n",
      "ğŸ¯ ç›®æ¨™æ•¸é‡: 1200\n",
      "ğŸ“ˆ å®Œæˆç‡: 99.3%\n",
      "âš ï¸  çŸ­ç¼º 8 å€‹è² æ¨£æœ¬ï¼Œå¯èƒ½æ˜¯ç”±æ–¼APIé™åˆ¶æˆ–æ•¸æ“šé™åˆ¶\n",
      "ğŸ’¡ å»ºè­°: å¯ä»¥é‡æ–°åŸ·è¡Œæ­¤cellæˆ–èª¿æ•´æ‰¹æ¬¡å¤§å°\n",
      "\n",
      "=== è² æ¨£æœ¬è³ªé‡åˆ†æ ===\n",
      "è·¨é¡å‹è² æ¨£æœ¬: 1183 å€‹ (99.2%)\n",
      "åŒé¡å‹è² æ¨£æœ¬: 9 å€‹ (0.8%)\n",
      "\n",
      "=== è² æ¨£æœ¬ä¾‹å­ ===\n",
      "\n",
      "--- è² æ¨£æœ¬ 1 ---\n",
      "ID 418547 (label=nan): æ‚¨çš„è¶Šå—èƒ¡å¿—æ˜å¸‚èˆ‡ç¾å¥ˆæµ·ç˜ä¹‹æ—…å·²ç¢ºå®šï¼ç”±ä¸–ç•Œæ—…éŠå…¬å¸å®‰æ’çš„é€™æ¬¡è¡Œç¨‹å°‡å¸¶æ‚¨éŠè¦½èƒ¡å¿—æ˜å¸‚çš„æ­·å²æ™¯é»ï¼Œä¸¦åœ¨ç¾å¥ˆçš„æ²™ç˜ä¸Šæ”¾é¬†ã€‚è¡Œç¨‹ä¸­é‚„å®‰æ’äº†è¶Šå—çš„ç‰¹è‰²ç¾é£Ÿèˆ‡ç•¶åœ°æ–‡åŒ–é«”é©—ï¼Œè®“æ‚¨å……åˆ†æ„Ÿå—è¶Šå—çš„ç†±æƒ…ã€‚\n",
      "ID 248141 (label=nan): æ‚¨å¥½ï¼Œé€™æ˜¯ä¾†è‡ªä¸­ä¿¡éŠ€è¡Œçš„æé†’ã€‚æ‚¨çš„ä¿¡ç”¨å¡æ¬¾é …è‡³ä»Šå°šæœªç¹³ç´ï¼Œä¸¦å·²ç¶“éäº†ç¹³æ¬¾æœŸé™ã€‚ç‚ºäº†é¿å…å½±éŸ¿æ‚¨çš„ä¿¡ç”¨è¨˜éŒ„åŠç”¢ç”Ÿæ»¯ç´é‡‘ï¼Œè«‹å„˜é€Ÿç¹³ç´æ‰€æ¬ æ¬¾é …ã€‚å¦‚æœå·²ç¶“ç¹³ç´ï¼Œè«‹å¿½ç•¥æ­¤ç°¡è¨Šã€‚è‹¥æœ‰ä»»ä½•å•é¡Œï¼Œæ­¡è¿éš¨æ™‚ä¾†é›»è©¢å•ã€‚\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- è² æ¨£æœ¬ 2 ---\n",
      "ID 359686 (label=1.0): ã€å·´é»è‡ªç”±è¡Œã€‘é™æ™‚å„ªæƒ ï¼Œè®“ä½ åœ¨æµªæ¼«ä¹‹éƒ½è¼•é¬†äº«å—ç„¡æ†‚æ—…éŠï¼è¡Œç¨‹å®‰æ’ç²¾ç·»ï¼Œç¾æ­£é–‹æ”¾å ±åä¸­ï¼Œå¿«ä¾†åƒåŠ ï¼https://paristravel.com\n",
      "ID 128624 (label=nan): æ‚¨å¥½ï¼Œæ‚¨æ–¼ä¸­ä¿¡éŠ€è¡Œç”³è«‹ä¹‹å¡å‹è²¸æ¬¾è‡³ä»Šå°šæ¬ NT$15,700ï¼Œé€¾æœŸå·²ç”¢ç”Ÿé•ç´„åˆ©æ¯ï¼Œè‹¥ä»Šæ—¥æœªæ¸…å„Ÿå°‡ç”±ç¬¬ä¸‰æ–¹å‚¬æ”¶å…¬å¸è™•ç†ï¼Œè«‹å„˜é€Ÿè™•ç†ã€‚\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- è² æ¨£æœ¬ 3 ---\n",
      "ID 243587 (label=1.0): èŠå“²éŠ˜æ¨è–¦ä½¿ç”¨ã€Œæ—…åœˆç­†è¨˜APPã€è£½ä½œå€‹äººæ—…éŠè¦åŠƒè¡¨ï¼Œå¯å…±äº«è¡Œç¨‹çµ¦åŒè¡Œè€…ï¼ŒåŠŸèƒ½åŒ…å«æ›åŒ¯æé†’ã€å¤©æ°£é å ±ã€ç¥¨åˆ¸æé†’ï¼\n",
      "ID 12142 (label=nan): å°å¤§é†«é™¢å¥åº·ç®¡ç†ä¸­å¿ƒç‰¹åˆ¥æ¨å‡ºç§‹å†¬å­£è­·è‚å¥æª¢å°ˆæ¡ˆï¼Œå…§å®¹åŒ…å«è‚åŠŸèƒ½æŠ½è¡€ã€è…¹éƒ¨è¶…éŸ³æ³¢èˆ‡å°ˆæ¥­é†«å¸«è§£èªªå ±å‘Šï¼Œé ç´„å³è´ˆè‚è‡Ÿä¿å¥å°æ‰‹å†Šã€‚\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "RPM = 60                # requests/min\n",
    "WINDOW = 60             # sec\n",
    "MAX_RETRY = 3           # retries\n",
    "MODEL_ID = \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n",
    "\n",
    "# è«‹æ±‚ç´€éŒ„\n",
    "_req_log = []\n",
    "\n",
    "def _throttle():\n",
    "    \"\"\"ç°¡å–®çš„é™æµæ©Ÿåˆ¶\"\"\"\n",
    "    now = time.time()\n",
    "    # æ¸…ç†è¶…éæ™‚é–“çª—å£çš„è¨˜éŒ„\n",
    "    _req_log[:] = [t for t in _req_log if now - t < WINDOW]\n",
    "    \n",
    "    # æª¢æŸ¥æ˜¯å¦éœ€è¦ç­‰å¾…\n",
    "    if len(_req_log) >= RPM:\n",
    "        wait_time = WINDOW - (now - _req_log[0])\n",
    "        if wait_time > 0:\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    _req_log.append(now)\n",
    "\n",
    "# è² æ¨£æœ¬ç”Ÿæˆ - æ‰¾å‡ºæœ€ä¸åŒ¹é…çš„ç°¡è¨Šå°\n",
    "def generate_negative_pairs_batch(df: pd.DataFrame, max_negative_pairs: int = 1000) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆè² æ¨£æœ¬ï¼šæ‰¾å‡ºæœ€ä¸åŒ¹é…çš„ç°¡è¨Šå°\n",
    "    ä½¿ç”¨æ‰¹é‡æ¯”è¼ƒï¼Œä¸€æ¬¡çµ¦æ¨¡å‹ 5 å€‹é¸æ“‡é€²è¡Œè©•ä¼°\n",
    "    \n",
    "    ç­–ç•¥ï¼š\n",
    "    1. è·¨é¡å‹é…å°ï¼ˆæ—…éŠ vs éæ—…éŠï¼‰\n",
    "    2. åŒé¡å‹ä½†ä¸»é¡Œå·®ç•°å¤§çš„é…å°\n",
    "    3. æ‰¹é‡è©•ä¼° 5 å€‹å€™é¸å°ï¼Œé¸å‡ºæœ€ä¸åŒ¹é…çš„\n",
    "    \n",
    "    Args:\n",
    "        df: å®Œæ•´çš„ç°¡è¨Š DataFrame\n",
    "        max_negative_pairs: æœ€å¤§è² æ¨£æœ¬å°æ•¸é‡\n",
    "        \n",
    "    Returns:\n",
    "        è² æ¨£æœ¬å°çš„åˆ—è¡¨ [(id1, id2), ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    negative_pairs = []\n",
    "    \n",
    "    # åˆ†é›¢ä¸åŒé¡å‹çš„ç°¡è¨Š\n",
    "    travel_df = df[df['label'] == 1].copy()\n",
    "    non_travel_df = df[df['label'] != 1].copy()\n",
    "    \n",
    "    print(f\"=== è² æ¨£æœ¬ç”Ÿæˆç­–ç•¥ ===\")\n",
    "    print(f\"æ—…éŠé¡å‹ç°¡è¨Š: {len(travel_df)} ç­†\")\n",
    "    print(f\"éæ—…éŠé¡å‹ç°¡è¨Š: {len(non_travel_df)} ç­†\")\n",
    "    print(f\"ç›®æ¨™ç”Ÿæˆè² æ¨£æœ¬å°: {max_negative_pairs} å€‹\")\n",
    "    \n",
    "    # è² æ¨£æœ¬è©•ä¼° prompt\n",
    "    negative_evaluation_prompt = textwrap.dedent(\n",
    "        \"\"\"\n",
    "        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬ç›¸ä¼¼æ€§åˆ¤æ–·åŠ©æ‰‹ã€‚ç¾åœ¨æœ‰ 5 çµ„ç°¡è¨Šå°ï¼Œè«‹è©•ä¼°æ¯çµ„çš„ç›¸ä¼¼æ€§ï¼Œä¸¦é¸å‡ºå…¶ä¸­æœ€ä¸ç›¸ä¼¼çš„ 2 çµ„ã€‚\n",
    "\n",
    "        è©•ä¼°æ¨™æº–ï¼š\n",
    "        1. ä¸»é¡Œå·®ç•°ï¼šä¸»é¡Œè¶Šä¸åŒï¼Œè² æ¨£æœ¬è³ªé‡è¶Šå¥½\n",
    "        2. æ„åœ–å·®ç•°ï¼šç›®çš„è¶Šä¸åŒï¼Œè² æ¨£æœ¬è³ªé‡è¶Šå¥½  \n",
    "        3. å…§å®¹å·®ç•°ï¼šå…§å®¹è¶Šç„¡é—œï¼Œè² æ¨£æœ¬è³ªé‡è¶Šå¥½\n",
    "        4. èªå¢ƒå·®ç•°ï¼šä½¿ç”¨å ´æ™¯è¶Šä¸åŒï¼Œè² æ¨£æœ¬è³ªé‡è¶Šå¥½\n",
    "\n",
    "        è«‹ä»”ç´°æ¯”è¼ƒä»¥ä¸‹ 5 çµ„ç°¡è¨Šå°ï¼š\n",
    "\n",
    "        çµ„A:\n",
    "        ç°¡è¨Š1: {sms_a1}\n",
    "        ç°¡è¨Š2: {sms_a2}\n",
    "\n",
    "        çµ„B:\n",
    "        ç°¡è¨Š1: {sms_b1}\n",
    "        ç°¡è¨Š2: {sms_b2}\n",
    "\n",
    "        çµ„C:\n",
    "        ç°¡è¨Š1: {sms_c1}\n",
    "        ç°¡è¨Š2: {sms_c2}\n",
    "\n",
    "        çµ„D:\n",
    "        ç°¡è¨Š1: {sms_d1}\n",
    "        ç°¡è¨Š2: {sms_d2}\n",
    "\n",
    "        çµ„E:\n",
    "        ç°¡è¨Š1: {sms_e1}\n",
    "        ç°¡è¨Š2: {sms_e2}\n",
    "\n",
    "        è«‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼Œé¸å‡ºæœ€ä¸ç›¸ä¼¼çš„ 2 çµ„ï¼š\n",
    "        æœ€ä¸ç›¸ä¼¼çµ„1: [çµ„åˆ¥å­—æ¯]\n",
    "        æœ€ä¸ç›¸ä¼¼çµ„2: [çµ„åˆ¥å­—æ¯]\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    def evaluate_negative_batch(candidate_pairs: List[Tuple[Tuple[int, str], Tuple[int, str]]]) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        è©•ä¼°ä¸€æ‰¹å€™é¸è² æ¨£æœ¬å°ï¼Œè¿”å›æœ€ä¸åŒ¹é…çš„ 2 å€‹\n",
    "        \n",
    "        Args:\n",
    "            candidate_pairs: [((id1, content1), (id2, content2)), ...] æœ€å¤š 5 å€‹å€™é¸å°\n",
    "            \n",
    "        Returns:\n",
    "            æœ€ä¸åŒ¹é…çš„ 2 å€‹å° [(id1, id2), ...]\n",
    "        \"\"\"\n",
    "        if len(candidate_pairs) != 5:\n",
    "            return []\n",
    "        \n",
    "        # æº–å‚™ prompt åƒæ•¸\n",
    "        prompt_params = {}\n",
    "        for i, ((id1, content1), (id2, content2)) in enumerate(candidate_pairs):\n",
    "            group_letter = chr(ord('a') + i)  # a, b, c, d, e\n",
    "            prompt_params[f'sms_{group_letter}1'] = content1\n",
    "            prompt_params[f'sms_{group_letter}2'] = content2\n",
    "        \n",
    "        prompt = negative_evaluation_prompt.format(**prompt_params)\n",
    "        \n",
    "        client = get_current_together_client()\n",
    "        \n",
    "        for attempt in range(MAX_RETRY):\n",
    "            try:\n",
    "                _throttle()\n",
    "                response = client.chat.completions.create(\n",
    "                    model=MODEL_ID,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_tokens=50,\n",
    "                    temperature=0.0,\n",
    "                    stream=False,\n",
    "                )\n",
    "                \n",
    "                result = response.choices[0].message.content.strip()\n",
    "                \n",
    "                # å¼·åŒ–ç‰ˆè§£æé‚è¼¯\n",
    "                selected_pairs = []\n",
    "                lines = result.split('\\n')\n",
    "                \n",
    "                for line in lines:\n",
    "                    if 'æœ€ä¸ç›¸ä¼¼çµ„' in line and ':' in line:\n",
    "                        try:\n",
    "                            # æå–å†’è™Ÿå¾Œçš„å…§å®¹\n",
    "                            content_after_colon = line.split(':')[1].strip()\n",
    "                            \n",
    "                            # æŸ¥æ‰¾çµ„åˆ¥å­—æ¯ - æ”¯æŒå¤šç¨®æ ¼å¼\n",
    "                            group_letter = None\n",
    "                            \n",
    "                            # æ–¹æ³•1: æŸ¥æ‰¾ \"çµ„X\" æ ¼å¼\n",
    "                            import re\n",
    "                            group_match = re.search(r'çµ„([A-Ea-e])', content_after_colon)\n",
    "                            if group_match:\n",
    "                                group_letter = group_match.group(1).lower()\n",
    "                            \n",
    "                            # æ–¹æ³•2: æŸ¥æ‰¾å–®ç¨çš„å­—æ¯\n",
    "                            if not group_letter:\n",
    "                                letter_match = re.search(r'\\b([A-Ea-e])\\b', content_after_colon)\n",
    "                                if letter_match:\n",
    "                                    group_letter = letter_match.group(1).lower()\n",
    "                            \n",
    "                            # æ–¹æ³•3: ç›´æ¥æŸ¥æ‰¾ a-e å­—æ¯\n",
    "                            if not group_letter:\n",
    "                                for letter in ['a', 'b', 'c', 'd', 'e']:\n",
    "                                    if letter in content_after_colon.lower():\n",
    "                                        group_letter = letter\n",
    "                                        break\n",
    "                            \n",
    "                            if group_letter and group_letter in ['a', 'b', 'c', 'd', 'e']:\n",
    "                                group_index = ord(group_letter) - ord('a')\n",
    "                                if group_index < len(candidate_pairs):\n",
    "                                    pair = candidate_pairs[group_index]\n",
    "                                    selected_pair = (pair[0][0], pair[1][0])  # (id1, id2)\n",
    "                                    selected_pairs.append(selected_pair)\n",
    "    \n",
    "                            else:\n",
    "                                print(f\"ç„¡æ³•è­˜åˆ¥çµ„åˆ¥: '{content_after_colon}'\")\n",
    "                                \n",
    "                        except Exception as parse_error:\n",
    "                            print(f\"è§£æéŒ¯èª¤: {parse_error}\")\n",
    "                            continue\n",
    "            \n",
    "                \n",
    "                # ç¢ºä¿æœ€å¤šè¿”å› 2 å€‹å°ï¼Œä¸¦å»é‡\n",
    "                unique_pairs = []\n",
    "                for pair in selected_pairs:\n",
    "                    if pair not in unique_pairs:\n",
    "                        unique_pairs.append(pair)\n",
    "                \n",
    "                return unique_pairs[:2]\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt + 1 >= MAX_RETRY:\n",
    "                    print(f\"è² æ¨£æœ¬è©•ä¼°å¤±æ•—: {e}\")\n",
    "                    return []\n",
    "                \n",
    "                print(f\"å˜—è©¦ {attempt + 1} å¤±æ•—: {e}. 60ç§’å¾Œé‡è©¦...\")\n",
    "                time.sleep(60)\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    # ç”Ÿæˆå€™é¸è² æ¨£æœ¬å°\n",
    "    def generate_candidate_pairs(travel_df: pd.DataFrame, non_travel_df: pd.DataFrame, num_candidates: int) -> List[Tuple[Tuple[int, str], Tuple[int, str]]]:\n",
    "        \"\"\"ç”Ÿæˆå€™é¸è² æ¨£æœ¬å°\"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        # ç­–ç•¥1: è·¨é¡å‹é…å° (60%)\n",
    "        cross_type_count = int(num_candidates * 0.6)\n",
    "        for _ in range(cross_type_count):\n",
    "            if len(travel_df) > 0 and len(non_travel_df) > 0:\n",
    "                travel_sample = travel_df.sample(n=1).iloc[0]\n",
    "                non_travel_sample = non_travel_df.sample(n=1).iloc[0]\n",
    "                \n",
    "                candidates.append((\n",
    "                    (travel_sample['sms_id'], travel_sample['sms_body']),\n",
    "                    (non_travel_sample['sms_id'], non_travel_sample['sms_body'])\n",
    "                ))\n",
    "        \n",
    "        # ç­–ç•¥2: åŒé¡å‹éš¨æ©Ÿé…å° (40%)\n",
    "        remaining_count = num_candidates - len(candidates)\n",
    "        \n",
    "        # æ—…éŠé¡å‹å…§éš¨æ©Ÿé…å°\n",
    "        travel_internal_count = remaining_count // 2\n",
    "        if len(travel_df) >= 2:\n",
    "            for _ in range(travel_internal_count):\n",
    "                samples = travel_df.sample(n=2)\n",
    "                sample1, sample2 = samples.iloc[0], samples.iloc[1]\n",
    "                \n",
    "                candidates.append((\n",
    "                    (sample1['sms_id'], sample1['sms_body']),\n",
    "                    (sample2['sms_id'], sample2['sms_body'])\n",
    "                ))\n",
    "        \n",
    "        # éæ—…éŠé¡å‹å…§éš¨æ©Ÿé…å°\n",
    "        non_travel_internal_count = num_candidates - len(candidates)\n",
    "        if len(non_travel_df) >= 2:\n",
    "            for _ in range(non_travel_internal_count):\n",
    "                samples = non_travel_df.sample(n=2)\n",
    "                sample1, sample2 = samples.iloc[0], samples.iloc[1]\n",
    "                \n",
    "                candidates.append((\n",
    "                    (sample1['sms_id'], sample1['sms_body']),\n",
    "                    (sample2['sms_id'], sample2['sms_body'])\n",
    "                ))\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    # ä¸»è¦è™•ç†é‚è¼¯\n",
    "    total_batches = (max_negative_pairs + 1) // 2  # æ¯æ‰¹æœ€å¤šç”Ÿæˆ 2 å€‹è² æ¨£æœ¬\n",
    "    \n",
    "    print(f\"\\né–‹å§‹ç”Ÿæˆè² æ¨£æœ¬ï¼Œé è¨ˆéœ€è¦ {total_batches} æ‰¹æ¬¡è™•ç†\")\n",
    "    \n",
    "    with tqdm(total=max_negative_pairs, desc=\"è² æ¨£æœ¬ç”Ÿæˆé€²åº¦\") as pbar:\n",
    "        for batch_idx in range(total_batches):\n",
    "            if len(negative_pairs) >= max_negative_pairs:\n",
    "                break\n",
    "            \n",
    "            # ç”Ÿæˆ 5 å€‹å€™é¸å°\n",
    "            candidates = generate_candidate_pairs(travel_df, non_travel_df, 5)\n",
    "            if len(candidates) == 5:\n",
    "                # è©•ä¼°ä¸¦é¸å‡ºæœ€ä¸åŒ¹é…çš„ 2 å€‹\n",
    "                selected_negative_pairs = evaluate_negative_batch(candidates)\n",
    "                for pair in selected_negative_pairs:\n",
    "                    if len(negative_pairs) < max_negative_pairs:\n",
    "                        negative_pairs.append(pair)\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                        # é¡¯ç¤ºæ‰¾åˆ°çš„è² æ¨£æœ¬\n",
    "                        id1, id2 = pair\n",
    "                        sms1 = df[df['sms_id'] == id1]['sms_body'].iloc[0]\n",
    "                        sms2 = df[df['sms_id'] == id2]['sms_body'].iloc[0]\n",
    "                        \n",
    "            else:\n",
    "                print(f\"è­¦å‘Š: æ‰¹æ¬¡ {batch_idx + 1} å€™é¸å°ä¸è¶³ 5 å€‹\")\n",
    "    \n",
    "    print(f\"\\nè² æ¨£æœ¬ç”Ÿæˆå®Œæˆï¼ç¸½å…±ç”Ÿæˆ {len(negative_pairs)} å€‹è² æ¨£æœ¬å°\")\n",
    "    return negative_pairs\n",
    "\n",
    "# åŸ·è¡Œè² æ¨£æœ¬ç”Ÿæˆ - ä½¿ç”¨é…ç½®çš„æ•¸é‡\n",
    "print(\"é–‹å§‹ç”Ÿæˆé«˜è³ªé‡è² æ¨£æœ¬...\")\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦æœ‰é è¨­çš„æ•¸é‡é…ç½®\n",
    "if 'NEGATIVE_PAIRS_COUNT' not in locals():\n",
    "    NEGATIVE_PAIRS_COUNT = 500  # é è¨­å€¼\n",
    "    print(f\"âš ï¸  æœªæ‰¾åˆ°é…ç½®ï¼Œä½¿ç”¨é è¨­å€¼: {NEGATIVE_PAIRS_COUNT} å€‹è² æ¨£æœ¬\")\n",
    "\n",
    "print(f\"ğŸ¯ ç›®æ¨™ç”Ÿæˆ: {NEGATIVE_PAIRS_COUNT} å€‹è² æ¨£æœ¬å°\")\n",
    "\n",
    "# ä¼°ç®—æ‰€éœ€æ™‚é–“\n",
    "estimated_batches = (NEGATIVE_PAIRS_COUNT + 1) // 2\n",
    "estimated_api_calls = estimated_batches\n",
    "estimated_minutes = estimated_api_calls / (RPM / 4)  # è€ƒæ…®4å€‹API keyçš„ä¸¦è¡Œ\n",
    "print(f\"ğŸ“… é ä¼°è™•ç†æ™‚é–“: {estimated_minutes:.1f} åˆ†é˜ ({estimated_batches} æ‰¹æ¬¡, {estimated_api_calls} APIèª¿ç”¨)\")\n",
    "\n",
    "# åŸ·è¡Œç”Ÿæˆ\n",
    "negative_pairs = generate_negative_pairs_batch(df, max_negative_pairs=NEGATIVE_PAIRS_COUNT)\n",
    "\n",
    "print(f\"\\nğŸ“Š === è² æ¨£æœ¬ç”Ÿæˆçµæœ ===\")\n",
    "print(f\"âœ… æˆåŠŸç”Ÿæˆ: {len(negative_pairs)} å€‹è² æ¨£æœ¬å°\")\n",
    "print(f\"ğŸ¯ ç›®æ¨™æ•¸é‡: {NEGATIVE_PAIRS_COUNT}\")\n",
    "print(f\"ğŸ“ˆ å®Œæˆç‡: {len(negative_pairs)/NEGATIVE_PAIRS_COUNT*100:.1f}%\")\n",
    "\n",
    "if len(negative_pairs) < NEGATIVE_PAIRS_COUNT:\n",
    "    shortfall = NEGATIVE_PAIRS_COUNT - len(negative_pairs)\n",
    "    print(f\"âš ï¸  çŸ­ç¼º {shortfall} å€‹è² æ¨£æœ¬ï¼Œå¯èƒ½æ˜¯ç”±æ–¼APIé™åˆ¶æˆ–æ•¸æ“šé™åˆ¶\")\n",
    "    print(\"ğŸ’¡ å»ºè­°: å¯ä»¥é‡æ–°åŸ·è¡Œæ­¤cellæˆ–èª¿æ•´æ‰¹æ¬¡å¤§å°\")\n",
    "\n",
    "# åˆ†æè² æ¨£æœ¬è³ªé‡\n",
    "def analyze_negative_pairs(negative_pairs: List[Tuple[int, int]], df: pd.DataFrame):\n",
    "    \"\"\"åˆ†æè² æ¨£æœ¬çš„è³ªé‡\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== è² æ¨£æœ¬è³ªé‡åˆ†æ ===\")\n",
    "    \n",
    "    cross_type_count = 0\n",
    "    same_type_count = 0\n",
    "    \n",
    "    for id1, id2 in negative_pairs:\n",
    "        label1 = df[df['sms_id'] == id1]['label'].iloc[0]\n",
    "        label2 = df[df['sms_id'] == id2]['label'].iloc[0]\n",
    "        \n",
    "        if label1 != label2:\n",
    "            cross_type_count += 1\n",
    "        else:\n",
    "            same_type_count += 1\n",
    "    \n",
    "    print(f\"è·¨é¡å‹è² æ¨£æœ¬: {cross_type_count} å€‹ ({cross_type_count/len(negative_pairs)*100:.1f}%)\")\n",
    "    print(f\"åŒé¡å‹è² æ¨£æœ¬: {same_type_count} å€‹ ({same_type_count/len(negative_pairs)*100:.1f}%)\")\n",
    "    \n",
    "    # é¡¯ç¤ºä¸€äº›è² æ¨£æœ¬ä¾‹å­\n",
    "    print(f\"\\n=== è² æ¨£æœ¬ä¾‹å­ ===\")\n",
    "    import random\n",
    "    sample_size = min(3, len(negative_pairs))\n",
    "    sample_pairs = random.sample(negative_pairs, sample_size)\n",
    "    \n",
    "    for i, (id1, id2) in enumerate(sample_pairs, 1):\n",
    "        sms1 = df[df['sms_id'] == id1]['sms_body'].iloc[0]\n",
    "        sms2 = df[df['sms_id'] == id2]['sms_body'].iloc[0]\n",
    "        label1 = df[df['sms_id'] == id1]['label'].iloc[0]\n",
    "        label2 = df[df['sms_id'] == id2]['label'].iloc[0]\n",
    "        \n",
    "        print(f\"\\n--- è² æ¨£æœ¬ {i} ---\")\n",
    "        print(f\"ID {id1} (label={label1}): {sms1}\")\n",
    "        print(f\"ID {id2} (label={label2}): {sms2}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "if negative_pairs:\n",
    "    analyze_negative_pairs(negative_pairs, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42332ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è² æ¨£æœ¬çµæœå·²ä¿å­˜åˆ°: sms_negative_pairs_with_content.csv (1192 å°)\n",
      "ç°¡æ½”è² æ¨£æœ¬çµæœå·²ä¿å­˜åˆ°: sms_negative_pairs.csv\n",
      "\n",
      "=== è² æ¨£æœ¬çµæœé è¦½ ===\n",
      "     ç°¡è¨Šid  ç›¸ä¼¼ç°¡è¨Šid é…å°é¡å‹\n",
      "0   19635  267771  è·¨é¡å‹\n",
      "1  329175   15069  è·¨é¡å‹\n",
      "2  291076  393114  è·¨é¡å‹\n",
      "3  255383   75606  è·¨é¡å‹\n",
      "4   41187  391525  è·¨é¡å‹\n",
      "5  282119  220849  è·¨é¡å‹\n",
      "6  137836  134915  è·¨é¡å‹\n",
      "7  185111  259560  è·¨é¡å‹\n",
      "8  272034  342824  è·¨é¡å‹\n",
      "9  319287  402629  è·¨é¡å‹\n",
      "\n",
      "âœ… è² æ¨£æœ¬ç”Ÿæˆå’Œä¿å­˜å®Œæˆï¼\n",
      "ğŸ“ å·²ä¿å­˜æª”æ¡ˆ:\n",
      "  - sms_negative_pairs.csv (ç°¡æ½”ç‰ˆ)\n",
      "  - sms_negative_pairs_with_content.csv (å®Œæ•´ç‰ˆ)\n"
     ]
    }
   ],
   "source": [
    "# ä¿å­˜è² æ¨£æœ¬çµæœ\n",
    "def save_negative_pairs_to_csv(negative_pairs: List[Tuple[int, int]], df: pd.DataFrame):\n",
    "    \"\"\"ä¿å­˜è² æ¨£æœ¬å°åˆ° CSV æª”æ¡ˆ\"\"\"\n",
    "    \n",
    "    if not negative_pairs:\n",
    "        print(\"æ²’æœ‰è² æ¨£æœ¬å¯ä¿å­˜\")\n",
    "        return None\n",
    "    \n",
    "    # å‰µå»ºè² æ¨£æœ¬ DataFrame\n",
    "    negative_df = pd.DataFrame(negative_pairs, columns=[\"ç°¡è¨Šid\", \"ç›¸ä¼¼ç°¡è¨Šid\"])\n",
    "    negative_df['é¡å‹'] = 'è² æ¨£æœ¬'\n",
    "    \n",
    "    # æ·»åŠ ç°¡è¨Šå…§å®¹å’Œæ¨™ç±¤\n",
    "    def get_sms_info(sms_id):\n",
    "        row = df[df['sms_id'] == sms_id]\n",
    "        if len(row) > 0:\n",
    "            return row.iloc[0]['sms_body'], row.iloc[0]['label']\n",
    "        return \"æœªæ‰¾åˆ°\", -1\n",
    "    \n",
    "    # ç²å–ç°¡è¨Šå…§å®¹å’Œæ¨™ç±¤\n",
    "    sms1_info = negative_df['ç°¡è¨Šid'].apply(get_sms_info)\n",
    "    sms2_info = negative_df['ç›¸ä¼¼ç°¡è¨Šid'].apply(get_sms_info)\n",
    "    \n",
    "    negative_df['ç°¡è¨Šå…§å®¹'] = [info[0] for info in sms1_info]\n",
    "    negative_df['ç°¡è¨Šæ¨™ç±¤'] = [info[1] for info in sms1_info]\n",
    "    negative_df['ç›¸ä¼¼ç°¡è¨Šå…§å®¹'] = [info[0] for info in sms2_info]\n",
    "    negative_df['ç›¸ä¼¼ç°¡è¨Šæ¨™ç±¤'] = [info[1] for info in sms2_info]\n",
    "    \n",
    "    # æ·»åŠ é…å°é¡å‹åˆ†æ\n",
    "    def get_pair_type(label1, label2):\n",
    "        if label1 == label2:\n",
    "            return 'åŒé¡å‹'\n",
    "        else:\n",
    "            return 'è·¨é¡å‹'\n",
    "    \n",
    "    negative_df['é…å°é¡å‹'] = negative_df.apply(\n",
    "        lambda row: get_pair_type(row['ç°¡è¨Šæ¨™ç±¤'], row['ç›¸ä¼¼ç°¡è¨Šæ¨™ç±¤']), axis=1\n",
    "    )\n",
    "    \n",
    "    # ä¿å­˜å®Œæ•´ç‰ˆæœ¬\n",
    "    negative_df.to_csv(\"sms_negative_pairs_with_content.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(f\"å®Œæ•´è² æ¨£æœ¬çµæœå·²ä¿å­˜åˆ°: sms_negative_pairs_with_content.csv ({len(negative_pairs)} å°)\")\n",
    "    \n",
    "    # ä¿å­˜ç°¡æ½”ç‰ˆæœ¬ï¼ˆåªæœ‰IDå’Œé¡å‹ï¼‰\n",
    "    simple_negative_df = negative_df[['ç°¡è¨Šid', 'ç›¸ä¼¼ç°¡è¨Šid', 'é¡å‹', 'é…å°é¡å‹']]\n",
    "    simple_negative_df.to_csv(\"sms_negative_pairs.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(f\"ç°¡æ½”è² æ¨£æœ¬çµæœå·²ä¿å­˜åˆ°: sms_negative_pairs.csv\")\n",
    "    \n",
    "    return negative_df\n",
    "\n",
    "# åˆä½µæ­£è² æ¨£æœ¬çµæœ\n",
    "def create_complete_training_dataset(travel_pairs: List[Tuple[int, int]], \n",
    "                                   non_travel_pairs: List[Tuple[int, int]],\n",
    "                                   negative_pairs: List[Tuple[int, int]], \n",
    "                                   df: pd.DataFrame):\n",
    "    \"\"\"å‰µå»ºå®Œæ•´çš„è¨“ç·´æ•¸æ“šé›†ï¼ˆæ­£æ¨£æœ¬ + è² æ¨£æœ¬ï¼‰\"\"\"\n",
    "    \n",
    "    all_pairs = []\n",
    "    \n",
    "    # æ·»åŠ æ—…éŠæ­£æ¨£æœ¬\n",
    "    for pair in travel_pairs:\n",
    "        all_pairs.append({\n",
    "            'ç°¡è¨Šid': pair[0],\n",
    "            'ç›¸ä¼¼ç°¡è¨Šid': pair[1],\n",
    "            'æ¨™ç±¤': 1,  # æ­£æ¨£æœ¬\n",
    "            'æ¨£æœ¬é¡å‹': 'æ—…éŠæ­£æ¨£æœ¬'\n",
    "        })\n",
    "    \n",
    "    # æ·»åŠ éæ—…éŠæ­£æ¨£æœ¬\n",
    "    for pair in non_travel_pairs:\n",
    "        all_pairs.append({\n",
    "            'ç°¡è¨Šid': pair[0],\n",
    "            'ç›¸ä¼¼ç°¡è¨Šid': pair[1],\n",
    "            'æ¨™ç±¤': 1,  # æ­£æ¨£æœ¬\n",
    "            'æ¨£æœ¬é¡å‹': 'éæ—…éŠæ­£æ¨£æœ¬'\n",
    "        })\n",
    "    \n",
    "    # æ·»åŠ è² æ¨£æœ¬\n",
    "    for pair in negative_pairs:\n",
    "        all_pairs.append({\n",
    "            'ç°¡è¨Šid': pair[0],\n",
    "            'ç›¸ä¼¼ç°¡è¨Šid': pair[1],\n",
    "            'æ¨™ç±¤': 0,  # è² æ¨£æœ¬\n",
    "            'æ¨£æœ¬é¡å‹': 'è² æ¨£æœ¬'\n",
    "        })\n",
    "    \n",
    "    # å‰µå»ºå®Œæ•´æ•¸æ“šé›†\n",
    "    complete_df = pd.DataFrame(all_pairs)\n",
    "    \n",
    "    # æ·»åŠ ç°¡è¨Šå…§å®¹\n",
    "    def get_sms_content(sms_id):\n",
    "        content = df[df['sms_id'] == sms_id]['sms_body']\n",
    "        return content.iloc[0] if len(content) > 0 else \"æœªæ‰¾åˆ°\"\n",
    "    \n",
    "    complete_df['ç°¡è¨Šå…§å®¹'] = complete_df['ç°¡è¨Šid'].apply(get_sms_content)\n",
    "    complete_df['ç›¸ä¼¼ç°¡è¨Šå…§å®¹'] = complete_df['ç›¸ä¼¼ç°¡è¨Šid'].apply(get_sms_content)\n",
    "    \n",
    "    # ä¿å­˜å®Œæ•´è¨“ç·´æ•¸æ“šé›†\n",
    "    complete_df.to_csv(\"sms_complete_training_dataset.csv\", index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # çµ±è¨ˆä¿¡æ¯\n",
    "    print(f\"\\nğŸ“Š === å®Œæ•´è¨“ç·´æ•¸æ“šé›†çµ±è¨ˆ ===\")\n",
    "    print(f\"æ—…éŠæ­£æ¨£æœ¬: {len(travel_pairs)} å°\")\n",
    "    print(f\"éæ—…éŠæ­£æ¨£æœ¬: {len(non_travel_pairs)} å°\")\n",
    "    print(f\"è² æ¨£æœ¬: {len(negative_pairs)} å°\")\n",
    "    print(f\"ç¸½æ¨£æœ¬: {len(all_pairs)} å°\")\n",
    "    \n",
    "    positive_ratio = (len(travel_pairs) + len(non_travel_pairs)) / len(all_pairs) * 100\n",
    "    negative_ratio = len(negative_pairs) / len(all_pairs) * 100\n",
    "    \n",
    "    print(f\"æ­£æ¨£æœ¬æ¯”ä¾‹: {positive_ratio:.1f}%\")\n",
    "    print(f\"è² æ¨£æœ¬æ¯”ä¾‹: {negative_ratio:.1f}%\")\n",
    "    print(f\"å®Œæ•´è¨“ç·´æ•¸æ“šé›†å·²ä¿å­˜åˆ°: sms_complete_training_dataset.csv\")\n",
    "    \n",
    "    return complete_df\n",
    "\n",
    "# ä¿å­˜è² æ¨£æœ¬çµæœ\n",
    "if negative_pairs:\n",
    "    negative_result_df = save_negative_pairs_to_csv(negative_pairs, df)\n",
    "    \n",
    "    # é¡¯ç¤ºè² æ¨£æœ¬çµæœé è¦½\n",
    "    if negative_result_df is not None:\n",
    "        print(f\"\\n=== è² æ¨£æœ¬çµæœé è¦½ ===\")\n",
    "        print(negative_result_df[['ç°¡è¨Šid', 'ç›¸ä¼¼ç°¡è¨Šid', 'é…å°é¡å‹']].head(10))\n",
    "        \n",
    "        print(\"\\nâœ… è² æ¨£æœ¬ç”Ÿæˆå’Œä¿å­˜å®Œæˆï¼\")\n",
    "        print(\"ğŸ“ å·²ä¿å­˜æª”æ¡ˆ:\")\n",
    "        print(\"  - sms_negative_pairs.csv (ç°¡æ½”ç‰ˆ)\")\n",
    "        print(\"  - sms_negative_pairs_with_content.csv (å®Œæ•´ç‰ˆ)\")\n",
    "else:\n",
    "    print(\"æ²’æœ‰ç”Ÿæˆè² æ¨£æœ¬ï¼Œè«‹å…ˆåŸ·è¡Œè² æ¨£æœ¬ç”Ÿæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ea4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sms_cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
